{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f8fb632-d270-4212-ae86-e3285261c321",
   "metadata": {},
   "source": [
    "# GPU训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a73df0-fdbd-4c31-b7bc-556157d37221",
   "metadata": {},
   "source": [
    "### Single Node, Single GPU Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff377257-a995-4149-ac64-5f8d7a906ed6",
   "metadata": {},
   "source": [
    "Training throughput depends on:\n",
    "- Neural network model (activations, parameters, compute operations)\n",
    "- Batch size\n",
    "- Compute hardware: GPU type (e.g., Nvidia M60, K80, P100, V100)\n",
    "- Floating point precision (FP32 vs FP16)\n",
    "    - 换了FP16之后相当于working with reduced precision arithmetic｜Using FP16 can reduce training times and enable larger batch sizes/models without significantly impacting the accuracy of the trained model\n",
    "    \n",
    "训练时间：Training time with single GPU very large: 6 days with Places dataset (2.5M images) using Alexnet on a single K40.\n",
    "\n",
    "提高performance的办法一般是Increasing batch size increases throughput\n",
    "- 然而Batch size is restricted by GPU memory！\n",
    "- GPU的限制下⇒Small batch size $=>$ noisier approximation of the gradient $\\Rightarrow$ lower learning rate $=>$ slower convergence\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f404a80-892f-4e93-8134-4544d117127b",
   "metadata": {},
   "source": [
    "### DL Scaling with GPU\n",
    "\n",
    "#### 硬件选择：Commonly used GPU Accelerators in Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b6845a-bb32-45b5-ab60-d303641b4eb9",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.mathpix.com/snip/images/EgvWEJqCtlVEjyoLDQ1kqDWtRSVN5q_ZW9jcmnqsJFM.original.fullsize.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17700d32-ce2d-4405-a33c-6f1968fcc2fc",
   "metadata": {},
   "source": [
    "\n",
    "- 两个K80就可以实现6个TFLOPS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b8450d-18a7-4fc3-8c68-56a33292729a",
   "metadata": {},
   "source": [
    "#### 对比单GPU的提升\n",
    "\n",
    "<img src=\"https://cdn.mathpix.com/snip/images/ym85dUm7bzwccOmHZq03IOOmqBdaK34LrMLl783Nyws.original.fullsize.png\" />\n",
    "\n",
    "- 不同模型的speedup不一样，因为different networks have different structure, they have different amount of floating point operations requirement, they have different memory requirement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7d33f6-9256-4df1-98fb-aa76a2fd39f3",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.mathpix.com/snip/images/F8v3Uz9CFYVg0XIHolZnrAKvV4u5H-AE1mapfdeJrMQ.original.fullsize.png\" />\n",
    "- 从batch size提高后的improve来看也是一样的model dependent！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37f5203-ba65-4bec-95ce-26fc6a13a9ca",
   "metadata": {},
   "source": [
    "### Single Node, Multi-GPU Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d452e1e7-00a6-436a-b639-4ff77bf62468",
   "metadata": {},
   "source": [
    "最重要的是communicate的cost！让Scaling not linear的各种影响因素——Synchronization\n",
    "- Communication libraries (e.g., NCCL) and supported communication algorithms/collectives (broadcast, all-reduce, gather)\n",
    "    - NCCL (\"Nickel\") is library of accelerated collectives that is easily integrated and topology-aware so as to improve the scalability of multi-GPU applications\n",
    "- Communication link bandwidth: PCle/QPI or NVlink\n",
    "- Communication algorithms depend on the communication topology (ring, hub-spoke, fully connected) between the GPUs.\n",
    "- Most collectives amenable to bandwidth-optimal implementation on rings, and many topologies can be interpreted as one or more rings [P. Patarasuk and X. Yuan]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8876e41a-5b4d-40ab-9ea9-4beee3ab5db2",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.mathpix.com/snip/images/XWDeJcrg4WsPOm1CDXkWF005zbjCK7KoXanRu0yo630.original.fullsize.png\" />\n",
    "\n",
    "- V100优势在于GPU和GPU之间的connectivity很好"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3592f8c-60c6-4e81-8202-caf3a4caf900",
   "metadata": {},
   "source": [
    "没有NVLINK的时候："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ebb538-3b05-4602-964e-f87cdc556148",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.mathpix.com/snip/images/pwqaKYxymHmAt-5HJ7HR37vmC5lgVfR8ITd73dYZVjE.original.fullsize.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb1d523-62f3-4a50-aa27-a090af1a94ad",
   "metadata": {},
   "source": [
    "有NVLINK的时候："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a20f80-4921-463c-bd21-0b9540c51c28",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.mathpix.com/snip/images/bFRPoR_Y-bsmIVYEzzLtJzSnchkKboILT1Zh3O7HCus.original.fullsize.png\" />\n",
    "\n",
    "<img src=\"https://cdn.mathpix.com/snip/images/EUtejV4EyMXJtlUlGNuAA6XayYDeicPhJlmuCSw_oH4.original.fullsize.png\" />\n",
    "\n",
    "<img src=\"https://cdn.mathpix.com/snip/images/6UpUK2ib1ACVqIj-N7a42p-Gs-V2xUZrTcFTkAWHbD8.original.fullsize.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0ed97b-6874-49ca-a943-f5b87d8928fe",
   "metadata": {},
   "source": [
    "## Distributed Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba101ab-1de7-46f0-8c5b-a22ff80ec6b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
