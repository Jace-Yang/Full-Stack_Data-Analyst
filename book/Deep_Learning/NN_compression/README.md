# 神经网络压缩

模型压缩大体上可以分为5种

- **模型剪枝**：即移除对结果作用较小的组件，如减少 head 的数量和去除作用较少的层，共享参数等，ALBERT属于这种；
- **量化**：比如将 float32 降到 float8；
- **知识蒸馏**：将 teacher 的能力蒸馏到 student上，一般 student 会比 teacher 小。我们可以把一个大而深的网络蒸馏到一个小的网络，也可以把集成的网络蒸馏到一个小的网络上。
- **参数共享**：通过共享参数，达到减少网络参数的目的，如 ALBERT 共享了 Transformer 层；
- **参数矩阵近似**：通过矩阵的低秩分解或其他方法达到降低矩阵参数的目的；


## Motivation

训练和部署之间存在着一定的不一致性: :
- 在训练过程中, 我们需要使用复杂的模型, 大量的计算资源, 以便从非常大、高度冗余的数据集 中提取出信息。在实验中, 效果最好的模型往往规模很大, 甚至由多个模型集成得到。而大模型 不方便部署到服务中去, 常见的瓶颈如下:
- 推断速度慢
- 对部署资源要求高(内存, 显存等)
- 在部署时, 我们对延迟以及计算资源都有着严格的限制。
这也就是模型压缩的动机：我们希望有一个规模较小的模型, 能达到和大模型一样或相当的结果。