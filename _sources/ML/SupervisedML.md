# æœ‰ç›‘ç£å­¦ä¹ 

## æ•´ä½“æ¡†æ¶

**æœ€ç»ˆè¾“å‡ºï¼šè¶…å‚æ•°è®¾å®šâ•æ¨¡å‹â•æ¨¡å‹è¡¨ç°**

- Development-test splitã€Hyperparameter tuning ã€Optimal model trainingã€Model evaluationã€Model deployment

![img](../images/(null)-20220724153653287.(null))

- evaluationå¾ˆé‡è¦ï¼šå› ä¸ºæˆ‘ä»¬éœ€è¦çŸ¥é“æ¯ä¸ªå¤æ‚åº¦çš„æ¨¡å‹å¯¹åº”çš„è¡¨ç°æ˜¯å¤šå°‘ï¼Œæ¥åˆ¤æ–­æœ‰æ²¡æœ‰å¿…è¦ç”¨å¤æ‚çš„æ¨¡å‹ï¼

  å› æ­¤ï¼šä¸å¯ä»¥ç”¨æ•´ä¸ªdatasetå‡ºè¶…å‚æ•°è€Œæ”¾å¼ƒevaluation

### Data Preprocessing

#### Missing Data

- ä¸å¤„ç†ï¼ˆå½“æˆä¸€ç§å€¼ï¼‰â€”â€”ç”¨å¯¹ç¼ºå¤±å€¼ä¸æ•æ„Ÿçš„æ ‘æ¨¡å‹

  - LightGBMå’ŒXGBoostéƒ½æ˜¯æ¯æ¬¡åˆ†å‰²çš„æ—¶å€™ï¼Œåˆ†åˆ«æŠŠç¼ºå¤±å€¼æ”¾åœ¨å·¦å³ä¸¤è¾¹å„è®¡ç®—ä¸€æ¬¡ï¼Œç„¶åæ¯”è¾ƒä¸¤ç§æƒ…å†µçš„å¢ç›Šï¼Œæ‹©ä¼˜å½•å–

    <img src="../images/v2-3c043cfb7323495327f8ad697b5a3a22_1440w.jpg" alt="img" style="width:40%;" />

  - ä½†æ³¨æ„è¿™é‡Œæˆ‘ä»¬å‡è®¾äº†è®­ç»ƒæ•°æ®å’Œé¢„æµ‹æ•°æ®çš„åˆ†å¸ƒç›¸åŒï¼Œæ¯”å¦‚ç¼ºå¤±å€¼çš„åˆ†å¸ƒä¹Ÿç›¸åŒ

- å‰”é™¤

  - Drop column (typically used as baseline)ï¼šç¼ºå¤±å¤ªå¤šçš„æ—¶å€™
  - Drop rows (if there are only a few with missing values)

- å¡«å……ï¼ˆImputeï¼‰/ä¼°ç®—(estimation)ï¼š

  - mean or median (SimpleImputer in sklearn API) æ²¡æœ‰å……åˆ†è€ƒè™‘æ•°æ®ä¸­å·²æœ‰çš„ä¿¡æ¯ï¼Œè¯¯å·®å¯èƒ½è¾ƒå¤§
  - kNN (neighbors are found using nan_euclidean_distance  metric)
  - Regression modelsæ ¹æ®è°ƒæŸ¥å¯¹è±¡å¯¹å…¶ä»–é—®é¢˜çš„ç­”æ¡ˆï¼Œé€šè¿‡å˜é‡ä¹‹é—´çš„ç›¸å…³åˆ†ææˆ–é€»è¾‘æ¨è®ºè¿›è¡Œä¼°è®¡ã€‚ä¾‹å¦‚ï¼ŒæŸä¸€äº§å“çš„æ‹¥æœ‰æƒ…å†µå¯èƒ½ä¸å®¶åº­æ”¶å…¥æœ‰å…³ï¼Œå¯ä»¥æ ¹æ®è°ƒæŸ¥å¯¹è±¡çš„å®¶åº­æ”¶å…¥æ¨ç®—æ‹¥æœ‰è¿™ä¸€äº§å“çš„å¯èƒ½æ€§

- Add a binary additional indicator column (è·Ÿä¸Šä¸€æ­¥ä¸€è‡´)

  (often captured by adding missing indicator columns) 

  - Missing in not random! It will add value to the model
  - æ¯”å¦‚ï¼æœ‰ä¸ªclass is always missingï¼å°±åƒ16å²ä»¥ä¸‹çš„è¿™ä¸ªç»„æ²¡æœ‰é©¾ç…§å¹´é™,è¿™ä¸ªå¯ä»¥æ˜¯predictive columnsï¼ï¼

- Matrix factorizationï¼šå°†ä¸€ä¸ªå«*ç¼ºå¤±*å€¼çš„çŸ©é˜µ X åˆ†è§£ä¸ºä¸¤ä¸ª(æˆ–å¤šä¸ª)çŸ©é˜µ,ç„¶åè¿™äº›åˆ†è§£åçš„çŸ©é˜µç›¸ä¹˜å°± å¯ä»¥å¾—åˆ°åŸçŸ©é˜µçš„è¿‘ä¼¼ X

#### Categorical data

æ³¨æ„ï¼šéƒ½æ˜¯å¯¹åˆ†å¼€ä¹‹åçš„æ•°æ®ï¼åªé’ˆå¯¹train data æ¥fit

- Ordinal encoding

  - Missing valueå¯ä»¥ç†è§£ä¸ºæœ€ä¸é‡è¦çš„classç„¶åç»™0ï¼Œä¹Ÿå¯ä»¥ç†è§£ä¸ºæœ€é‡è¦çš„ç»™maxï¼æˆ–è€…imputeæˆmode
- One-hot encoding: no information loss.

  - ç‰¹ç‚¹

    - å¤„ç†ç¼ºå¤±ï¼šmissingçš„æ—¶å€™å¯ä»¥æŠŠmissingå½“ä½œä¸€ç§category
    - æµ‹è¯•é›†é‡åˆ°æ–°çš„ç±»åˆ«çš„æ—¶å€™ï¼šåŠ å…¥`handle_unkown = "ignore"`å¯ä»¥
  - åœºæ™¯ï¼š
  
    - One-hot encoding introduces **multi-collinearity**
  
      - For e.g., x3 = 1 - x1 - x2 (in case when we have three categories)
      - Possible to remove one feature, because it's a **linear combination of the other columns**, could be problematic for some non-regularized regression models
      - Has **implications** on model interpretation
        - å¯ä»¥dropè¿™ä¸ªä¹Ÿå¯ä»¥dropåˆ«çš„ï¼Œè¿™æ ·çš„è¯feature importanceå°±ä¸åŒäº†
        - æœ‰äººå¯ä»¥keep all columns, and apply regulariazation to take care of during the training process, then get insights into the model
    - æœ‰çš„æ¨¡å‹ æ¯”å¦‚treesï¼Œå¯ä»¥split on categorical variables, so it will automatically handles categorical variablesï¼š
    
      - Tree-based models
      - Naive Bayes models
  - é—®é¢˜ï¼šLeads to high-dimensional datasets
- Target encodingï¼šä¸æ˜¯introduce 1 column for 1 category, è€Œæ˜¯summarize the information for each category and convert into 1 column

  - Generally applicable for high **cardinality** categorical features 

  - å…·ä½“encodeçš„æ–¹å¼å–å†³äºæ¨¡å‹é—®é¢˜ï¼š

    - `Regression`: Average target value for each category

    - `Classification`: Average of æ¦‚ç‡â€”â€”è¿™ä¸ªæ¯”ç›´æ¥mapåˆ°labelå¥½ï¼Œå› ä¸ºä¾ç„¶å¯ä»¥æ ¹æ®probabilityåŒºåˆ†å‡ºä¸åŒçš„classå¯¹yçš„å½±å“
      - `Binary classiï¬cation`: Probability of being in class 1
      
      - `Multiclass classiï¬cation`: *One feature per class* that gives the probability distribution

#### Numerical Feature Scaling

- scalingä¸ä¼šæ”¹å˜åŸå§‹æ•°æ®ï¼Œä½†æ˜¯ä¼šè®©æ¨¡å‹å˜å¾—å¥½
- è®°å¾—è¦fit_transform(è®­ç»ƒé›†)ï¼Œç„¶åtransform(æµ‹è¯•é›†)è€Œä¸æ˜¯fit_transform(æµ‹è¯•é›†)ï¼Œå› ä¸ºæˆ‘ä»¬ä¸çŸ¥é“æµ‹è¯•é›†çš„meanå’Œstdç­‰

<img src="../images/(null)-20220724215527501.(null)" alt="img" style="width:50%;" /><img src="../images/(null)-20220724215537886.(null)" alt="img" style="width:50%;" />

- å…·ä½“çš„æ–¹å¼ï¼š

  æ ‡å‡†åŒ–ï¼šæœ€å¤§æœ€å°æ ‡å‡†åŒ–ã€zæ ‡å‡†åŒ–â€”â€”StandardScaler()ã€MinMaxScaler()ã€MaxAbsScaler()ï¼ˆé™¤ä»¥æœ€å¤§å€¼çš„è¯è´Ÿæ•°è¿˜ä¼šæ˜¯è´Ÿæ•°ï¼‰ã€RobustScaler()ã€Nomalizer()ï¼ˆå˜æˆåœ†å½¢ï¼‰

  å½’ä¸€åŒ–ï¼šå¯¹äºæ–‡æœ¬æˆ–è¯„åˆ†ç‰¹å¾ï¼Œä¸åŒæ ·æœ¬ä¹‹é—´å¯èƒ½æœ‰æ•´ä½“ä¸Šçš„å·®å¼‚ï¼Œå¦‚aæ–‡æœ¬å…±20ä¸ªè¯ï¼Œbæ–‡æœ¬30000ä¸ªè¯ï¼Œbæ–‡æœ¬ä¸­å„ä¸ªç»´åº¦ä¸Šçš„é¢‘æ¬¡éƒ½å¾ˆå¯èƒ½è¿œè¿œé«˜äºaæ–‡æœ¬

- æ³¨æ„åº”è¯¥åšfitçš„æ•°æ®é›†è·Ÿåº”è¯¥åšfitçš„æ¨¡å‹æ˜¯ä¸€è‡´çš„

  - æ¯”å¦‚hyper parameter tuningçš„æ—¶å€™  scalerä¸åº”è¯¥ç¢°validation data

    <img src="../images/(null)-20220724215547062.(null)" alt="img" style="width:50%;" />

#### Outliers

- æ£€æµ‹æ–¹å¼ï¼š

  - åŸºäºä¸šåŠ¡ç†è§£ï¼š
    - è¶…è¿‡é˜ˆå€¼çš„è„æ•°æ®ï½œæ¯”å¦‚èº«é«˜è¶…è¿‡2ç±³5ï¼›å¹´é¾„è¶…è¿‡150Â·Â·Â·

  - åŸºäºé‚»è¿‘åº¦çš„æŠ€æœ¯ï¼šé€šå¸¸å¯ä»¥åœ¨å¯¹è±¡ä¹‹é—´å®šä¹‰é‚»è¿‘æ€§åº¦é‡ï¼Œå¼‚å¸¸å¯¹è±¡æ˜¯é‚£äº›è¿œç¦»å…¶ä»–å¯¹è±¡çš„å¯¹è±¡
    - **ç®±çº¿å›¾**ï¼šQ1 Q3å†ç¦»å¼€1.5ä¸ªIQR(interquartile range Q1~Q3)
    - å‡å€¼ç¦»å¼€3ä¸ªæ ‡å‡†å·®
    - èšç±»åˆ†æï¼šè®¡ç®—ç°‡å†…æ¯ä¸ªç‚¹å¯¹äºç°‡ä¸­å¿ƒçš„ç›¸å¯¹è·ç¦»ï¼Œæ‰¾åˆ°è·ç¦»å¤§çš„

  - å»ºç«‹ä¸€ä¸ªæ•°æ®æ¨¡å‹ï¼Œå¼‚å¸¸æ˜¯é‚£äº›åŒæ¨¡å‹ä¸èƒ½å®Œç¾æ‹Ÿåˆçš„å¯¹è±¡
    - LRä¹‹åçœ‹cookè·ç¦»

  - åŸºäºå¯†åº¦çš„æŠ€æœ¯ï¼šä»…å½“ä¸€ä¸ªç‚¹çš„å±€éƒ¨å¯†åº¦æ˜¾è‘—ä½äºå®ƒçš„å¤§éƒ¨åˆ†è¿‘é‚»æ—¶æ‰å°†å…¶åˆ†ç±»ä¸ºç¦»ç¾¤ç‚¹
    - å±€éƒ¨ç¦»ç¾¤ç‚¹å› å­æ£€æµ‹ï¼šå±€éƒ¨ç¦»ç¾¤ç‚¹å› å­æ˜¯ä¸€ç§è¯†åˆ«åŸºäºå¯†åº¦çš„å±€éƒ¨ç¦»ç¾¤ç‚¹çš„ç®—æ³•ã€‚ä½¿ç”¨å±€éƒ¨ç¦»ç¾¤å› å­, å°†ä¸€ä¸ªç‚¹çš„å±€éƒ¨å¯†åº¦ä¸ å…¶ä»–é‚»åŸŸè¿›è¡Œæ¯”è¾ƒã€‚å¦‚æœå‰è€…è¿œå°äºåè€… ( LOF $>1$ ), é‚£ä¹ˆæ”¹ç‚¹ç›¸å¯¹äºå…¶ä»–é‚»åŸŸåœ¨ä¸€ä¸ªå¯†åº¦è¾ƒä¸ºç¨€ç–çš„ä½ç½®, å³è§†ä¸ºç¦»ç¾¤ç‚¹ã€‚LOF çš„å±€é™æ€§åœ¨äºåªé€‚åˆç”¨äºæ•°å€¼å‹æ•°æ®ã€‚
    - å…¶ä½¿ç”¨çš„å‡½æ•°ä¸º `lofactor()`, æ‰€åœ¨çš„åŒ…ä¸º DMwR å’Œ dprepã€‚

- å¤„ç†æ–¹å¼ï¼š

  - åˆ¤å®šä¸ºç¼ºå¤±ï¼Œç„¶åèµ°ç¼ºå¤±çš„å¤„ç†æ–¹å¼

  - ä¸å¤„ç†

  - Winsorizingï¼š limiting extreme values in the statistical data to reduce the effect of possibly spurious outliers

    ```python
    from scipy.stats.mstats import winsorize
    winsorize([92, 19, 101, 58, 1053, 91, 26, 78, 10, 13, -40, 101, 86, 85, 15, 89, 89, 28, -5, 41], limits=[0.05, 0.05])
    ```

    


#### å¤„ç†æ ·æœ¬ä¸å¹³è¡¡

**Change data**

- Random Undersampling

- Random Oversampling

- Ensemble Resampling

  - A random re-sample of majority class is used for training each instance in an ensemble
  - The minority class is retained while training the instance.

- Synthetic Minority Oversampling Technique (SMOTE)

  - SMOTEå…¨ç§°æ˜¯Synthetic Minority Oversampling Techniqueå³åˆæˆå°‘æ•°ç±»è¿‡é‡‡æ ·æŠ€æœ¯ï¼Œå®ƒæ˜¯åŸºäºéšæœºè¿‡é‡‡æ ·ç®—æ³•çš„ä¸€ç§æ”¹è¿›æ–¹æ¡ˆï¼Œç”± äºéšæœºè¿‡é‡‡æ ·é‡‡å–ç®€å•å¤åˆ¶æ ·æœ¬çš„ç­–ç•¥æ¥å¢åŠ å°‘æ•°ç±»æ ·æœ¬, è¿™æ ·å®¹æ˜“äº§ç”Ÿæ¨¡å‹è¿‡æ‹Ÿåˆçš„é—®é¢˜, å³ä½¿å¾—æ¨¡å‹å­¦ä¹ åˆ°çš„ä¿¡æ¯è¿‡äºç‰¹åˆ« (Specific)è€Œä¸å¤Ÿæ³›åŒ–(General), SMOTEç®—æ³•çš„åŸºæœ¬æ€æƒ³æ˜¯å¯¹å°‘æ•°ç±»æ ·æœ¬è¿›è¡Œåˆ†æå¹¶æ ¹æ®å°‘æ•°ç±»æ ·æœ¬äººå·¥åˆæˆæ–°æ ·æœ¬æ·»åŠ åˆ°æ•°æ®é›†ä¸­
    - ç®—æ³•æµç¨‹ï¼š
      1ã€å¯¹äºå°‘æ•°ç±»ä¸­æ¯ä¸€ä¸ªæ ·æœ¬x, ä»¥æ¬§æ°è·ç¦»ä¸ºæ ‡å‡†è®¡ç®—å®ƒåˆ°å°‘æ•°ç±»æ ·æœ¬é›†ä¸­æ‰€æœ‰æ ·æœ¬çš„è·ç¦», å¾—åˆ°å…¶ $k$ è¿‘é‚»ã€‚
      2ã€æ ¹æ®æ ·æœ¬ä¸å¹³è¡¡æ¯”ä¾‹è®¾ç½®ä¸€ä¸ªé‡‡æ ·æ¯”ä¾‹ä»¥ç¡®å®šé‡‡æ ·å€ç‡ $\mathrm{N}$, å¯¹äºæ¯ä¸€ä¸ªå°‘æ•°ç±»æ ·æœ¬ $x$, ä»å…¶ $k$ è¿‘é‚»ä¸­éšæœºé€‰æ‹©è‹¥å¹²ä¸ªæ ·æœ¬, å‡è®¾é€‰æ‹©çš„ è¿‘é‚»ä¸º $x n^2$ ã€‚
      3ã€å¯¹äºæ¯ä¸€ä¸ªéšæœºé€‰å‡ºçš„è¿‘é‚» xn, åˆ†åˆ«ä¸åŸæ ·æœ¬æŒ‰ç…§å¦‚ä¸‹çš„å…¬å¼æ„å»ºæ–°çš„æ ·æœ¬ $x$ new $=x+\operatorname{rand}(0,1) *|x-x n|$
  
  
  <img src="../images/image-20220726111730712.png" alt="image-20220726111730712" style="width:50%;" />
  
  - Synthetic Minority Oversampling Technique (SMOTE) is a popular method to handle training with imbalanced datasets
  - SMOTE **adds synthetic interpolated sample**s to minority class
  - The following procedure is repeated for every original data point in minority class:
    - Pick a neighbor from $k$ nearest neighbors
    - **Sample a point randomly from the line** joining the two data points.
    - Add the point to the minority class
  - Leads to large datasets (due to oversampling)

**Change training procedure**

- assighing Class weightsï¼šmake sure the penalty of predicting minority wrong is more high!
  - Reweight each sample during training
    - Modify the loss function to account for class weights
    - Similar effect as oversampling (except that this is not random)
- ä¿®æ”¹æ¨¡å‹çš„æŸå¤±å‡½æ•°ï¼šä¸æ˜¯F1äº†è€Œæ˜¯æ›´ä¸åŒçš„

**é‡æ–°é€‰æ‹©è¯„ä»·æŒ‡æ ‡**ï¼š

- AP

**é‡æ„é—®é¢˜**

- ä»”ç»†å¯¹ä½ çš„é—®é¢˜è¿›è¡Œåˆ†æä¸æŒ–æ˜ï¼Œæ˜¯å¦å¯ä»¥å°†ä½ çš„é—®é¢˜åˆ’åˆ†æˆå¤šä¸ªæ›´å°çš„é—®é¢˜ï¼Œè€Œè¿™äº›å°é—®é¢˜æ›´å®¹æ˜“è§£å†³ã€‚



### æ¨¡å‹è®­ç»ƒæ­¥éª¤

#### Development-test split

- Random split

  - æ¯”ä¾‹å–å†³äºå®é™…é—®é¢˜
    - Large Sample Sizeï¼šä¸¤è¾¹éƒ½å¤Ÿ éšä¾¿
    - è®­ç»ƒé›†å°çš„ æ¯”å¦‚åªæœ‰100ä¸ªçš„æ—¶å€™å¯èƒ½éœ€è¦put asideå°‘ä¸€ç‚¹
    - æœ‰æ—¶å€™å¤ªå¤šäº†ï¼Œåªéœ€è¦è®­ç»ƒ50%çš„æ•°æ®å°±å¤Ÿäº†æ¥èŠ‚çœæ—¶é—´ï¼Œåé¢æ‹¿50%å»æµ‹è¯•ï¼Œtraining procures is shorten without comprimising the quality of model
  - æœ€åè¾“å‡ºå„ä¸ªtargetçš„ä¸ä¸€å®šæ˜¯å æ¯”ä¸€æ ·çš„

- Stratiï¬ed Splitting

  - The stratiï¬ed splitting ensures that the **ratio of classes in development** and **test datasets** equals that of the original dataset. 

  - Generally employed when performing classiï¬cation tasks on highly imbalanced datasets

  - indexæ˜¯class

    <img src="../images/(null)-20220724221510415.(null)" alt="img" style="width: 33%;" />

  - æ˜¯SK learnçš„é»˜è®¤å€¼ï¼

- Structured Splitting
  - The structured splitting is generally employed to prevent data leakage.
  - Examplesï¼šStock price predictionsã€Time-series predictions

<img src="../images/(null)-20220724221513024.(null)" alt="img" style="width: 33%;" />

#### Hyper-parameter tuning

æ ¸å¿ƒç›®æ ‡ï¼šTraining data â‡’ Select Best Parameters

<img src="../images/(null)-20220724221503960.(null)" alt="img" style="width:25%;" />

å‚æ•°å’Œè¶…å‚æ•°çš„åŒºåˆ«ï¼šparameteræ˜¯learn from dataçš„ hyperparameteræ˜¯ä½ å®šçš„

- ä¹Ÿå¯ä»¥è®©æ•°æ®å‡ºhyperparameterï¼Œä½†è¿™æ ·çš„è¯å°±optimization problemä¼šå˜å¾—å¤æ‚ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªç®€å•çš„å¯ä»¥solveçš„convex optimizationï¼Œæ‰€ä»¥æˆ‘ä»¬ä¼šfixå®ƒ

##### Bias-Variance

è¶…å‚çš„æ³¨æ„äº‹é¡¹â€”â€”æ³¨æ„å¤æ‚åº¦

- å¯¹æ¨¡å‹å¤æ‚åº¦çš„ç†è§£ï¼šå¯¹æ¨¡å‹å˜å¤æ‚ï¼Œæˆ‘ä»¬åœ¨åšBias-Varianceçš„Tradeoï¬€.æ¨¡å‹çš„é¢„æµ‹è¯¯å·®å¯ä»¥åˆ†è§£ä¸ºä¸‰ä¸ªéƒ¨åˆ†: åå·®(bias)ï¼Œ æ–¹å·®(variance) å’Œå™ªå£°(noise).

  - the conflict in trying to simultaneously minimize these two sources of [error](https://en.wikipedia.org/wiki/Errors_and_residuals_in_statistics) that prevent supervised learning algorithms from generalizing beyond their training set
    - **The** ***[bias](https://en.wikipedia.org/wiki/Bias_of_an_estimator)*** **error** is an error from erroneous assumptions in the learning [algorithm](https://en.wikipedia.org/wiki/Algorithm). High bias can cause an algorithm to *miss the relevant relations between features and target outputs* (underfitting). åå·®åº¦é‡äº†æ¨¡å‹çš„æœŸæœ›é¢„æµ‹ä¸çœŸå®ç»“æœçš„åç¦»ç¨‹åº¦ï¼Œ å³åˆ»ç”»äº†å­¦ä¹ ç®—æ³•æœ¬èº«çš„æ‹Ÿåˆèƒ½åŠ›ã€‚åå·®åˆ™è¡¨ç°ä¸ºåœ¨ç‰¹å®šåˆ†å¸ƒä¸Šçš„é€‚åº”èƒ½åŠ›ï¼Œåå·®è¶Šå¤§è¶Šåç¦»çœŸå®å€¼ã€‚

    - **The** ***[variance](https://en.wikipedia.org/wiki/Variance)*** is an error from **sensitivity** to **small fluctuations in the training set**. High variance may result from an algorithm modeling the random [noise](https://en.wikipedia.org/wiki/Noise_(signal_processing)) in the training data ([overfitting](https://en.wikipedia.org/wiki/Overfitting)). æ–¹å·®åº¦é‡äº†åŒæ ·å¤§å°çš„è®­ç»ƒé›†çš„å˜åŠ¨æ‰€å¯¼è‡´çš„å­¦ä¹ æ€§èƒ½çš„å˜åŒ–ï¼Œ å³åˆ»ç”»äº†æ•°æ®æ‰°åŠ¨æ‰€é€ æˆçš„å½±å“ã€‚æ–¹å·®è¶Šå¤§ï¼Œè¯´æ˜æ•°æ®åˆ†å¸ƒè¶Šåˆ†æ•£

    - å™ªå£°ï¼šå™ªå£°è¡¨è¾¾äº†åœ¨å½“å‰ä»»åŠ¡ä¸Šä»»ä½•æ¨¡å‹æ‰€èƒ½è¾¾åˆ°çš„æœŸæœ›æ³›åŒ–è¯¯å·®çš„ä¸‹ç•Œï¼Œ å³åˆ»ç”»äº†å­¦ä¹ é—®é¢˜æœ¬èº«çš„éš¾åº¦ ã€‚

      <img src="../images/(null)-20220724221505235.(null)" alt="img" style="width:45%;" /><img src="../images/(null)-20220724221443081.(null)" alt="img" style="width:45%;" />

      - æˆ‘ä»¬æƒ³è¦å·¦ä¸Šè§’ï¼šéƒ½å¾ˆå‡†ç¡®

##### æ¬ æ‹Ÿå’Œé—®é¢˜

underfittingæ˜¯low variance high biasï¼šæ²¡æœ‰varianceä½†éƒ½é¢„æµ‹å‡ºåå·®äº† 

- å½“ç®—æ³•ä»æ•°æ®é›†å­¦ä¹ çœŸå®ä¿¡å·çš„**çµæ´»æ€§æœ‰é™**æ—¶ï¼Œå°±ä¼šå‡ºç°åå·®ã€‚( æƒ³çš„å¤ªè¿‡ç®€å•ï¼Œæ¬ æ‹Ÿåˆ), æ‰€ä»¥æ¨¡å‹æ•´ä½“äº§ç”Ÿåå·®ã€‚
- æ¬ æ‹ŸåˆæŒ‡çš„æ˜¯æ¨¡å‹æ²¡æœ‰å¾ˆå¥½åœ°å­¦ä¹ åˆ°æ•°æ®ç‰¹å¾ï¼Œä¸èƒ½å¤Ÿå¾ˆå¥½åœ°æ‹Ÿåˆæ•°æ®ï¼Œåœ¨è®­ç»ƒæ•°æ®å’ŒæœªçŸ¥æ•°æ®ä¸Šè¡¨ç°éƒ½å¾ˆå·®ã€‚
- æ¬ æ‹Ÿåˆçš„åŸå› åœ¨äºï¼š
  - ç‰¹å¾é‡è¿‡å°‘ï¼›
  - æ¨¡å‹å¤æ‚åº¦è¿‡ä½
- è§£å†³ï¼š
  - å¢åŠ æ–°ç‰¹å¾ï¼Œå¯ä»¥è€ƒè™‘åŠ å…¥è¿›ç‰¹å¾ç»„åˆã€é«˜æ¬¡ç‰¹å¾ï¼Œæ¥å¢å¤§å‡è®¾ç©ºé—´ï¼›

  - æ·»åŠ **å¤šé¡¹å¼ç‰¹å¾**ï¼Œè¿™ä¸ªåœ¨æœºå™¨å­¦ä¹ ç®—æ³•é‡Œé¢ç”¨çš„å¾ˆæ™®éï¼Œä¾‹å¦‚å°†çº¿æ€§æ¨¡å‹é€šè¿‡æ·»åŠ äºŒæ¬¡é¡¹æˆ–è€…ä¸‰æ¬¡é¡¹ä½¿æ¨¡å‹**æ³›åŒ–èƒ½åŠ›æ›´å¼º**ï¼›

  - **å‡å°‘æ­£åˆ™åŒ–å‚æ•°**ï¼Œæ­£åˆ™åŒ–çš„ç›®çš„æ˜¯ç”¨æ¥é˜²æ­¢è¿‡æ‹Ÿåˆçš„ï¼Œä½†æ˜¯æ¨¡å‹å‡ºç°äº†æ¬ æ‹Ÿåˆï¼Œåˆ™éœ€è¦å‡å°‘æ­£åˆ™åŒ–å‚æ•°ï¼›
  - ä½¿ç”¨**éçº¿æ€§æ¨¡å‹**ï¼Œæ¯”å¦‚æ ¸SVM ã€å†³ç­–æ ‘ã€æ·±åº¦å­¦ä¹ ç­‰æ¨¡å‹ï¼›
  - è°ƒæ•´**æ¨¡å‹çš„å®¹é‡(capacity)**ï¼Œé€šä¿—åœ°ï¼Œæ¨¡å‹çš„å®¹é‡æ˜¯æŒ‡å…¶æ‹Ÿåˆå„ç§å‡½æ•°çš„èƒ½åŠ›ï¼›
  - å®¹é‡ä½çš„æ¨¡å‹å¯èƒ½å¾ˆéš¾æ‹Ÿåˆè®­ç»ƒé›†ã€‚

##### **è¿‡æ‹Ÿåˆé—®é¢˜**

<img src="../images/(null)-20220724221442702.(null)" alt="img" style="width: 50%;" />


- overfittingæ˜¯high variance low biasï¼šå¹³å‡æ¥çœ‹çš„è¯ æ˜¯center the plot means doing well! ä½†varianceéå¸¸é«˜

    - å¤ªå…³æ³¨è®­ç»ƒé›†ä¸­ä¸ªä½“æ³¢åŠ¨ï¼Œè¿‡æ‹Ÿåˆ
    - é«˜æ–¹å·®æ¨¡å‹ï¼Œå¯¹ç‰¹å®šè®­ç»ƒæ•°æ®é›†çš„çµæ´»æ€§æé«˜ã€‚
    - é«˜æ–¹å·®æ¨¡å‹éå¸¸å…³æ³¨è®­ç»ƒæ•°æ®ï¼Œè€Œå¯¹ä»¥å‰æ²¡æœ‰è§è¿‡çš„æ•°æ®ä¸è¿›è¡Œæ³›åŒ–generalizabilityã€‚å› æ­¤ï¼Œè¿™æ ·çš„æ¨¡å‹åœ¨è®­ç»ƒæ•°æ®ä¸Šè¡¨ç°å¾—å¾ˆå¥½ï¼Œä½†åœ¨æµ‹è¯•æ•°æ®ä¸Šå´æœ‰å¾ˆé«˜çš„é”™è¯¯ç‡ã€‚

- è¿‡æ‹Ÿåˆçš„åŸå› åœ¨äºï¼š

  æœºå™¨å­¦ä¹ ç®—æ³•ä¸ºäº†æ»¡è¶³å°½å¯èƒ½å¤æ‚çš„ä»»åŠ¡, å…¶æ¨¡å‹çš„æ‹Ÿåˆ èƒ½åŠ›ä¸€èˆ¬è¿œè¿œé«˜äºé—®é¢˜å¤æ‚åº¦, ä¹Ÿå°±æ˜¯è¯´, æœºå™¨å­¦ä¹ ç®—æ³•æœ‰ã€Œæ‹Ÿåˆå‡ºæ­£ç¡®è§„åˆ™çš„å‰æä¸‹, è¿›ä¸€æ­¥æ‹Ÿ åˆå™ªå£°ã€çš„èƒ½åŠ›ã€‚

  - **å‚æ•°å¤ªå¤š**ï¼Œæ¨¡å‹å¤æ‚åº¦è¿‡é«˜ï¼›
  - å»ºæ¨¡**æ ·æœ¬é€‰å–æœ‰è¯¯**ï¼Œå¯¼è‡´é€‰å–çš„æ ·æœ¬æ•°æ®ä¸è¶³ä»¥ä»£è¡¨é¢„å®šçš„åˆ†ç±»è§„åˆ™ï¼›
  - **æ ·æœ¬å™ªéŸ³å¹²æ‰°è¿‡å¤§**ï¼Œä½¿å¾—æœºå™¨å°†éƒ¨åˆ†å™ªéŸ³è®¤ä¸ºæ˜¯ç‰¹å¾ä»è€Œæ‰°ä¹±äº†é¢„è®¾çš„åˆ†ç±»è§„åˆ™ï¼›
  - å‡è®¾çš„**æ¨¡å‹æ— æ³•åˆç†å­˜åœ¨**ï¼Œæˆ–è€…è¯´æ˜¯å‡è®¾æˆç«‹çš„æ¡ä»¶å®é™…å¹¶ä¸æˆç«‹ã€‚

- æ€ä¹ˆè§£å†³è¿‡æ‹Ÿåˆï¼ˆé‡ç‚¹ï¼‰ğŸŒŸğŸŒŸğŸŒŸ
  - è·å–å’Œä½¿ç”¨**æ›´å¤šçš„æ•°æ®**ï¼ˆæ•°æ®é›†å¢å¼ºï¼‰â€”â€”è§£å†³è¿‡æ‹Ÿåˆçš„æ ¹æœ¬æ€§æ–¹æ³•

  - æ¸…æ´—æ•°æ®ï¼šæŠŠæ˜æ˜¾å¼‚å¸¸çš„æ•°æ®å‰”é™¤

    - å¢åŠ å™ªå£°ï¼šå¹³è¡¡å¼‚å¸¸å€¼

  - **ç‰¹å¾é™ç»´**

    - å‡å°‘ç‰¹å¾
    - PCAé™ç»´ç­‰

  - åŠ å…¥l1 / l2æ­£åˆ™åŒ–ï¼Œæ§åˆ¶æ¨¡å‹çš„å¤æ‚åº¦
    - å› ä¸ºå‚æ•°çš„ç¨€ç–ï¼Œåœ¨ä¸€å®šç¨‹åº¦ä¸Šå®ç°äº†ç‰¹å¾çš„é€‰æ‹©ã€‚
    - ä¸ºä»€ä¹ˆå‚æ•°è¶Šå°ä»£è¡¨æ¨¡å‹è¶Šç®€å•ï¼Ÿ
      - è¶Šå¤æ‚çš„æ¨¡å‹ï¼Œè¶Šä¼šå°è¯•å¯¹æ‰€æœ‰çš„æ ·æœ¬è¿›è¡Œæ‹Ÿåˆï¼Œç”šè‡³åŒ…æ‹¬ä¸€äº›å¼‚å¸¸æ ·æœ¬ç‚¹ï¼Œè¿™å°±å®¹æ˜“é€ æˆåœ¨è¾ƒå°çš„åŒºé—´é‡Œé¢„æµ‹å€¼äº§ç”Ÿè¾ƒå¤§çš„æ³¢åŠ¨
      - è¿™ç§è¾ƒå¤§çš„æ³¢åŠ¨ä¹Ÿåæ˜ äº†åœ¨è¿™ä¸ªåŒºé—´é‡Œçš„å¯¼æ•°å¾ˆå¤§ï¼Œè€Œ**åªæœ‰è¾ƒå¤§çš„å‚æ•°å€¼æ‰èƒ½äº§ç”Ÿè¾ƒå¤§çš„å¯¼æ•°ã€‚å› æ­¤å¤æ‚çš„æ¨¡å‹ï¼Œå…¶å‚æ•°å€¼ä¼šæ¯”è¾ƒå¤§**ã€‚ å› æ­¤å‚æ•°è¶Šå°‘ä»£è¡¨æ¨¡å‹è¶Šç®€å•

  - äº¤å‰éªŒè¯æ¥è¿›è¡Œæ¨¡å‹é€‰æ‹©

  - æ·±åº¦å­¦ä¹ æ¨¡å‹**Dropout** / **Early stopping **/Batch Normalization

    - dropout æ˜¯ä¸€ç§é¿å…ç¥ç»ç½‘ç»œè¿‡æ‹Ÿåˆçš„æ­£åˆ™åŒ–æŠ€æœ¯ã€‚åƒL1å’ŒL2è¿™æ ·çš„æ­£åˆ™åŒ–æŠ€æœ¯é€šè¿‡ä¿®æ”¹ä»£ä»·å‡½æ•°æ¥å‡å°‘è¿‡æ‹Ÿåˆã€‚è€Œä¸¢å¼ƒæ³•ä¿®æ”¹ç¥ç»ç½‘ç»œæœ¬èº«ã€‚å®ƒåœ¨è®­ç»ƒçš„æ¯ä¸€æ¬¡è¿­ä»£è¿‡ç¨‹ä¸­éšæœºåœ°ä¸¢å¼ƒç¥ç»ç½‘ç»œä¸­çš„ç¥ç»å…ƒã€‚å½“æˆ‘ä»¬ä¸¢å¼ƒä¸åŒç¥ç»å…ƒé›†åˆçš„æ—¶å€™ï¼Œå°±ç­‰åŒäºè®­ç»ƒä¸åŒçš„ç¥ç»ç½‘ç»œã€‚ä¸åŒçš„ç¥ç»ç½‘ç»œä¼šä»¥ä¸åŒçš„æ–¹å¼å‘ç”Ÿè¿‡æ‹Ÿåˆï¼Œæ‰€ä»¥ä¸¢å¼ƒçš„å‡€æ•ˆåº”å°†ä¼šå‡å°‘è¿‡æ‹Ÿåˆçš„å‘ç”Ÿã€‚

      ![img](../images/1433301-20200930170004003-479276499.png)

      å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œä¸¢å¼ƒæ³•è¢«ç”¨äºåœ¨è®­ç»ƒç¥ç»ç½‘ç»œçš„è¿‡ç¨‹ä¸­éšæœºä¸¢å¼ƒç¥ç»ç½‘ç»œä¸­çš„ç¥ç»å…ƒã€‚è¿™ç§æŠ€æœ¯è¢«è¯æ˜å¯ä»¥å‡å°‘å¾ˆå¤šé—®é¢˜çš„è¿‡æ‹Ÿåˆï¼Œè¿™äº›é—®é¢˜åŒ…æ‹¬å›¾åƒåˆ†ç±»ã€å›¾åƒåˆ‡å‰²ã€è¯åµŒå…¥ã€è¯­ä¹‰åŒ¹é…ç­‰é—®é¢˜ã€‚

##### è¶…å‚æ•°æœç´¢æ–¹æ³•

<img src="../images/(null)-20220724221446494.(null)" alt="img" style="width:33%;" />

- Grid: there are really three distinct values of one parameter and three distinct values of another parameter, so there are only 3 x 3, 6 different values tried.
- Random: probably there are nine different values, and in this case there are nine different values. when you actually doing random search, you're actually **trying different values more than the search itself, which has a finite set of values**

  - å¥½å¤„ï¼š**å¯¹dominatingçš„parameterå¯ä»¥å°è¯•æ›´å¤šçš„å€¼**

- Bayesian optimizationï¼šgiven search, figure out the best next point to search 

  - Bayesian optimization works by constructing a probability distribution of possible functions (gaussian process) that best describe the function you want to optimize.

    - Gaussian processæŠŠæ‰€æœ‰æœç´¢è¿‡çš„ç‚¹æ‹Ÿåˆæˆä¸€ä¸ªå‡½æ•°

  - A utility function helps explore the parameter space by trading between exploration and exploitation.

  - The probability distribution of functions is *updated (bayesian) based on observations so far.*

  - åŒºåˆ«ï¼šä¸æ˜¯pre determinedçš„ï¼Œç„¶ågrid å’Œrandomä¸éœ€è¦ä½¿ç”¨æˆ‘ä»¬è¾“å…¥çš„ç»“æœ

- ä¸¤è€…ç»“åˆâ€”â€”Evolutionary optimization

- è¶…å‚é€‰æ‹©æ–¹æ³•ï¼ˆmodel selectionï¼‰ï¼šå¯¹æ¯ä¸ªè¶…å‚strategyï¼Œæˆ‘ä»¬éœ€è¦çŸ¥é“è¿™ä¸ªè¶…å‚æ•°è¡¨ç°æ€ä¹ˆæ ·

  - å¦‚æœä½¿ç”¨test dataçš„è¯ï¼Œä¼šå¯¼è‡´overfittingï¼Œæ‰€ä»¥æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªvalidation datasetæ¥è¡¡é‡effectiveness of a hyperparameter valueï¼Œä»è€Œå®ç°model selectionï¼

  - Three-way holdoutï¼šè·Ÿä¹‹å‰çš„split testsetçš„æ–¹æ³•ä¸€æ ·do another splitï¼Œå¯ä»¥random stratifiedä¹‹ç±»çš„

    - æ•ˆæœï¼šgive reasonable approximation of test performance on large balanced datasets 

  - K-fold cross validation (CV)ï¼š æ•°æ®åˆ†æˆkä»½ï¼Œæ‰§è¡Œkæ¬¡ï¼ˆk-1ä»½å½“æ¨¡å‹ å‰©ä¸€ä»½è¯„ä¼°ï¼‰â‡’å¹³å‡è¡¨ç°

<img src="../images/(null)-20220724221503787.(null)" alt="img" style="width:50%;" />

- Leave-one-out CVï¼šk = nï¼Œ**æ‰€æœ‰çš„æ ·æœ¬éƒ½å•ç‹¬è¢«æ‹¿èµ°ä¸€æ¬¡**
  - High variance é€‚ç”¨äºå°æ•°æ®ï¼

- Repeated stratiï¬ed K-fold CVï¼šK-foldçš„åŸºç¡€ä¸Š æ¯æ¬¡development data is *shuffled* before creating the training & validation datasets

- Stratiï¬ed K-fold CV

  <img src="../images/(null)-20220724221510330.(null)" alt="img" style="width:33%;" />

  -  Stratiï¬ed sampling is used when working with highly imbalanced datasets ï¼


-  Random permutation CVï¼šgenerate a user defined number of independent train / test dataset splits. Samples are first shuffled and then split into a pair of train and test sets.

  - æ‰€ä»¥ä¼šæ˜¯ä¹±çš„ï¼

#### Features selection

å½“æ•°æ®é¢„å¤„ç†å®Œæˆåï¼Œæˆ‘ä»¬éœ€è¦é€‰æ‹©å¯¹äºå½“å‰å­¦ä¹ ä»»åŠ¡æœ‰ç”¨çš„å±æ€§ç‰¹å¾æ¥è¿›è¡Œè®­ç»ƒï¼Œè¿™ç±»å±æ€§ç§°ä¸ºâ€œç›¸å…³ç‰¹å¾â€ï¼Œè€Œå‰©ä¸‹æ²¡ä»€ä¹ˆç”¨çš„å±æ€§ç§°ä¸ºâ€œæ— å…³ç‰¹å¾â€ã€‚å…¶å®ï¼Œç‰¹å¾çš„ä½¿ç”¨æ–¹æ¡ˆä¹Ÿå¯ä»¥è®¤ä¸ºæ˜¯ä¸€ç§è¶…å‚ï¼Œç„¶åæ ¹æ®å‡†ç¡®ç‡çš„ç»“æœæ¥é€‰æ‹©ï¼Œèµ°ä¸Šä¸€æ­¥çš„æ¡†æ¶ã€‚ä¹Ÿæœ‰å…¶ä»–çš„ç‰¹å¾é€‰æ‹©æ–¹æ³•â¬‡ï¸

**ç‰¹å¾é€‰æ‹©çš„ä¸»è¦åŸå› **ï¼š

- åœ¨ç°å®ä»»åŠ¡ä¸­ç»å¸¸ä¼šé‡åˆ°ç»´æ•°ç¾éš¾é—®é¢˜ï¼Œè¿™æ˜¯ç”±äºå±æ€§è¿‡å¤šè€Œé€ æˆçš„ï¼Œè‹¥èƒ½ä»ä¸­é€‰æ‹©å‡ºé‡è¦ç‰¹å¾ï¼Œä½¿å¾—åç»­å­¦ä¹ è¿‡ç¨‹ä»…éœ€åœ¨ä¸€éƒ¨åˆ†ç‰¹å¾ä¸Šæ„å»ºæ¨¡å‹ï¼Œåˆ™ç»´æ•°ç¾éš¾é—®é¢˜ä¼šå¤§ä¸ºå‡è½»ã€‚
- å»é™¤ä¸ç›¸å…³ç‰¹å¾å¾€å¾€ä¼šé™ä½å­¦ä¹ ä»»åŠ¡çš„éš¾åº¦ï¼Œè¿™å°±åƒä¾¦æ¢ç ´æ¡ˆä¸€æ ·ï¼ŒæŠ½ä¸å‰¥èŒ§ååªç•™ä¸‹å…³é”®å› ç´ ï¼Œåˆ™çœŸç›¸å¾€å¾€æ›´å®¹æ˜“çœ‹æ¸…ã€‚

**æ³¨æ„äº‹é¡¹ï¼š**ç‰¹å¾é€‰æ‹©è¿‡ç¨‹å¿…é¡»ç¡®ä¿ä¸ä¸¢å¤±é‡è¦ç‰¹å¾ï¼Œå¦åˆ™åç»­å­¦ä¹ è¿‡ç¨‹ä¼šå› ä¸ºé‡è¦ä¿¡æ¯çš„ç¼ºå¤±è€Œæ— æ³•è·å¾—å¥½çš„æ€§èƒ½

**è§çš„ç‰¹å¾é€‰æ‹©æ–¹æ³•**

- **è¿‡æ»¤å¼ï¼ˆfilterï¼‰**ï¼Œå…ˆå¯¹æ•°æ®é›†è¿›è¡Œ*ç‰¹å¾é€‰æ‹©ï¼Œå†è®­ç»ƒå­¦ä¹ å™¨*ï¼Œç‰¹å¾é€‰æ‹©ä¸åç»­å­¦ä¹ å™¨æ— å…³ã€‚ä¸€èˆ¬æŒ‰ç…§å‘æ•£æ€§æˆ–è€…ç›¸å…³æ€§å¯¹å„ä¸ªç‰¹å¾è¿›è¡Œè¯„åˆ†ï¼Œè®¾å®šé˜ˆå€¼tï¼Œé€‰æ‹©è¯„åˆ†æ¯”tå¤§çš„ç‰¹å¾ï¼Œæˆ–è€…ç¡®å®šå¾…é€‰æ‹©ç‰¹å¾ä¸ªæ•°kï¼Œé€‰æ‹©ç‰¹å¾è¯„åˆ†æœ€å¤§çš„kä¸ªç‰¹å¾ã€‚è‘—åçš„è¿‡æ»¤å¼ç‰¹å¾é€‰æ‹©æ–¹æ³•å¦‚Relief(Relevant Features)ã€‚
- **åŒ…è£¹å¼ï¼ˆwrapperï¼‰**ï¼Œä¸è¿‡æ»¤å¼ä¸è€ƒè™‘åç»­å­¦ä¹ å™¨ä¸åŒï¼ŒåŒ…è£¹å¼ç›´æ¥**æŠŠæœ€ç»ˆå°†ä½¿ç”¨çš„å­¦ä¹ å™¨çš„æ€§èƒ½ä½œä¸ºç‰¹å¾è‡ªå·±çš„è¯„ä»·å‡†åˆ™**ã€‚æ ¹æ®ç›®æ ‡å‡½æ•°ï¼ˆé€šå¸¸æ˜¯é¢„æµ‹æ•ˆæœè¯„åˆ†ï¼‰ï¼Œæ¯æ¬¡é€‰æ‹©è‹¥å¹²ç‰¹å¾ï¼Œæˆ–è€…æ’é™¤è‹¥å¹²ç‰¹å¾ã€‚å…¸å‹çš„åŒ…è£¹å¼ç‰¹å¾é€‰æ‹©ç®—æ³•å¦‚LVW(Las Vegas Wrapper)ã€‚
  - ä¸€èˆ¬è€Œè¨€ï¼Œç”±äºåŒ…è£¹å¼ç‰¹å¾é€‰æ‹©æ–¹æ³•ç›´æ¥é’ˆå¯¹å­¦ä¹ å™¨è¿›è¡Œä¼˜åŒ–ï¼Œå› æ­¤ä»æœ€ç»ˆå­¦ä¹ å™¨æ€§èƒ½æ¥çœ‹ï¼ŒåŒ…è£¹å¼æ¯”è¿‡æ»¤å¼æ›´å¥½ã€‚ä½†å¦ä¸€æ–¹é¢ï¼Œç”±äºåœ¨ç‰¹å¾é€‰æ‹©è¿‡ç¨‹ä¸­éœ€è¦å¤šæ¬¡è®­ç»ƒå­¦ä¹ å™¨ï¼Œå› æ­¤åŒ…è£¹å¼ç‰¹å¾é€‰æ‹©çš„è®¡ç®—å¼€é”€é€šå¸¸æ¯”è¿‡æ»¤å¼å¤§å¾—å¤šã€‚
  - Permutation Feature Importance

- **åµŒå…¥å¼ï¼ˆembeddingï¼‰**ï¼Œå‰ä¸¤ç§æ–¹æ³•ï¼Œç‰¹å¾é€‰æ‹©å’Œå­¦ä¹ å™¨è®­ç»ƒè¿‡ç¨‹æœ‰æ˜æ˜¾çš„åˆ†åˆ«ï¼Œè€ŒåµŒå…¥å¼åˆ™å°†ä¸¤è€…èä¸ºä¸€ä½“ï¼Œ**åœ¨å­¦ä¹ å™¨è®­ç»ƒè¿‡ç¨‹ä¸­è‡ªåŠ¨åœ°è¿›è¡Œç‰¹å¾é€‰æ‹©**ã€‚ä¸€èˆ¬å…ˆä½¿ç”¨æŸäº›æœºå™¨å­¦ä¹ çš„ç®—æ³•å’Œæ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œå¾—åˆ°å„ä¸ªç‰¹å¾çš„æƒå€¼ç³»æ•°ï¼Œæ ¹æ®ç³»æ•°ä»å¤§åˆ°å°é€‰æ‹©ç‰¹å¾ã€‚ç±»ä¼¼äºFilteræ–¹æ³•ï¼Œä½†æ˜¯æ˜¯é€šè¿‡è®­ç»ƒæ¥ç¡®å®šç‰¹å¾çš„ä¼˜åŠ£ã€‚ä¾‹å¦‚åŸºäºæ ‘æ¨¡å‹çš„ç‰¹å¾é€‰æ‹©æ–¹æ³•ã€‚



#### **Optimal model training, model evaluation**

Development data = Training data + validation data â‡’ Model to evaluate

- The purpose of test dataset is to evaluate the **performance of the ï¬nal optimal model**
- Model evaluation is supposed to give a pulse on how the model would perform in the wild.  (æµ‹è¯•é›†çš„è¡¨ç°æ˜¯ä¸ºäº†è¡¡é‡åœ¨unseen data çš„è¡¨ç°ï¼æµ‹è¯•é›†ç›¸å½“äºæ˜¯ä¸€ä¸ªproxy)
  - è¿™å°±æ˜¯ä¸ºä»€ä¹ˆåœ¨training processæˆ‘ä»¬å®Œå…¨ä¸touch test set

<img src="../images/(null)-20220724221510063.(null)" alt="img" style="width:33%;" /><img src="../images/(null)-20220724221509687.(null)" alt="img" style="width:33%;" />

#### **Model deployment** ï¼š

Date setï¼šTraining data + validation data + Test data â‡’ Deployed model

<img src="../images/(null)-20220724222627619.(null)" alt="img" style="width:33%;" />



## KNN

> https://blog.csdn.net/sinat_30353259/article/details/80901746

A simple **non-parametric** supervised learning methodï¼š Assigns the value of the nearest neighbor(s) to the unseen data point

- Prediction is computationally expensive, while training is trivial
- Generally performs poorly at high dimensions

<img src="../images/(null)-20220724222726246.(null)" alt="img" style="width:25%;" />

è®¡ç®—è¿™ä¸ªç‚¹è·Ÿæ‰€æœ‰ç‚¹çš„è·ç¦»

- K = 1çš„æ—¶å€™ ï¼Œç”¨ç¦»ä»–æœ€è¿‘çš„ä¸€ä¸ªçš„labelæ¥é¢„æµ‹

## æœ´ç´ è´å¶æ–¯

**è´å¶æ–¯åˆ†ç±»å™¨ç›´æ¥ç”¨è´å¶æ–¯å…¬å¼è§£å†³åˆ†ç±»é—®é¢˜**ã€‚å‡è®¾æ ·æœ¬çš„ç‰¹å¾å‘é‡ä¸º$x$ï¼Œç±»åˆ«æ ‡ç­¾ä¸º$y$ï¼Œæ ¹æ®è´å¶æ–¯å…¬å¼ï¼Œæ ·æœ¬å±äºæ¯ä¸ªç±»çš„æ¡ä»¶æ¦‚ç‡ï¼ˆåéªŒæ¦‚ç‡ï¼‰ä¸ºï¼š 
$$
p(y | \mathbf{x})=\frac{p(\mathbf{x} | y) p(y)}{p(\mathbf{x})}
$$
 åˆ†æ¯$p(x)$å¯¹æ‰€æœ‰ç±»éƒ½æ˜¯ç›¸åŒçš„ï¼Œ**åˆ†ç±»çš„è§„åˆ™æ˜¯å°†æ ·æœ¬å½’åˆ°åéªŒæ¦‚ç‡æœ€å¤§çš„é‚£ä¸ªç±»**ï¼Œä¸éœ€è¦è®¡ç®—å‡†ç¡®çš„æ¦‚ç‡å€¼ï¼Œåªéœ€è¦çŸ¥é“å±äºå“ªä¸ªç±»çš„æ¦‚ç‡æœ€å¤§å³å¯ï¼Œè¿™æ ·å¯ä»¥å¿½ç•¥æ‰åˆ†æ¯ã€‚åˆ†ç±»å™¨çš„åˆ¤åˆ«å‡½æ•°ä¸ºï¼š 
$$
\arg \max _{y} p(\mathrm{x} | y) p(y)
$$
åœ¨å®ç°è´å¶æ–¯åˆ†ç±»å™¨æ—¶ï¼Œ**éœ€è¦çŸ¥é“æ¯ä¸ªç±»çš„æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ$p(x|y)$å³å…ˆéªŒæ¦‚ç‡**ã€‚ä¸€èˆ¬å‡è®¾æ ·æœ¬æœä»æ­£æ€åˆ†å¸ƒã€‚è®­ç»ƒæ—¶ç¡®å®šå…ˆéªŒæ¦‚ç‡åˆ†å¸ƒçš„å‚æ•°ï¼Œä¸€èˆ¬ç”¨æœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼Œå³æœ€å¤§åŒ–å¯¹æ•°ä¼¼ç„¶å‡½æ•°ã€‚

**è´å¶æ–¯åˆ†ç±»å™¨æ˜¯ä¸€ç§ç”Ÿæˆæ¨¡å‹ï¼Œå¯ä»¥å¤„ç†å¤šåˆ†ç±»é—®é¢˜ï¼Œæ˜¯ä¸€ç§éçº¿æ€§æ¨¡å‹**

**å·¥ä½œåŸç†**ï¼š

* å‡è®¾ç°åœ¨
  * æœ‰æ ·æœ¬$x=(x_1, x_2, x_3, \dots x_n)$å¾…åˆ†ç±»é¡¹
  * $m$ä¸ªç‰¹å¾$(a_1,a_2,a_3,\dots a_m)$(ç‰¹å¾ç‹¬ç«‹)
  * åˆ†ç±»ç›®æ ‡$Y=\{ y_1ï¼Œy_2ï¼Œy_3ï¼Œ\dots ,y_n\}$

* é‚£ä¹ˆå°±$\max ({P}({y}_1 | {x}), {P}({y}_2 | {x}), {P}({y}_3 | {x}) ,{P}({y_n} | {x}))$æ˜¯æœ€ç»ˆçš„åˆ†ç±»ç±»åˆ«ã€‚
* è€Œ$P(y_i | x)=\frac{P(x | y_i) * P(y_i)}{ P(x)} $ï¼Œå› ä¸º$x$å¯¹äºæ¯ä¸ªåˆ†ç±»ç›®æ ‡æ¥è¯´éƒ½ä¸€æ ·ï¼Œæ‰€ä»¥å°±æ˜¯æ±‚$\max({P}({x}|{y_i})*{P}({y_i}))$
* $P(x | y _i) * P(y_i)=P(y_i) * \prod(P(a_j| y_i))$ï¼Œè€Œå…·ä½“çš„$P(a_j|y_i)$å’Œ$P(y_i)$éƒ½æ˜¯èƒ½ä»è®­ç»ƒæ ·æœ¬ä¸­ç»Ÿè®¡å‡ºæ¥
* ${P}({a_j} | {y_i})$è¡¨ç¤ºè¯¥ç±»åˆ«ä¸‹è¯¥ç‰¹å¾$a_j$å‡ºç°çš„æ¦‚ç‡$P(y_i)$è¡¨ç¤ºå…¨éƒ¨ç±»åˆ«ä¸­è¿™ä¸ªè¿™ä¸ªç±»åˆ«å‡ºç°çš„æ¦‚ç‡,è¿™æ ·å°±èƒ½æ‰¾åˆ°åº”è¯¥å±äºçš„ç±»åˆ«äº†

**æœ´ç´ è´å¶æ–¯å¦‚æ­¤â€œæœ´ç´ â€ï¼Ÿ**

- å› ä¸ºå®ƒ**å‡å®šæ‰€æœ‰çš„ç‰¹å¾åœ¨æ•°æ®é›†ä¸­çš„ä½œç”¨æ˜¯åŒæ ·é‡è¦å’Œç‹¬ç«‹çš„**ã€‚æ­£å¦‚æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™ä¸ªå‡è®¾åœ¨ç°å®ä¸–ç•Œä¸­æ˜¯å¾ˆä¸çœŸå®çš„ï¼Œå› æ­¤ï¼Œè¯´æœ´ç´ è´å¶æ–¯çœŸçš„å¾ˆâ€œæœ´ç´ â€ã€‚ç”¨è´å¶æ–¯å…¬å¼è¡¨è¾¾å¦‚ä¸‹ï¼š

$$
P(Y|X_1, X_2) = \frac{P(X_1|Y) P(X_2|Y) P(Y)}{P(X_1)P(X_2)}
$$
- **è€Œåœ¨å¾ˆå¤šæƒ…å†µä¸‹ï¼Œæ‰€æœ‰å˜é‡å‡ ä¹ä¸å¯èƒ½æ»¡è¶³ä¸¤ä¸¤ä¹‹é—´çš„æ¡ä»¶ã€‚**
- æœ´ç´ è´å¶æ–¯æ¨¡å‹(Naive Bayesian Model)çš„æœ´ç´ (Naive)çš„å«ä¹‰æ˜¯**â€œå¾ˆç®€å•å¾ˆå¤©çœŸâ€**åœ°å‡è®¾æ ·æœ¬ç‰¹å¾å½¼æ­¤ç‹¬ç«‹.è¿™ä¸ªå‡è®¾ç°å®ä¸­åŸºæœ¬ä¸Šä¸å­˜åœ¨ï¼Œä½†ç‰¹å¾ç›¸å…³æ€§å¾ˆå°çš„å®é™…æƒ…å†µè¿˜æ˜¯å¾ˆå¤šçš„ï¼Œæ‰€ä»¥è¿™ä¸ªæ¨¡å‹ä»ç„¶èƒ½å¤Ÿå·¥ä½œå¾—å¾ˆå¥½ã€‚

**æœ´ç´ è´å¶æ–¯ç®—æ³•çš„å‰æå‡è®¾**

* ç‰¹å¾ä¹‹é—´ç›¸äº’ç‹¬ç«‹
* æ¯ä¸ªç‰¹å¾åŒç­‰é‡è¦

**ä¸ºä»€ä¹ˆå±æ€§ç‹¬ç«‹æ€§å‡è®¾åœ¨å®é™…æƒ…å†µä¸­å¾ˆéš¾æˆç«‹ï¼Œä½†æœ´ç´ è´å¶æ–¯ä»èƒ½å–å¾—è¾ƒå¥½çš„æ•ˆæœ?**

* å¯¹äºåˆ†ç±»ä»»åŠ¡æ¥è¯´ï¼Œåªè¦å„ç±»åˆ«çš„æ¡ä»¶æ¦‚ç‡æ’åºæ­£ç¡®ã€æ— éœ€ç²¾å‡†æ¦‚ç‡å€¼å³å¯å¯¼è‡´æ­£ç¡®åˆ†ç±»ï¼›
* å¦‚æœå±æ€§é—´ä¾èµ–å¯¹æ‰€æœ‰ç±»åˆ«å½±å“ç›¸åŒï¼Œæˆ–ä¾èµ–å…³ç³»çš„å½±å“èƒ½ç›¸äº’æŠµæ¶ˆï¼Œåˆ™å±æ€§æ¡ä»¶ç‹¬ç«‹æ€§å‡è®¾åœ¨é™ä½è®¡ç®—å¼€é”€çš„åŒæ—¶ä¸ä¼šå¯¹æ€§èƒ½äº§ç”Ÿè´Ÿé¢å½±å“ã€‚

**æœ´ç´ è´å¶æ–¯ä¸­çš„é›¶æ¦‚ç‡é—®é¢˜**

- **é›¶æ¦‚ç‡é—®é¢˜**ï¼šåœ¨è®¡ç®—å®ä¾‹çš„æ¦‚ç‡æ—¶ï¼Œå¦‚æœæŸä¸ªé‡$x$ï¼Œåœ¨è§‚å¯Ÿæ ·æœ¬åº“(è®­ç»ƒé›†)ä¸­æ²¡æœ‰å‡ºç°è¿‡ï¼Œä¼šå¯¼è‡´æ•´ä¸ªå®ä¾‹çš„æ¦‚ç‡ç»“æœæ˜¯0ã€‚
- **è§£å†³åŠæ³•**ï¼šè‹¥$P(x)$ä¸ºé›¶åˆ™æ— æ³•è®¡ç®—ã€‚ä¸ºäº†è§£å†³é›¶æ¦‚ç‡çš„é—®é¢˜ï¼Œæ³•å›½æ•°å­¦å®¶æ‹‰æ™®æ‹‰æ–¯æœ€æ—©æå‡ºç”¨åŠ 1çš„æ–¹æ³•ä¼°è®¡æ²¡æœ‰å‡ºç°è¿‡çš„ç°è±¡çš„æ¦‚ç‡ï¼Œæ‰€ä»¥åŠ æ³•å¹³æ»‘ä¹Ÿå«åš**æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘**ã€‚
- **ä¸¾ä¸ªæ —å­**ï¼šå‡è®¾åœ¨æ–‡æœ¬åˆ†ç±»ä¸­ï¼Œæœ‰3ä¸ªç±»ï¼Œ$C1ã€C2ã€C3$ï¼Œåœ¨æŒ‡å®šçš„è®­ç»ƒæ ·æœ¬ä¸­ï¼ŒæŸä¸ªè¯è¯­$K1$ï¼Œåœ¨å„ä¸ªç±»ä¸­è§‚æµ‹è®¡æ•°åˆ†åˆ«ä¸º0ï¼Œ990ï¼Œ10ï¼Œ$K1$çš„æ¦‚ç‡ä¸º0ï¼Œ0.99ï¼Œ0.01ï¼Œå¯¹è¿™ä¸‰ä¸ªé‡ä½¿ç”¨æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘çš„è®¡ç®—æ–¹æ³•å¦‚ä¸‹ï¼š

```
1/1003=0.001ï¼Œ
991/1003=0.988ï¼Œ
11/1003=0.011
åœ¨å®é™…çš„ä½¿ç”¨ä¸­ä¹Ÿç»å¸¸ä½¿ç”¨åŠ  lambda(1â‰¥lambdaâ‰¥0)æ¥ä»£æ›¿ç®€å•åŠ 1ã€‚å¦‚æœå¯¹Nä¸ªè®¡æ•°éƒ½åŠ ä¸Šlambdaï¼Œè¿™æ—¶åˆ†æ¯ä¹Ÿè¦è®°å¾—åŠ ä¸ŠN*lambdaã€‚
```

- å°†æœ´ç´ è´å¶æ–¯ä¸­çš„æ‰€æœ‰æ¦‚ç‡è®¡ç®—**åº”ç”¨æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘å³å¯ä»¥è§£å†³é›¶æ¦‚ç‡é—®é¢˜**ã€‚

**æ¦‚ç‡è®¡ç®—çš„ä¸‹æº¢é—®é¢˜**

- **ä¸‹æº¢é—®é¢˜**ï¼šåœ¨æœ´ç´ è´å¶æ–¯çš„è®¡ç®—è¿‡ç¨‹ä¸­ï¼Œéœ€è¦å¯¹ç‰¹å®šåˆ†ç±»ä¸­å„ä¸ªç‰¹å¾å‡ºç°çš„**æ¦‚ç‡è¿›è¡Œè¿ä¹˜ï¼Œå°æ•°ç›¸ä¹˜ï¼Œè¶Šä¹˜è¶Šå°ï¼Œè¿™æ ·å°±é€ æˆäº†ä¸‹æº¢å‡º**ã€‚
  ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå¯¹ä¹˜ç§¯ç»“æœå–è‡ªç„¶å¯¹æ•°ã€‚é€šè¿‡æ±‚å¯¹æ•°å¯ä»¥é¿å…ä¸‹æº¢å‡ºæˆ–è€…æµ®ç‚¹æ•°èˆå…¥å¯¼è‡´çš„é”™è¯¯ã€‚

$$
\prod_{i=x}^{n} p\left(x_{i} | y_{j}\right)
$$
- **è§£å†³åŠæ³•**ï¼šå¯¹å…¶**å–å¯¹æ•°**ï¼š

$$
\log \prod_{i=1}^{n} p\left(x_{i} | y_{j}\right)
$$

$$
=\sum_{i=1}^{n} \log p\left(x_{i} | y_{j}\right)
$$

- â€‹	å°†å°æ•°çš„ä¹˜æ³•æ“ä½œè½¬åŒ–ä¸ºå–å¯¹æ•°åçš„åŠ æ³•æ“ä½œï¼Œè§„é¿äº†å˜ä¸ºé›¶çš„é£é™©åŒæ—¶å¹¶ä¸å½±å“åˆ†ç±»ç»“æœã€‚

**å½“æ•°æ®çš„å±æ€§æ˜¯è¿ç»­å‹å˜é‡æ—¶ï¼Œæœ´ç´ è´å¶æ–¯ç®—æ³•å¦‚ä½•å¤„ç†**

* ç¬¬ä¸€ç§æ–¹æ³•ï¼šæŠŠä¸€ä¸ªè¿ç»­çš„å±æ€§ç¦»æ•£åŒ–ï¼Œç„¶åç”¨ç›¸åº”çš„ç¦»æ•£åŒºé—´æ›¿æ¢è¿ç»­å±æ€§å€¼ã€‚ä½†è¿™ç§æ–¹æ³•ä¸å¥½æ§åˆ¶ç¦»æ•£åŒºé—´åˆ’åˆ†çš„ç²’åº¦ã€‚å¦‚æœç²’åº¦å¤ªç»†ï¼Œå°±ä¼šå› ä¸ºæ¯ä¸ªåŒºé—´å†…è®­ç»ƒè®°å½•å¤ªå°‘è€Œä¸èƒ½å¯¹$P(X|Y)$
  åšå‡ºå¯é çš„ä¼°è®¡ï¼Œå¦‚æœç²’åº¦å¤ªç²—ï¼Œé‚£ä¹ˆæœ‰äº›åŒºé—´å°±ä¼šæœ‰æ¥è‡ªä¸åŒç±»çš„è®°å½•ï¼Œå› æ­¤å¤±å»äº†æ­£ç¡®çš„å†³ç­–è¾¹ç•Œã€‚
* ç¬¬äºŒç§æ–¹æ³•ï¼šå‡è®¾è¿ç»­å˜é‡æœä»æŸç§æ¦‚ç‡åˆ†å¸ƒï¼Œç„¶åä½¿ç”¨è®­ç»ƒæ•°æ®ä¼°è®¡åˆ†å¸ƒçš„å‚æ•°ï¼Œä¾‹å¦‚å¯ä»¥ä½¿ç”¨é«˜æ–¯åˆ†å¸ƒæ¥è¡¨ç¤ºè¿ç»­å±æ€§çš„ç±»æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒã€‚
  * é«˜æ–¯åˆ†å¸ƒæœ‰ä¸¤ä¸ªå‚æ•°ï¼Œå‡å€¼$\mu$å’Œæ–¹å·®$\sigma 2$ï¼Œå¯¹äºæ¯ä¸ªç±»$y_i$ï¼Œå±æ€§$X_i$çš„ç±»æ¡ä»¶æ¦‚ç‡ç­‰äºï¼š
  
  * $$
    P\left(X_{i}=x_{i} | Y=y_{j}\right)=\frac{1}{\sqrt{2 \Pi} \sigma_{i j}^{2}} e^{\frac{\left(x_{i}-\mu_{j}\right)^{2}}{2 \sigma_{i}^{2}}}
    $$
  
    * $\mu_{i j}$ï¼šç±»$y_j$çš„æ‰€æœ‰è®­ç»ƒè®°å½•å…³äº$X_i$çš„æ ·æœ¬å‡å€¼ä¼°è®¡
    * $\sigma_{i j}^{2}$ï¼šç±»$y_j$çš„æ‰€æœ‰è®­ç»ƒè®°å½•å…³äº$X$çš„æ ·æœ¬æ–¹å·®
  
  - é€šè¿‡é«˜æ–¯åˆ†å¸ƒä¼°è®¡å‡ºç±»æ¡ä»¶æ¦‚ç‡ã€‚

**å¸¸ç”¨çš„åˆ†ç±»æ¨¡å‹**

æœ´ç´ è´å¶æ–¯çš„ä¸‰ä¸ªå¸¸ç”¨æ¨¡å‹ï¼šé«˜æ–¯ã€å¤šé¡¹å¼ã€ä¼¯åŠªåˆ©

* é«˜æ–¯æ¨¡å‹ï¼š

  * å¤„ç†åŒ…å«è¿ç»­å‹å˜é‡çš„æ•°æ®ï¼Œä½¿ç”¨é«˜æ–¯åˆ†å¸ƒæ¦‚ç‡å¯†åº¦æ¥è®¡ç®—ç±»çš„æ¡ä»¶æ¦‚ç‡å¯†åº¦

* å¤šé¡¹å¼æ¨¡å‹ï¼š

  * å…¶ä¸­$\alpha$ä¸ºæ‹‰æ™®æ‹‰æ–¯å¹³æ»‘ï¼ŒåŠ å’Œçš„æ˜¯å±æ€§å‡ºç°çš„æ€»æ¬¡æ•°ï¼Œæ¯”å¦‚æ–‡æœ¬åˆ†ç±»é—®é¢˜é‡Œé¢ï¼Œä¸å…‰çœ‹è¯è¯­æ˜¯å¦åœ¨æ–‡æœ¬ä¸­å‡ºç°ï¼Œä¹Ÿå¾—çœ‹å‡ºç°çš„æ¬¡æ•°ã€‚å¦‚æœæ€»è¯æ•°ä¸º$n$ï¼Œå‡ºç°è¯æ•°ä¸º$m$çš„è¯ï¼Œè¯´èµ·æ¥æœ‰ç‚¹åƒæ·éª°å­$n$æ¬¡å‡ºç°$m$æ¬¡è¿™ä¸ªè¯çš„åœºæ™¯ã€‚
    $$
    P\left(x_{i} | y_{k}\right)=\frac{N_{y k_{1}}+\alpha}{N_{y_{k}}+\alpha n}
    $$
    
  * å¤šé¡¹å¼æ¨¡å‹é€‚ç”¨äºç¦»æ•£ç‰¹å¾æƒ…å†µï¼Œåœ¨æ–‡æœ¬é¢†åŸŸåº”ç”¨å¹¿æ³›ï¼Œ å…¶åŸºæœ¬æ€æƒ³æ˜¯ï¼š**æˆ‘ä»¬å°†é‡å¤çš„è¯è¯­è§†ä¸ºå…¶å‡ºç°å¤šæ¬¡**ã€‚
  
* ä¼¯åŠªåˆ©æ¨¡å‹ï¼š

  * ä¼¯åŠªåˆ©æ¨¡å‹ç‰¹å¾çš„å–å€¼ä¸ºå¸ƒå°”å‹ï¼Œå³å‡ºç°ä¸ºtrueæ²¡æœ‰å‡ºç°ä¸ºfalseï¼Œåœ¨æ–‡æœ¬åˆ†ç±»ä¸­ï¼Œå°±æ˜¯ä¸€ä¸ªå•è¯æœ‰æ²¡æœ‰åœ¨ä¸€ä¸ªæ–‡æ¡£ä¸­å‡ºç°ã€‚

  * ä¼¯åŠªåˆ©æ¨¡å‹é€‚ç”¨äºç¦»æ•£ç‰¹å¾æƒ…å†µï¼Œå®ƒå°†é‡å¤çš„è¯è¯­éƒ½è§†ä¸ºåªå‡ºç°ä¸€æ¬¡ã€‚
    $$
    P( 'ä»£å¼€'ï¼Œ 'å‘ç¥¨'ï¼Œ 'å‘ç¥¨'ï¼Œ 'æˆ‘' | S) = P('ä»£å¼€' | S)   P( 'å‘ç¥¨' | S) P('æˆ‘' | S)
    $$
    æˆ‘ä»¬çœ‹åˆ°ï¼Œâ€å‘ç¥¨â€œå‡ºç°äº†ä¸¤æ¬¡ï¼Œä½†æ˜¯æˆ‘ä»¬åªå°†å…¶ç®—ä½œä¸€æ¬¡ã€‚æˆ‘ä»¬çœ‹åˆ°ï¼Œâ€å‘ç¥¨â€œå‡ºç°äº†ä¸¤æ¬¡ï¼Œä½†æ˜¯æˆ‘ä»¬åªå°†å…¶ç®—ä½œä¸€æ¬¡ã€‚

**æœ´ç´ è´å¶æ–¯æ˜¯é«˜åå·®ä½æ–¹å·®**

- åœ¨ç»Ÿè®¡å­¦ä¹ æ¡†æ¶ä¸‹ï¼Œå¤§å®¶åˆ»ç”»æ¨¡å‹å¤æ‚åº¦çš„æ—¶å€™ï¼Œæœ‰è¿™ä¹ˆä¸ªè§‚ç‚¹ï¼Œè®¤ä¸º$Error=Bias +Variance$ã€‚

  * $Error$åæ˜ çš„æ˜¯æ•´ä¸ªæ¨¡å‹çš„å‡†ç¡®åº¦ï¼Œ

  * $Bias$åæ˜ çš„æ˜¯æ¨¡å‹åœ¨æ ·æœ¬ä¸Šçš„è¾“å‡ºä¸çœŸå®å€¼ä¹‹é—´çš„è¯¯å·®ï¼Œå³æ¨¡å‹æœ¬èº«çš„ç²¾å‡†åº¦ï¼Œ

  * $Variance$åæ˜ çš„æ˜¯æ¨¡å‹æ¯ä¸€æ¬¡è¾“å‡ºç»“æœä¸æ¨¡å‹è¾“å‡ºæœŸæœ›(å¹³å‡å€¼)ä¹‹é—´çš„è¯¯å·®ï¼Œå³æ¨¡å‹çš„ç¨³å®šæ€§ï¼Œæ•°æ®æ˜¯å¦é›†ä¸­ã€‚


* å¯¹äºå¤æ‚æ¨¡å‹ï¼Œå……åˆ†æ‹Ÿåˆäº†éƒ¨åˆ†æ•°æ®ï¼Œä½¿å¾—ä»–ä»¬çš„åå·®è¾ƒå°ï¼Œè€Œç”±äºå¯¹éƒ¨åˆ†æ•°æ®çš„è¿‡åº¦æ‹Ÿåˆï¼Œå¯¹äºéƒ¨åˆ†æ•°æ®é¢„æµ‹æ•ˆæœä¸å¥½ï¼Œæ•´ä½“æ¥çœ‹å¯èƒ½å¼•èµ·æ–¹å·®è¾ƒå¤§ã€‚
  * å¯¹äºæœ´ç´ è´å¶æ–¯äº†ã€‚å®ƒç®€å•çš„å‡è®¾äº†å„ä¸ªæ•°æ®ä¹‹é—´æ˜¯æ— å…³çš„ï¼Œæ˜¯ä¸€ä¸ªè¢«ä¸¥é‡ç®€åŒ–äº†çš„æ¨¡å‹ï¼Œç®€å•æ¨¡å‹ä¸å¤æ‚æ¨¡å‹ç›¸åï¼Œå¤§éƒ¨åˆ†åœºåˆåå·®éƒ¨åˆ†å¤§äºæ–¹å·®éƒ¨åˆ†ï¼Œä¹Ÿå°±æ˜¯è¯´é«˜åå·®è€Œä½æ–¹å·®


**æœ´ç´ è´å¶æ–¯ä¸ºä»€ä¹ˆé€‚åˆå¢é‡è®¡ç®—ï¼Ÿ**

- å› ä¸ºæœ´ç´ è´å¶æ–¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®é™…åªéœ€è¦è®¡ç®—å‡ºå„ä¸ªç±»åˆ«çš„æ¦‚ç‡å’Œå„ä¸ªç‰¹å¾çš„ç±»æ¡ä»¶æ¦‚ç‡ï¼Œè¿™äº›æ¦‚ç‡å€¼å¯ä»¥å¿«é€Ÿçš„æ ¹æ®å¢é‡æ•°æ®è¿›è¡Œæ›´æ–°ï¼Œæ— éœ€é‡æ–°å…¨é‡è®­ç»ƒï¼Œæ‰€ä»¥å…¶ååˆ†é€‚åˆå¢é‡è®¡ç®—ï¼Œè¯¥ç‰¹æ€§å¯ä»¥ä½¿ç”¨åœ¨è¶…å‡ºå†…å­˜çš„å¤§é‡æ•°æ®è®¡ç®—å’ŒæŒ‰å°æ—¶çº§ç­‰è·å–çš„æ•°æ®è®¡ç®—ä¸­ã€‚

**é«˜åº¦ç›¸å…³çš„ç‰¹å¾å¯¹æœ´ç´ è´å¶æ–¯æœ‰ä»€ä¹ˆå½±å“**

- å‡è®¾æœ‰ä¸¤ä¸ªç‰¹å¾é«˜åº¦ç›¸å…³ï¼Œç›¸å½“äºè¯¥ç‰¹å¾åœ¨æ¨¡å‹ä¸­å‘æŒ¥äº†ä¸¤æ¬¡ä½œç”¨(è®¡ç®—ä¸¤æ¬¡æ¡ä»¶æ¦‚ç‡)ï¼Œä½¿å¾—æœ´ç´ è´å¶æ–¯è·å¾—çš„ç»“æœå‘è¯¥ç‰¹å¾æ‰€å¸Œæœ›çš„æ–¹å‘è¿›è¡Œäº†åç§»ï¼Œå½±å“äº†æœ€ç»ˆç»“æœçš„å‡†ç¡®æ€§ï¼Œæ‰€ä»¥æœ´ç´ è´å¶æ–¯ç®—æ³•åº”å…ˆå¤„ç†ç‰¹å¾ï¼ŒæŠŠç›¸å…³ç‰¹å¾å»æ‰ã€‚

**åº”ç”¨åœºæ™¯**

* **æ–‡æœ¬åˆ†ç±»/åƒåœ¾æ–‡æœ¬è¿‡æ»¤/æƒ…æ„Ÿåˆ¤åˆ«**ï¼š
  è¿™å¤§æ¦‚æ˜¯æœ´ç´ è´å¶æ–¯åº”ç”¨æœ€å¤šçš„åœ°æ–¹äº†ï¼Œå³ä½¿åœ¨ç°åœ¨è¿™ç§åˆ†ç±»å™¨å±‚å‡ºä¸ç©·çš„å¹´ä»£ï¼Œåœ¨æ–‡æœ¬åˆ†ç±»åœºæ™¯ä¸­ï¼Œæœ´ç´ è´å¶æ–¯ä¾æ—§åšæŒºåœ°å æ®ç€ä¸€å¸­ä¹‹åœ°ã€‚å› ä¸ºå¤šåˆ†ç±»å¾ˆç®€å•ï¼ŒåŒæ—¶åœ¨æ–‡æœ¬æ•°æ®ä¸­ï¼Œåˆ†å¸ƒç‹¬ç«‹è¿™ä¸ªå‡è®¾åŸºæœ¬æ˜¯æˆç«‹çš„ã€‚è€Œåƒåœ¾æ–‡æœ¬è¿‡æ»¤(æ¯”å¦‚åƒåœ¾é‚®ä»¶è¯†åˆ«)å’Œæƒ…æ„Ÿåˆ†æ(å¾®åšä¸Šçš„è¤’è´¬æƒ…ç»ª)ç”¨æœ´ç´ è´å¶æ–¯ä¹Ÿé€šå¸¸èƒ½å–å¾—å¾ˆå¥½çš„æ•ˆæœã€‚
* **å¤šåˆ†ç±»å®æ—¶é¢„æµ‹**ï¼š
  å¯¹äºæ–‡æœ¬ç›¸å…³çš„å¤šåˆ†ç±»å®æ—¶é¢„æµ‹ï¼Œå®ƒå› ä¸ºä¸Šé¢æåˆ°çš„ä¼˜ç‚¹ï¼Œè¢«å¹¿æ³›åº”ç”¨ï¼Œç®€å•åˆé«˜æ•ˆã€‚
* **æ¨èç³»ç»Ÿ**ï¼š
  æœ´ç´ è´å¶æ–¯å’ŒååŒè¿‡æ»¤æ˜¯ä¸€å¯¹å¥½æ­æ¡£ï¼ŒååŒè¿‡æ»¤æ˜¯å¼ºç›¸å…³æ€§ï¼Œä½†æ˜¯æ³›åŒ–èƒ½åŠ›ç•¥å¼±ï¼Œæœ´ç´ è´å¶æ–¯å’ŒååŒè¿‡æ»¤ä¸€èµ·ï¼Œèƒ½å¢å¼ºæ¨èçš„è¦†ç›–åº¦å’Œæ•ˆæœã€‚

**ä¼˜ç¼ºç‚¹**

* ç‰¹ç‚¹
  * æœ´ç´ è´å¶æ–¯æ˜¯ä¸€ç§**å¯¹å¼‚å¸¸å€¼ä¸æ•æ„Ÿ**çš„åˆ†ç±»å™¨ï¼Œä¿ç•™æ•°æ®ä¸­çš„å¼‚å¸¸å€¼ï¼Œå¸¸å¸¸å¯ä»¥ä¿æŒè´å¶æ–¯ç®—æ³•çš„æ•´ä½“ç²¾åº¦ï¼Œå¦‚æœå¯¹åŸå§‹æ•°æ®è¿›è¡Œé™å™ªè®­ç»ƒï¼Œåˆ†ç±»å™¨å¯èƒ½ä¼šå› ä¸ºå¤±å»éƒ¨åˆ†å¼‚å¸¸å€¼çš„ä¿¡æ¯è€Œå¯¼è‡´æ³›åŒ–èƒ½åŠ›ä¸‹é™ã€‚
  
* ä¼˜ç‚¹ï¼š
  * å¯¹æ•°æ®çš„è®­ç»ƒå¿«ï¼Œåˆ†ç±»ä¹Ÿå¿«
  * å¯¹ç¼ºå¤±æ•°æ®ä¸å¤ªæ•æ„Ÿï¼Œç®—æ³•ä¹Ÿæ¯”è¾ƒç®€å•
    * æœ´ç´ è´å¶æ–¯æ˜¯ä¸€ç§**å¯¹ç¼ºå¤±å€¼ä¸æ•æ„Ÿ**çš„åˆ†ç±»å™¨ï¼Œæœ´ç´ è´å¶æ–¯ç®—æ³•èƒ½å¤Ÿå¤„ç†ç¼ºå¤±çš„æ•°æ®ï¼Œåœ¨ç®—æ³•çš„å»ºæ¨¡æ—¶å’Œé¢„æµ‹æ—¶æ•°æ®çš„å±æ€§éƒ½æ˜¯å•ç‹¬å¤„ç†çš„ã€‚å› æ­¤**å¦‚æœä¸€ä¸ªæ•°æ®å®ä¾‹ç¼ºå¤±äº†ä¸€ä¸ªå±æ€§çš„æ•°å€¼ï¼Œåœ¨å»ºæ¨¡æ—¶å°†è¢«å¿½ç•¥**ï¼Œä¸å½±å“ç±»æ¡ä»¶æ¦‚ç‡çš„è®¡ç®—ï¼Œåœ¨é¢„æµ‹æ—¶ï¼Œè®¡ç®—æ•°æ®å®ä¾‹æ˜¯å¦å±äºæŸç±»çš„æ¦‚ç‡æ—¶ä¹Ÿå°†å¿½ç•¥ç¼ºå¤±å±æ€§ï¼Œä¸å½±å“æœ€ç»ˆç»“æœã€‚
  * å¯¹å°è§„æ¨¡çš„æ•°æ®è¡¨ç°å¾ˆå¥½ï¼Œèƒ½ä¸ªå¤„ç†å¤šåˆ†ç±»ä»»åŠ¡ï¼Œé€‚åˆå¢é‡å¼è®­ç»ƒï¼Œå°¤å…¶æ˜¯æ•°æ®é‡è¶…å‡ºå†…å­˜æ—¶ï¼Œå¯ä»¥ä¸€æ‰¹æ‰¹çš„å»å¢é‡è®­ç»ƒ
* ç¼ºç‚¹ï¼š
  * å¯¹è¾“å…¥æ•°æ®çš„è¡¨è¾¾å½¢å¼å¾ˆæ•æ„Ÿ
  * ç”±äºæœ´ç´ è´å¶æ–¯çš„â€œæœ´ç´ â€ç‰¹ç‚¹ï¼Œæ‰€ä»¥ä¼šå¸¦æ¥ä¸€äº›å‡†ç¡®ç‡ä¸Šçš„æŸå¤±ã€‚
  * éœ€è¦è®¡ç®—å…ˆéªŒæ¦‚ç‡ï¼Œåˆ†ç±»å†³ç­–å­˜åœ¨é”™è¯¯ç‡ã€‚

**æœ´ç´ è´å¶æ–¯ä¸ LR åŒºåˆ«**ï¼Ÿ

- **æœ´ç´ è´å¶æ–¯æ˜¯ç”Ÿæˆæ¨¡å‹**ï¼Œæ ¹æ®å·²æœ‰æ ·æœ¬è¿›è¡Œè´å¶æ–¯ä¼°è®¡å­¦ä¹ å‡ºå…ˆéªŒæ¦‚ç‡ $P(Y)$ å’Œæ¡ä»¶æ¦‚ç‡ $P(X|Y)$ï¼Œè¿›è€Œæ±‚å‡ºè”åˆåˆ†å¸ƒæ¦‚ç‡ $P(X,Y)$ï¼Œæœ€ååˆ©ç”¨è´å¶æ–¯å®šç†æ±‚è§£$P(Y|X)$ï¼Œ è€Œ**LRæ˜¯åˆ¤åˆ«æ¨¡å‹**ï¼Œæ ¹æ®æå¤§åŒ–å¯¹æ•°ä¼¼ç„¶å‡½æ•°ç›´æ¥æ±‚å‡ºæ¡ä»¶æ¦‚ç‡ $P(Y|X)$
- æœ´ç´ è´å¶æ–¯æ˜¯åŸºäºå¾ˆå¼ºçš„**æ¡ä»¶ç‹¬ç«‹å‡è®¾**(åœ¨å·²çŸ¥åˆ†ç±»Yçš„æ¡ä»¶ä¸‹ï¼Œå„ä¸ªç‰¹å¾å˜é‡å–å€¼æ˜¯ç›¸äº’ç‹¬ç«‹çš„)ï¼Œè€Œ LR åˆ™å¯¹æ­¤æ²¡æœ‰è¦æ±‚
- æœ´ç´ è´å¶æ–¯é€‚ç”¨äºæ•°æ®é›†å°‘çš„æƒ…æ™¯ï¼Œè€ŒLRé€‚ç”¨äºå¤§è§„æ¨¡æ•°æ®é›†ã€‚







## çº¿æ€§å›å½’ç±»

### Simple Linear Regression

Assumptions

- Linearity: Y are linearly dependent on each X variable. ( a linear (technically affine) function of x)
- Independence: Observations are independent to each other. the x's are independently drawn, and not dependent on each other.
  - åä¾‹ï¼šç”¨2å¤©å‰è‚¡ä»·ã€3å¤©å‰è‚¡ä»·é¢„æµ‹ä»Šå¤©çš„
- Homo**scedas**ticityï¼šthe Ïµ's, and thus the y's, have constant variance.â€”â€”æ®‹å·® distributed arround 0
- Normalityï¼šæ®‹å·®æ­£æ€the Ïµ's are drawn from a Normal distribution (i.e. Normally-distributed errors)

å…¬å¼

<img src="../images/(null)-20220724222758272.(null)" alt="img" style="width:33%;" />

é—®é¢˜ï¼šæœ‰highly-correlated variablesçš„æ—¶å€™ï¼Œcoefficientå¯èƒ½ä¼šflip

æ”¹è¿›

- outlierå¾ˆå¤šçš„æ—¶å€™å¯ä»¥è€ƒè™‘log transformation on Y



### Ridge Regression

<img src="../images/(null)-20220724222758758.(null)" alt="img" style="width:33%;" />

- æ³¨æ„æ˜¯L2æ­£åˆ™åŒ–$$\operatorname{Min}_{w} \sum_{i=1}^{m}\left(\hat{y}_{i}-y_{i}\right)^{2}+\alpha\|w\|_{2}^{2} $$
- è§£å‡ºæ¥æ˜¯åœ¨é‡Œé¢å¤šäº†ä¸€ä¸ª$$\alpha I$$ï¼š$$\boldsymbol{w}=(X^{T} X+\alpha I )^{-1} X^{T} y$$
  - Î±è¶Šå¤§ï¼Œä¼šè¶Špush $$\boldsymbol{w}$$ to 0

### Lasso Regression

- å…¬å¼ï¼š

<img src="../images/(null)-20220724222758325.(null)" alt="img" style="width:33%;" />

- è·ŸRidgeçš„åŒºåˆ«ï¼š

  <img src="../images/(null)-20220724222758875.(null)" alt="img" style="width:33%;" />

  - Firstly hit one of the corner 4 points and 1 coefficient become 0 in lasso!
    - nç»´åº¦çš„æ—¶å€™å°±ä¼šhit one of the colomn!

  - Ridgeçš„æ—¶å€™æœ‰å¯èƒ½æ˜¯0ä½†å¤§éƒ¨åˆ†ä¸æ˜¯ï¼šhttps://stats.stackexchange.com/questions/176599/why-will-ridge-regression-not-shrink-some-coefficients-to-zero-like-lasso



### Elastic-Net regression

<img src="../images/(null)-20220724222759130.(null)" alt="img" style="width:33%;" />

- Î±ï¼šon whole regularization constrain
- Î»ï¼šcombination weight

<img src="../images/(null)-20220724222758490.(null)" alt="img" style="width:33%;" />

### L1å’ŒL2çš„åŒºåˆ«

L1ï¼ˆLassoï¼‰æ¯”L2ï¼ˆRidgeï¼‰æ›´å®¹æ˜“è·å¾—ç¨€ç–è§£ï¼ŒL2æ¯”L1æ›´å®¹æ˜“è·å¾—smoothè§£

![img](../images/(null)-20220724222758563.(null))

![img](../images/(null)-20220724222758573.(null))

![img](../images/(null)-20220724222758809.(null))



Logistic Regression

 

## Logistic Regression

#### æ¨¡å‹Setup

**Loss Function**

<img src="../images/(null)-20220724223457758.(null)" alt="img" style="width:33%;" />

- **Hinge losså’Œlog lossç›¸æ¯”0-1 losså¯ä»¥æ±‚å¯¼ï¼Œè¿™ä¸¤ä¸ªéƒ½æ˜¯upper bond of the loss functionï¼ˆé»‘è‰²é‚£ä¸ªï¼‰**

- å‡è®¾ y = 1

  - `0-1 loss`ï¼šéœ€è¦ w.T @ x + b >0 æ‰ä¼šæ˜¯0**
  - `Hinge loss`ï¼šå¦‚æœä½ é¢„æµ‹-4 æ­£ç¡®ä¸º1ï¼Œé‚£å°±æ˜¯1-(1 * (-4)) = 5ï½œå¦‚æœé¢„æµ‹4æ­£ç¡®ä¸º1ï¼Œlossæ˜¯1-1Ã—4 = -3 ç„¶åå†max(0, -3) = 0
    - **æ€»ä¹‹æ­£ç¡®çš„æ—¶å€™é¢„æµ‹æ¦‚ç‡è¶Šæ¥è¿‘1ï¼Œlossè¶Šæ¥è¿‘0ï¼Œç„¶åè¶Šç¦»è°±çš„è¯ç»™çš„Hingeå°±ä¼šç»™è¶Šé«˜çš„lossï¼Œè€Œä¸æ˜¯0-1é‚£æ ·fixedä½**
  - `Logistic Loss`: è§

Logistic Lossæ˜¯**log odds ratio**

<img src="../images/(null)-20220724223457931.(null)" alt="img" style="width:33%;" />

æ¨å¯¼è¿‡ç¨‹ï¼š

- é¦–å…ˆè¿™æ˜¯ä¸€ç§å¹¿ä¹‰çº¿æ€§æ¨¡å‹ï¼šæŠŠy=1çš„æ¦‚ç‡pç”¨$$log(\frac{p}{1-p})$$çš„è”ç³»å‡½æ•°è·Ÿç³»ç»Ÿéƒ¨åˆ†$$w^Tx + b$$ç»™è”ç³»åœ¨äº†ä¸€èµ·ï¼Œè¿™ä¸ªfunctionä¼šæ¨å¯¼å‡ºsigmoidï¼š

<img src="../images/(null)-20220724223457515.(null)" alt="img" style="width:33%;" />

- æ¥ç€æˆ‘ä»¬ç”¨æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ¥optimizeæƒ³è¦æ±‚ï¼š$$p(y=1 \mid x)=\frac{1}{1+\exp \left(-\left(w^{T} x+b\right)\right)}$$
- **ç„¶åå°±å¾—åˆ°äº†Log Likelihood**

<img src="../images/(null)-20220724223457224.(null)" alt="img" style="width:33%;" />

- **äºæ˜¯æˆ‘ä»¬æŠŠæŸå¤±å‡½æ•°è®¾å®šæˆMin(-LL)**

<img src="../images/(null)-20220724223457293.(null)" alt="img" style="width:33%;" />

- å†åŠ å…¥æ­£åˆ™é¡¹å°±å¾—åˆ°äº†ä¸€ç³»åˆ—ï¼š**Loss function for regularized logistics regressionï¼š**

  <img src="../images/(null)-20220724223457754.(null)" alt="img" style="width:33%;" />

  - SK learné‡Œé¢æ˜¯Cï¼Œé«˜çš„$$C = \frac{1}{\alpha}$$ç›¸å½“äºæ²¡æœ‰å¯¹å‚æ•°é™åˆ¶è§æ•ˆ

#### **å¤šåˆ†ç±»é—®é¢˜å¸¸è§„è§£å†³æ–¹æ¡ˆ**

**å åŠ binary**

- **OVR (One vs Rest)**

<img src="../images/(null)-20220724223459288.(null)" alt="img" style="width:33%;" />

- **OVO (One vs One)**

<img src="../images/(null)-20220724223458436.(null)" alt="img" style="width:33%;" />

- å»ºå¥½å‡ ä¸ªbinary classificationï¼Œå¦‚æœå¤§å¤šæ•°modelè¯´ä½ æ˜¯class Xå°±æ˜¯é‚£ä¸ª

- å¯¹æ¯”

  <img src="../images/(null)-20220724223458032.(null)" alt="img" style="width:50%;" />

  - æ¯”å¦‚ä¸‰æ¡çº¿ä¸­é—´çš„åœ°æ–¹ï¼Œä¸€äººè¯´ä½ æ˜¯ä¸€ä¸ªclassï¼Œæ˜¯uncertainty


#### **Logisticsè§£å†³å¤šåˆ†ç±»é—®é¢˜**

ç›´æ¥extend

<img src="../images/(null)-20220724223459022.(null)" alt="img" style="width:50%;" />

- å› æ­¤å¯èƒ½ä¼šæ¯”ç”¨ovoå’Œovrçš„SVMè¦å¥½ï¼å› ä¸ºä»–ç›´æ¥globally **optimize all the log odds of ratio**. It actually *solves it as a multiclass classification problem!*

## SVM 

- Hard/soft maginå«ä¹‰ï¼š

<img src="../images/(null)-20220724224339951.(null)" alt="img" style="width:50%;" />



#### Primal + Hard-margin

<img src="../images/(null)-20220724224337505.(null)" alt="img" style="width:50%;" />

- **Objective functionçš„å«ä¹‰ï¼š**optimize åˆ’åˆ†è¶…å¹³é¢çš„ maximum margin ï¼ˆæœ€å¤§é—´éš”ï¼‰=$$\frac{2}{\|w\|_{2}^{2}}$$ï¼Œä¹Ÿå°±æ˜¯minimize$$\frac{\|w\|_{2}^{2}}{2}$$

- é™åˆ¶æ¡ä»¶çš„å«ä¹‰ï¼š

  - ç‚¹(xi, yi)åˆ°ç›´çº¿$$y=w^{T} x+b$$çš„è·ç¦»$$\frac{ |w^{T} x_{i}+b-yi|}{{\|w\|_{2}^{2}}} \geq 1$$

    $$\Rightarrow { |w^{T} x_{i}+b-yi|} \geq {{\|w\|_{2}^{2}}}$$

    $$\Rightarrow w^{T} x_{i}+b \geq y_i \text{ or } w^{T} x_{i}+b \leq -y_i$$

    $$\Rightarrow \begin{cases}\omega^{T} x_{i}+b \geq+1, & y_{i}=+1 \\ \omega^{T} x_{i}+b \leq-1, & y_{i}=-1\end{cases}$$

    $$\Rightarrow y_{i}\left(w^{T} x_{i}+b\right) \geq 1$$

    - ä»–ä¹‹ä¸Šé‚£ä¹ˆè¦>1ï¼Œå¦‚æœåœ¨å®ƒä¹‹ä¸‹è¦>-1ï¼Œä¹Ÿå°±æ˜¯*y1 > 1

#### Primal + Dual

<img src="../images/(null)-20220724224339721.(null)" alt="img" style="width:50%;" />

- **Linkï¼š**é€šè¿‡å¯¹wå’Œbæ±‚å¯¼ æŠŠç»“æœä»£å›loss functionå°±å¾—åˆ°äº†å¯¹å¶é—®é¢˜

- ç‰¹ç‚¹ï¼šä¹‹å‰éœ€è¦è§£wå’Œbã€ç°åœ¨åªéœ€è¦è§£Î±

  - Î± ä¸€ä¸ªmç»´å‘é‡ï¼ˆmæ˜¯æ ·æœ¬çš„sizeï¼Œnæ˜¯å˜é‡ä¸ªæ•°ï¼‰
  - ä¹‹å‰è§£çš„æ˜¯**ä¸€ä¸ªn_featureç»´é—®é¢˜ï¼ç°åœ¨å˜æˆn_sampleç»´é—®é¢˜**
    - å¦‚æœå°‘é‡featureçš„æ—¶å€™ Primalé—®é¢˜è§£çš„æ›´å¿«ï¼ˆå¤§å¤šæ•°æƒ…å†µï¼‰
    - å¦‚æœæœ‰å¤§é‡featureçš„è¯ï¼ŒDualè§£çš„æ›´å¿«

- ç›®çš„

  - dualå¯ä»¥æ›´å®¹æ˜“ç§»åŠ¨åˆ°non- linearçš„åœºæ™¯é‡Œ

- æ•ˆæœï¼šè¿™ä¸ªç­‰å¼ä¼šä¸€ç›´æ˜¯0 $\alpha_{i}\left(1-y_{i}\left(w^{T} x_{i}+b\right)\right)=0$

  - å¯¹äºéæ”¯æŒå‘é‡ï¼šÎ±i=0

  - åªæœ‰è§£å‡ºæ¥çš„æ”¯æŒå‘é‡æ»¡è¶³ï¼šÎ±i != 0,$$y_{i}\left(w^{T} x_{i}+b\right) = 1$$

#### Primal + Soft- margin

- é¦–å…ˆÎ¾ introduce lossï¼Œç„¶åå†é€šè¿‡æ•°å­¦å˜æ¢æŠŠÎ¾è§£å‡ºæ¥ï¼Œç„¶åå¾—åˆ°çš„å°±æ˜¯Hinge loss

<img src="../images/(null)-20220724224337896.(null)" alt="img" style="width:50%;" /><img src="../images/(null)-20220724224336553.(null)" alt="img" style="width:50%;" />

- Cçš„ä½œç”¨ï¼šæ§åˆ¶errorçš„é‡è¦æ€§â€”â€”Cè¶Šå¤§ï¼Œmarginè¶Šä¸é‡è¦å°±è¶Šå°ï½œ**Cè¶Šå°ï¼Œmarginè¶Šé‡è¦å°±è¶Šå¤§**

<img src="../images/(null)-20220724224337202.(null)" alt="img" style="width:50%;" />

#### Dual + Soft-margin

<img src="../images/(null)-20220724224337040.(null)" alt="img" style="width:50%;" />

- ä¹Ÿå¯ä»¥åŠ å…¥æ­£åˆ™åŒ–ï¼š

<img src="../images/(null)-20220724224337444.(null)" alt="img" style="width:50%;" />

- é¢„æµ‹çš„æ–¹æ³•ï¼š$$\operatorname{sign}\left(\sum_{i} \alpha_{i} y_{i}\left(x \cdot x_{i}\right)+b\right)$$
  - $$\alpha_i$$ï¼šç¬¬iä¸ªsupport vectorçš„dual coefficient
  - $$x_i, y_i$$ï¼šç¬¬iä¸ªsupport vectorçš„åæ ‡

#### Kernel Function

**Kernel function ğ•‚(ğ’™;ğ’™)** å¯ä»¥ **estimates inner product between two points in the projected space.**

- å‡è®¾æœ‰ä¸€ä¸ªmagic function ğ“(ğ’™)ï¼Œå¯ä»¥ projects data to **high-dimensional space**

  è¿˜æ˜¯ç»™ä¸¤ä¸ªç‚¹è¿”å›è·ç¦»ï¼ä½†æ˜¯è¿™ä¸ªè·ç¦»æ˜¯åœ¨é«˜ç»´ç©ºé—´ä¸Šçš„ æ‰€ä»¥å¯èƒ½åœ¨è¿™ä¸ªç»´ä¸Šçœ‹åˆ°çš„ä¸ä¸€æ ·

<img src="../images/(null)-20220724224338320.(null)" alt="img" style="width:50%;" />

- ä½†æˆ‘ä»¬å…³å¿ƒçš„æ˜¯ä¸¤ä¸ªğ“(ğ’™i) Ã— ğ“(ğ’™j) çš„ç»“æœï¼Œè€Œä¸æ˜¯ğ“æœ¬èº«ï¼Œæ‰€ä»¥å¯ä»¥ä½¿ç”¨ä¸€ä¸ªKernel trickï¼Œå‡è®¾**ğ•‚**æ˜¯ç›´æ¥ä½œç”¨åœ¨dot productä¸Šé¢çš„ï¼Œä»è€Œåªéœ€è¦assume ğ“çš„å­˜åœ¨å°±å¯ä»¥äº†

<img src="../images/(null)-20220724224337915.(null)" alt="img" style="width:50%;" />

- å› æ­¤è¿™ä¸ªé—®é¢˜å°±å˜æˆäº†ï¼š

<img src="../images/(null)-20220724224338928.(null)" alt="img" style="width:50%;" />

- ç±»å‹ï¼š

  <img src="../images/(null)-20220724224338479.(null)" alt="img" style="width:50%;" />

  - SVMè§£å†³éçº¿æ€§çš„é—®é¢˜ï¼šçº¿æ€§æ ¸å‡½æ•°çš„å†³ç­–è¾¹ç•Œçº¿æ€§ï¼Œå•é«˜æ–¯æ ¸å‡½æ•°çš„å†³ç­–è¾¹ç•Œæ˜¯éçº¿æ€§çš„ï¼


- Linear Kernelï¼šæ²¡åšå•¥ï¼
- Polynomialï¼š
  - æ¯”å¦‚ä¸€ä¸ªä¸€ç»´æ˜ å°„åˆ°ä¸‰ç»´çš„å‡½æ•°$$\phi\left(x\right)=\left(x, \sqrt{2} x, 1\right)$$ï¼Œä»–çš„æ•ˆæœæ˜¯$$\phi\left(x_{i}\right) \cdot \phi\left(x_{j}\right)=\left(x_{i}, \sqrt{2} x_{i}, 1\right) \cdot \left(x_{j}, \sqrt{2} x_{j}, 1\right) = \left(x_{i} x_{j}+1\right)^{2}$$
  - ä»–å…¶å®ç­‰ä»·äºä¸€ä¸ªPolynomialçš„Kernel Function$$\mathbb{K}\left(x_{i}, x_{j}\right)=\left(x_{i} x_{j}+1\right)^{2}$$å°±å¯ä»¥ç»™å‡ºé«˜ç»´ç©ºé—´Ï†ä¸‹çš„ç‚¹ä¹˜ç»“æœäº†
  - è®¡ç®—å¤æ‚åº¦ä¹Ÿä½å¤šäº†ï¼

<img src="../images/(null)-20220724224338277.(null)" alt="img" style="width:50%;" />

- RBF(Radial Basis Function) Kernelï¼šå¦‚æœå……åˆ†tune Î³ enoughï¼ŒèƒŒåå®é™…ä¸Šæ˜¯ä¸€ä¸ªinfinite diminutionalçš„spaceï¼Œä»è€Œalwayså¯ä»¥separate dataï¼å¯ä»¥perfectly separate data.

  RBDä¸»è¦ç”¨äºçº¿æ€§ä¸å¯åˆ†çš„æƒ…å½¢

  - ä¸¤ä¸ªç‚¹åœ¨é«˜ç»´ç©ºé—´è¶Šæ¥è¿‘ï¼Œå°±ä¼šæœ‰è¶Šé«˜çš„valuesï½œSpread out çš„ç‚¹ä¼šsmaller
    - `Î³`ä¼šæ§åˆ¶ how the function value decays with distanceï¼ˆÎ³è¶Šå¤§çš„è¯ï¼Œè·ç¦»è¶Šè¿œçš„sample å€¼ä¸‹é™å¾—è¶Šå¿«ï¼‰ 
    - e.g

<img src="../images/(null)-20220724224338727.(null)" alt="img" style="width:50%;" />

**Kernelé€‰æ‹©çš„æ€è·¯ï¼š**

ï¼ˆ1ï¼‰å¦‚æœç‰¹å¾ç»´æ•°å¾ˆé«˜ï¼Œå¾€å¾€çº¿æ€§å¯åˆ†ï¼ˆSVMè§£å†³éçº¿æ€§åˆ†ç±»é—®é¢˜çš„æ€è·¯å°±æ˜¯å°†æ ·æœ¬æ˜ å°„åˆ°æ›´é«˜ç»´çš„ç‰¹å¾ç©ºé—´ä¸­ï¼‰ï¼Œå¯ä»¥é‡‡ç”¨LRæˆ–è€…çº¿æ€§æ ¸çš„SVMï¼›

ï¼ˆ2ï¼‰å¦‚æœæ ·æœ¬æ•°é‡å¾ˆå¤šï¼Œç”±äºæ±‚è§£æœ€ä¼˜åŒ–é—®é¢˜çš„æ—¶å€™ï¼Œç›®æ ‡å‡½æ•°æ¶‰åŠä¸¤ä¸¤æ ·æœ¬è®¡ç®—å†…ç§¯ï¼Œä½¿ç”¨é«˜æ–¯æ ¸æ˜æ˜¾è®¡ç®—é‡ä¼šå¤§äºçº¿æ€§æ ¸ï¼Œæ‰€ä»¥æ‰‹åŠ¨æ·»åŠ ä¸€äº›ç‰¹å¾ï¼Œä½¿å¾—çº¿æ€§å¯åˆ†ï¼Œç„¶åå¯ä»¥ç”¨LRæˆ–è€…çº¿æ€§æ ¸çš„SVMï¼›

ï¼ˆ3ï¼‰å¦‚æœä¸æ»¡è¶³ä¸Šè¿°ä¸¤ç‚¹ï¼Œå³**ç‰¹å¾ç»´æ•°å°‘ï¼Œæ ·æœ¬æ•°é‡æ­£å¸¸**ï¼Œå¯ä»¥ä½¿ç”¨é«˜æ–¯æ ¸çš„SVMã€‚



## Ensemble Methods

Baggingå’ŒBoostingéƒ½æ˜¯ensembleï¼Œå°±æ˜¯æŠŠå¼±åˆ†ç±»å™¨ç»„è£…æˆå¼ºåˆ†ç±»å™¨çš„æ–¹æ³•

**Motivation**

- The decision trees are highly **unstable** and can structurally change with slight variation in input data
- Decision trees perform poorly on continuous outcomes (regression) due to limited model capacity.

**å®šä¹‰**

- Several weak/simple learners are **combined** to make the ï¬nal prediction
- ç›®çš„ï¼šGenerally ensemble methods aim to **reduce model variance**
  - Like have multiple such outputs and then you take an average of that.
- æ•ˆæœï¼šEnsemble methods improve performance especially if the individual learners are not correlated.
  - **took a different or a different perspective** of the data itself.
  - é‡‡æ ·ä¼šâ‡’æˆåŠŸ
    - if you had one or two highly **dominant features** that probably is saying is highly correlated to your outcome. 
      - Suppose every tree that a building has access to that feature. Probably every tree is going to look very similar right now.
      - Assume that you actually had some trees **not have access to that feature. Then they'll start looking at the data from a different perspective and they'll probably build trees that are giving you another notion of your data center. And it's not dominated by these one or two features that are are highly correlated to the outcome.**
- ç±»å‹ï¼šDepending on training sample construction and output aggregation, there are two categories:
  - Bagging (Bootstrap aggregation)
  - Boosting

#### Bagging

Baggingçš„ä¸»è¦ç›®çš„æ˜¯å‡å°‘æ–¹å·®

- å¤šæ¬¡é‡‡æ ·ï¼Œè®­ç»ƒå¤šä¸ªåˆ†ç±»å™¨Several training samples (of same size) are created by sampling the dataset **with replacement**

  - ä»åŸå§‹æ ·æœ¬é›†ä¸­æŠ½å–è®­ç»ƒé›†ã€‚æ¯è½®ä»åŸå§‹æ ·æœ¬é›†ä¸­ä½¿ç”¨Bootstrapingçš„æ–¹æ³•æŠ½å–nä¸ªè®­ç»ƒæ ·æœ¬ï¼ˆåœ¨è®­ç»ƒé›†ä¸­ï¼Œæœ‰äº›æ ·æœ¬å¯èƒ½è¢«å¤šæ¬¡æŠ½å–åˆ°ï¼Œè€Œæœ‰äº›æ ·æœ¬å¯èƒ½ä¸€æ¬¡éƒ½æ²¡æœ‰è¢«æŠ½ä¸­ï¼‰ã€‚å…±è¿›è¡Œkè½®æŠ½å–ï¼Œå¾—åˆ°kä¸ªè®­ç»ƒé›†ã€‚ï¼ˆkä¸ªè®­ç»ƒé›†ä¹‹é—´æ˜¯ç›¸äº’ç‹¬ç«‹çš„ï¼‰

    there could be samples that are **repeated** and there are samples that **do not get picked at all.**

    <img src="../images/(null)-20220725100730752.(null)" alt="img" style="width: 50%;" />

  - æ¯æ¬¡ä½¿ç”¨ä¸€ä¸ªè®­ç»ƒé›†å¾—åˆ°ä¸€ä¸ªæ¨¡å‹ï¼Œkä¸ªè®­ç»ƒé›†å…±å¾—åˆ°kä¸ªæ¨¡å‹ã€‚ï¼ˆæ³¨ï¼šè¿™é‡Œå¹¶æ²¡æœ‰å…·ä½“çš„åˆ†ç±»ç®—æ³•æˆ–å›å½’æ–¹æ³•ï¼Œæˆ‘ä»¬å¯ä»¥æ ¹æ®å…·ä½“é—®é¢˜é‡‡ç”¨ä¸åŒçš„åˆ†ç±»æˆ–å›å½’æ–¹æ³•ï¼Œå¦‚å†³ç­–æ ‘ã€æ„ŸçŸ¥å™¨ç­‰ï¼‰

- åˆ†ç±»é—®é¢˜ï¼šå¯¹åˆ†ç±»é—®é¢˜ï¼šå°†ä¸Šæ­¥å¾—åˆ°çš„kä¸ªæ¨¡å‹é‡‡ç”¨æŠ•ç¥¨çš„æ–¹å¼å¾—åˆ°åˆ†ç±»ç»“æœ

- å¯¹å›å½’é—®é¢˜ï¼Œè®¡ç®—ä¸Šè¿°æ¨¡å‹çš„å‡å€¼ä½œä¸ºæœ€åçš„ç»“æœã€‚ï¼ˆæ‰€æœ‰æ¨¡å‹çš„é‡è¦æ€§ç›¸åŒï¼‰ 

#### Boosting

Boostingçš„ç›®çš„ä¸»è¦æ˜¯å‡å°‘åå·®

- Includes a family of ML algorithms that convert weak learners to strong ones.

  - The weak learners are learned sequentially with early learners ï¬tting simple models to the data and then analysing data for errors.

  - When an input is misclassiï¬ed by one tree, its output is adjusted so that next tree is more likely to learn it correctly.

     æ¯ä¸ªtreeçš„ç›®æ ‡æ˜¯do better on the misclassify samples of the previous.

    - ä¹‹å‰åˆ†å¯¹çš„down weight
    - ä¹‹å‰åˆ†é”™çš„out weight

<img src="../images/(null)-20220725100731093.(null)" alt="img" style="width:50%;" />

- æœ€å combined by a weighted average of each of those trees. ä½†è¿™ä¸ªæƒé‡è·Ÿæ¨¡å‹æœ‰å…³ï¼

> Random Forestæ˜¯ç›´æ¥averageï¼



- ç®—æ³•æµç¨‹ï¼š

  - ç»™å®šåˆå§‹è®­ç»ƒæ•°æ®ï¼Œç”±æ­¤è®­ç»ƒå‡ºç¬¬ä¸€ä¸ªåŸºå­¦ä¹ å™¨ï¼›

  - æ ¹æ®åŸºå­¦ä¹ å™¨çš„è¡¨ç°å¯¹æ ·æœ¬è¿›è¡Œè°ƒæ•´ï¼Œåœ¨ä¹‹å‰å­¦ä¹ å™¨åšé”™çš„æ ·æœ¬ä¸ŠæŠ•å…¥æ›´å¤šå…³æ³¨ï¼›

  - ç”¨è°ƒæ•´åçš„æ ·æœ¬ï¼Œè®­ç»ƒä¸‹ä¸€ä¸ªåŸºå­¦ä¹ å™¨ï¼›

  - é‡å¤ä¸Šè¿°è¿‡ç¨‹Tæ¬¡ï¼Œå°†Tä¸ªå­¦ä¹ å™¨åŠ æƒç»“åˆã€‚

- é€šè¿‡æé«˜é‚£äº›åœ¨å‰ä¸€è½®è¢«å¼±åˆ†ç±»å™¨åˆ†é”™æ ·ä¾‹çš„æƒå€¼ï¼Œå‡å°å‰ä¸€è½®åˆ†å¯¹æ ·ä¾‹çš„æƒå€¼ï¼Œæ¥ä½¿å¾—åˆ†ç±»å™¨å¯¹è¯¯åˆ†çš„æ•°æ®æœ‰è¾ƒå¥½çš„æ•ˆæœã€‚

- é€šè¿‡ä»€ä¹ˆæ–¹å¼æ¥ç»„åˆå¼±åˆ†ç±»å™¨ï¼Ÿ

  - é€šè¿‡åŠ æ³•æ¨¡å‹å°†å¼±åˆ†ç±»å™¨è¿›è¡Œçº¿æ€§ç»„åˆï¼Œæ¯”å¦‚AdaBoosté€šè¿‡åŠ æƒå¤šæ•°è¡¨å†³çš„æ–¹å¼ï¼Œå³å¢å¤§é”™è¯¯ç‡å°çš„åˆ†ç±»å™¨çš„æƒå€¼ï¼ŒåŒæ—¶å‡å°é”™è¯¯ç‡è¾ƒå¤§çš„åˆ†ç±»å™¨çš„æƒå€¼ã€‚

  - è€Œæå‡æ ‘é€šè¿‡æ‹Ÿåˆæ®‹å·®çš„æ–¹å¼é€æ­¥å‡å°æ®‹å·®ï¼Œå°†æ¯ä¸€æ­¥ç”Ÿæˆçš„æ¨¡å‹å åŠ å¾—åˆ°æœ€ç»ˆæ¨¡å‹ã€‚



#### Baggingå’ŒBoostingçš„åŒºåˆ«ä¸è”ç³»

Baggingå’ŒBoostingéƒ½æ˜¯ensembleï¼Œå°±æ˜¯æŠŠå¼±åˆ†ç±»å™¨ç»„è£…æˆå¼ºåˆ†ç±»å™¨çš„æ–¹æ³•ã€‚

åŒºåˆ«åœ¨äºä»¥ä¸‹å‡ ç‚¹ï¼š

- 1ï¼‰æ ·æœ¬é€‰æ‹©ä¸Šï¼š

  - Baggingï¼šè®­ç»ƒé›†æ˜¯åœ¨åŸå§‹é›†ä¸­æœ‰æ”¾å›é€‰å–çš„ï¼Œä»åŸå§‹é›†ä¸­é€‰å‡ºçš„å„è½®è®­ç»ƒé›†ä¹‹é—´æ˜¯ç‹¬ç«‹çš„ã€‚

  - Boostingï¼šæ¯ä¸€è½®çš„è®­ç»ƒé›†ä¸å˜ï¼Œåªæ˜¯è®­ç»ƒé›†ä¸­æ¯ä¸ªæ ·ä¾‹åœ¨åˆ†ç±»å™¨ä¸­çš„æƒé‡å‘ç”Ÿå˜åŒ–ã€‚è€Œæƒå€¼æ˜¯æ ¹æ®ä¸Šä¸€è½®çš„åˆ†ç±»ç»“æœè¿›è¡Œè°ƒæ•´ã€‚

- 2ï¼‰æ ·ä¾‹æƒé‡ï¼š

  - Baggingï¼šä½¿ç”¨å‡åŒ€å–æ ·ï¼Œæ¯ä¸ªæ ·ä¾‹çš„æƒé‡ç›¸ç­‰

  - Boostingï¼šæ ¹æ®é”™è¯¯ç‡ä¸æ–­è°ƒæ•´æ ·ä¾‹çš„æƒå€¼ï¼Œé”™è¯¯ç‡è¶Šå¤§åˆ™æƒé‡è¶Šå¤§ã€‚

- 3ï¼‰é¢„æµ‹å‡½æ•°ï¼š

  - Baggingï¼šæ‰€æœ‰é¢„æµ‹å‡½æ•°çš„æƒé‡ç›¸ç­‰ã€‚

  - Boostingï¼šæ¯ä¸ªå¼±åˆ†ç±»å™¨éƒ½æœ‰ç›¸åº”çš„æƒé‡ï¼Œå¯¹äºåˆ†ç±»è¯¯å·®å°çš„åˆ†ç±»å™¨ä¼šæœ‰æ›´å¤§çš„æƒé‡ã€‚

- 4ï¼‰å¹¶è¡Œè®¡ç®—ï¼š

  - Baggingï¼šå„ä¸ªé¢„æµ‹å‡½æ•°å¯ä»¥å¹¶è¡Œç”Ÿæˆ

  - Boostingï¼šå„ä¸ªé¢„æµ‹å‡½æ•°åªèƒ½é¡ºåºç”Ÿæˆï¼Œå› ä¸ºåä¸€ä¸ªæ¨¡å‹å‚æ•°éœ€è¦å‰ä¸€è½®æ¨¡å‹çš„ç»“æœã€‚

#### Stacking

- å¤šæ¬¡é‡‡æ ·ï¼Œè®­ç»ƒå¤šä¸ªåˆ†ç±»å™¨ï¼Œå°†è¾“å‡ºä½œä¸ºæœ€åçš„è¾“å…¥ç‰¹å¾

- å°†è®­ç»ƒå¥½çš„æ‰€æœ‰åŸºæ¨¡å‹å¯¹è®­ç»ƒé›†è¿›è¡Œé¢„æµ‹ï¼Œç¬¬ä¸ª$i$åŸºæ¨¡å‹å¯¹ç¬¬$i$ä¸ªè®­ç»ƒæ ·æœ¬çš„é¢„æµ‹å€¼å°†ä½œä¸ºæ–°çš„è®­ç»ƒé›†ä¸­ç¬¬$i$ä¸ªæ ·æœ¬çš„ç¬¬$i$ä¸ªç‰¹å¾å€¼ï¼Œæœ€ååŸºäºæ–°çš„è®­ç»ƒé›†è¿›è¡Œè®­ç»ƒã€‚åŒç†ï¼Œé¢„æµ‹çš„è¿‡ç¨‹ä¹Ÿè¦å…ˆç»è¿‡æ‰€æœ‰åŸºæ¨¡å‹çš„é¢„æµ‹å½¢æˆæ–°çš„æµ‹è¯•é›†ï¼Œæœ€åå†å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹ã€‚

- stackingå¸¸è§çš„ä½¿ç”¨æ–¹å¼ï¼š
  - ç”±k-NNã€éšæœºæ£®æ—å’Œæœ´ç´ è´å¶æ–¯åŸºç¡€åˆ†ç±»å™¨ç»„æˆï¼Œå®ƒçš„é¢„æµ‹ç»“æœç”±ä½œä¸ºå…ƒåˆ†ç±»å™¨çš„é€»å›å½’ç»„åˆã€‚



## Trees

###  Decision Trees

**ç‰¹ç‚¹**

- Greedy algorithm
- Applicable to both classiï¬cation & regression problems
  - Regressionçš„è¯åªèƒ½æ˜¯finiteçš„æ•°å€¼

<img src="../images/(null)-20220724230711129.(null)" alt="img" style="width:67%;" />

- Easy to interpret & deployï¼šå¯ä»¥è®©åˆ«äººhandwrite
- Non-linear decision boundary
  - treeç›¸å½“äºæ˜¯åœ¨é«˜ç»´ç©ºé—´çš„å¥½å‡ ä¸ªç»´åº¦ä¸Šå»splitting up the space
- Minimal preprocessing
  - missing å’Œ categoricalå¯ä»¥handleï¼
- Invariant to scale of data
  - Invariant to the scale of the data because it is it is *not really looking at at absolute values of the of the features*. More on the ranges of the features.

**Framework**

<img src="../images/(null)-20220724230711222.(null)" alt="img" style="width:67%;" />

**Loss**



åˆ†ç±»ä»»åŠ¡ï¼š

- Impurityè¡¡é‡ï¼š$Entropy(node)=-\sum_{i=1}^{K} p_{i} \log _{2} p_{i} $ ï½œ $Gini Index(node)=1-\sum_{i=1}^{K} p_{i}^{2}$$

  - $$p_{i}=\text{probability of beloing to a class} =\frac{\text{number of samples of the class}}{\text{total number of samples in that node}}$$ 

  - ä¾‹å­

    <img src="../images/(null)-20220724230710858.(null)" alt="img" style="width:33%;" /><img src="../images/(null)-20220724230711014.(null)" alt="img" style="width:33%;" /> 

  - ä¸¤è€…åŒºåˆ«ä¸å¤§â€”â€”giniçš„æœ€å¤§å€¼æ˜¯1ï¼Œentropyçš„æœ€å¤§å€¼æ˜¯0.5

- Information Gainï¼ˆè¯„ä¼°ä¿¡æ¯å¢ç›ŠInformation Gainå¯¹ç¡®å®šçš„featureå’Œç¡®å®šçš„split thresholdï¼‰

  [Expected](https://en.wikipedia.org/wiki/Expectation_value) information gain $$IG(T, a)=H(T)-H(T \mid a)$$ is the reduction in [information entropy](https://en.wikipedia.org/wiki/Information_entropy) *Î—* from a prior state to a state that takes some information as given. ä¹Ÿå°±æ˜¯ä¸çº¯åº¦çš„ä¸‹é™

  - $$H(T)$$ï¼ša priori Shannon entropy 
  - $$H(T\mid a) = \sum_{v \in vals(a)} \frac{\left|S_{a}(v)\right|}{|T|}  H\left(S_{a}(v)\right)$$ï¼šæ ·æœ¬å æ¯”ä¸ºæƒçš„åŠ æƒå¹³å‡entropy
    - $$\frac{\left|S_{a}(v)\right|}{|T|} $$: å…¶å®åˆ†åˆ°açš„å æ¯”ï¼ˆæƒé‡ï¼‰ï¼Œæ‹¿è¿™ä¸ªå»åŠ æƒå¹³å‡
      - $$S_{a}(v)=\left\{x \in T \mid x_{a}=v\right\}$$è¡¨ç¤ºTä¸­åˆ†è£‚åˆ°aé‡Œé¢çš„nodeç»„æˆçš„é›†åˆ
    - $$H\left(S_{a}(v)\right)$$ï¼š$$S_{a}(v)$$çš„Entropy
  - e.g

  <img src="../images/(null)-20220724231640012.(null)" alt="img" style="width:33%;" />

  - Numericalçš„ï¼š An exhaustive **search across all features and values** to ï¬nd the (feature, threshold) combination with the highest information gain (IG).

  <img src="../images/(null)-20220724231943423.(null)" alt="img" style="width:50%;" />

  - Categoricalçš„ï¼š

    - An exhaustive **search across all categorical features** and their categories to ï¬nd the(feature, subsets) combination with the highest information gain (IG).

      <img src="../images/(null)-20220724231943637.(null)" alt="img" style="width:50%;" />

    - Use **target encoding** to reduce time complexity by only evaluating O(L)  splits of the ordered categories.

      - é¦–å…ˆï¼šThe categories are ordered by mean response rate 

      - æ¥ç€æŒ‰é¡ºåºä¸€ä¸ªä¸€ä¸ªinclude feature

      - è¿™æ ·æ‰¾å‡ºæ¥çš„ä¼šæ˜¯optimalçš„ï¼

        <img src="../images/(null)-20220724231943028.(null)" alt="img" style="width:50%;" />



#### å†³ç­–æ ‘Overfittingçš„è§£å†³

> ä¸è®­ç»ƒåˆ°å®Œæ•´çš„tree

- `Pruning`ï¼šstart from the bottom and start chopping off parts of the tree that don't make sense.
  - Reduced error
    - Starting at the leaves, each node is replaced with its most popular class (chopping)
    - If the validation metric is not negatively aï¬€ected, then the change is kept, else it is reverted.
    - Reduced error pruning has the advantage of speed and simplicity.
    
  - Cost complexity
    - The node with the least $$\alpha_{eff}=\frac{R(t)-R_{\alpha}\left(T_{t}\right)}{T-1}$$is pruned
    
    - Î±ç›¸å½“äºä¸€ä¸ªæ­£åˆ™é¡¹çš„ç³»æ•°ï¼Œä»è€Œ$$R_{\alpha}(T)=R(T)+\alpha|T|$$
    
      - $$R\left(T_{t}\right)=\sum_{i \in \text { leaf nodes }} R(i)$$, sum of impurities for all leaf nodes t of a tree rooted at node t.
    - Î±è¶Šé«˜ï¼Œæƒ©ç½šè¶Šå¤§ï¼Œä¼šä»overfitting -> sweetspot -> underfitting. éšç€alphaæé«˜ï¼Œæ€»impurityä¼šæœ‰steep change

<img src="../images/(null)-20220724230711183.(null)" alt="img" style="width:40%;" /><img src="../images/(null)-20220724230711425.(null)" alt="img" style="width:40%;" />

- `Early stopping`ï¼šbuild up to a point and then you stop.

  - Maximum depthï¼šonly build it to a certain depth which prevents you from going very deep.
    - æ¯”å¦‚ç”¨DFSï¼Œé‚£å°±æ˜¯å¾€ä¸‹splitç›´åˆ°å˜æˆpure node or reach max_depth
  - Maximum leaf nodesï¼šonly have a certain number of leaf nodes
  - Minimum samples splitï¼šthere are a minimum number of samples before I consider it a split
  - Minimum impurity decrease
  
  

#### Feature Importance

probability of sample reaching that nodeï¼š the (normalized) total reduction of the criterion brought by that feature





### Random Forests

éšæœºä¸€ç§åŸºäºæ ‘æ¨¡å‹çš„Baggingçš„ä¼˜åŒ–ç‰ˆæœ¬ï¼Œä¸€æ£µæ ‘çš„ç”Ÿæˆè‚¯å®šè¿˜æ˜¯ä¸å¦‚å¤šæ£µæ ‘ï¼Œå› æ­¤å°±æœ‰äº†éšæœºæ£®æ—ï¼Œè§£å†³å†³ç­–æ ‘æ³›åŒ–èƒ½åŠ›å¼±çš„ç‰¹ç‚¹ã€‚

- å¤šæ¬¡éšæœºå–æ ·ï¼Œå¤šæ¬¡éšæœºå–å±æ€§ï¼Œé€‰å–æœ€ä¼˜åˆ†å‰²ç‚¹ï¼Œæ„å»ºå¤šä¸ª(CART)åˆ†ç±»å™¨ï¼ŒæŠ•ç¥¨è¡¨å†³

- ç®—æ³•æµç¨‹ï¼š

  - è¾“å…¥ä¸ºæ ·æœ¬é›†$D={(xï¼Œy_1)ï¼Œ(x_2ï¼Œy_2) \dots (x_mï¼Œy_m)}$ï¼Œå¼±åˆ†ç±»å™¨è¿­ä»£æ¬¡æ•°$T$ã€‚

  - è¾“å‡ºä¸ºæœ€ç»ˆçš„å¼ºåˆ†ç±»å™¨$f(x)$

  - å¯¹äº$t=1ï¼Œ2 \dots T$

    - å¯¹è®­ç»ƒé›†è¿›è¡Œç¬¬$t$æ¬¡éšæœºé‡‡æ ·ï¼Œå…±é‡‡é›†$m$æ¬¡ï¼Œå¾—åˆ°åŒ…å«$m$ä¸ªæ ·æœ¬çš„é‡‡æ ·é›†Dt

    - ç”¨é‡‡æ ·é›†$D_t$è®­ç»ƒç¬¬$t$ä¸ªå†³ç­–æ ‘æ¨¡å‹$G_t(x)$ï¼Œåœ¨è®­ç»ƒå†³ç­–æ ‘æ¨¡å‹çš„èŠ‚ç‚¹çš„æ—¶å€™ï¼Œåœ¨èŠ‚ç‚¹ä¸Šæ‰€æœ‰çš„æ ·æœ¬ç‰¹å¾ä¸­é€‰æ‹©ä¸€éƒ¨åˆ†æ ·æœ¬ç‰¹å¾ï¼Œåœ¨è¿™äº›éšæœºé€‰æ‹©çš„éƒ¨åˆ†æ ·æœ¬ç‰¹å¾ä¸­é€‰æ‹©ä¸€ä¸ªæœ€ä¼˜çš„ç‰¹å¾æ¥åšå†³ç­–æ ‘çš„å·¦å³å­æ ‘åˆ’åˆ†

  - å¦‚æœæ˜¯åˆ†ç±»ç®—æ³•é¢„æµ‹ï¼Œåˆ™$T$ä¸ªå¼±å­¦ä¹ å™¨æŠ•å‡ºæœ€å¤šç¥¨æ•°çš„ç±»åˆ«æˆ–è€…ç±»åˆ«ä¹‹ä¸€ä¸ºæœ€ç»ˆç±»åˆ«

    å¦‚æœæ˜¯å›å½’ç®—æ³•ï¼Œ$T$ä¸ªå¼±å­¦ä¹ å™¨å¾—åˆ°çš„å›å½’ç»“æœè¿›è¡Œç®—æœ¯å¹³å‡å¾—åˆ°çš„å€¼ä¸ºæœ€ç»ˆçš„æ¨¡å‹è¾“å‡ºã€‚

- éšæœºæ£®æ—ä¸ºä»€ä¹ˆä¸å®¹æ˜“è¿‡æ‹Ÿåˆï¼Ÿ

  - éšæœºæ£®æ—ä¸­çš„æ¯ä¸€é¢—æ ‘éƒ½æ˜¯è¿‡æ‹Ÿåˆçš„ï¼Œæ‹Ÿåˆåˆ°éå¸¸å°çš„ç»†èŠ‚ä¸Š
    - éšæœºæ£®æ—é€šè¿‡å¼•å…¥éšæœºæ€§ï¼Œä½¿æ¯ä¸€é¢—æ ‘æ‹Ÿåˆçš„ç»†èŠ‚ä¸åŒ

  - æ‰€æœ‰æ ‘ç»„åˆåœ¨ä¸€èµ·ï¼Œè¿‡æ‹Ÿåˆçš„éƒ¨åˆ†å°±ä¼šè‡ªåŠ¨è¢«æ¶ˆé™¤æ‰ã€‚

**ç®—æ³•**ï¼š

<img src="../images/(null)-20220725101113336.(null)" alt="img" style="width:50%;" />

- Applicable to both classiï¬cation and regression problems
- Smarter bagging for trees
- Motivated by theory that generalization **improves with uncorrelated trees**
- Bootstrapped samples and random subset of features are used to train each tree
  - sample rows and columns, æ¯ä¸ªå»train decision tree
  - highly dominate çš„å˜é‡ RF ä¼šå»debiased features
- The outputs from each of the models are averaged to make the ï¬nal prediction.

è¶…**å‚æ•°**ï¼š

- Random Forest hyperparameters:
  - \# of trees
  - \# of features
    - Classiï¬cation - sqrt(# of features) æ˜¯ general guideline
    - Regression - # of featuresï¼Œä¸€èˆ¬ä¸sample feature
- Decision Tree hyperparameters (splitting criteria, maximum depth, etc. ) 

**RandomForestä¸éœ€è¦CVçš„åŸå› **

- æ¯æ¬¡è®­ç»ƒçš„æ—¶å€™ éƒ½åªçœ‹äº†bootstrap sampleï¼Œæœ‰ä¸€éƒ¨åˆ†çš„æ•°æ®æ˜¯æ²¡æœ‰touchçš„
- Uses `out-of-bag (OOB)` error for model selection
  - OOB error is the **average error of** **a data point** calculated using predictions from the trees that do not contain it in their respective bootstrap sample
  - æ¯ä¸ªdata point çš„ error = æ ·æœ¬å¤–é¢„æµ‹çš„errorçš„å¹³å‡
    - å¦‚æœæœ‰ä¸€ä¸ªsampleå»äº†æ‰€æœ‰çš„treeï¼Œé‚£ä¹ˆå®ƒå°±ä¸ä¼šåŠ å…¥out-of-bagçš„è®¡ç®—
    - å¦‚æœæœ‰ä¸€ä¸ªsampleåªå»äº†ä¸€ä¸ªtreeï¼Œé‚£ä¹ˆè¿™ä¸ªtreeçš„errorå°±æ˜¯è¿™ä¸ªdata pointçš„oob error
      - If I built 100 trees and 99 trees used 1 sample and one tree did not use that sample, then that one tree will make a prediction on this, and you can calculate the error from that.

####  **Feature Importances**

- RFæœ‰ä¸¤ç§æ–¹æ³•ï¼š

  - é€šè¿‡è®¡ç®—Giniç³»æ•°çš„å‡å°‘é‡VIm=GIâˆ’(GIL+GIR)åˆ¤æ–­ç‰¹å¾é‡è¦æ€§ï¼Œè¶Šå¤§è¶Šé‡è¦ã€‚

  - å¯¹äºä¸€é¢—æ ‘ï¼Œå…ˆä½¿ç”¨è¢‹å¤–é”™è¯¯ç‡(OOB)æ ·æœ¬è®¡ç®—æµ‹è¯•è¯¯å·®aï¼Œå†éšæœºæ‰“ä¹±OOBæ ·æœ¬ä¸­ç¬¬iä¸ªç‰¹å¾ï¼ˆä¸Šä¸‹æ‰“ä¹±ç‰¹å¾çŸ©é˜µç¬¬iåˆ—çš„é¡ºåºï¼‰åè®¡ç®—æµ‹è¯•è¯¯å·®bï¼Œaä¸bå·®è·è¶Šå¤§ç‰¹å¾iè¶Šé‡è¦ã€‚

    - `è¢‹å¤–æ•°æ®(OOB)`ï¼š å¤§çº¦æœ‰1/3çš„è®­ç»ƒå®ä¾‹æ²¡æœ‰å‚ä¸ç¬¬kæ£µæ ‘çš„ç”Ÿæˆï¼Œå®ƒä»¬ç§°ä¸ºç¬¬$k$æ£µæ ‘çš„è¢‹å¤–æ•°æ®æ ·æœ¬ã€‚

    - åœ¨éšæœºæ£®æ—ä¸­æŸä¸ªç‰¹å¾$X$çš„é‡è¦æ€§çš„è®¡ç®—æ–¹æ³•å¦‚ä¸‹ï¼š

    - å¯¹äºéšæœºæ£®æ—ä¸­çš„æ¯ä¸€é¢—å†³ç­–æ ‘ï¼Œä½¿ç”¨ç›¸åº”çš„OOB(è¢‹å¤–æ•°æ®)æ¥è®¡ç®—å®ƒçš„è¢‹å¤–æ•°æ®è¯¯å·®ï¼Œè®°ä¸º$err_{OOB1}$ã€‚

    - éšæœºåœ°å¯¹è¢‹å¤–æ•°æ®OOBæ‰€æœ‰æ ·æœ¬çš„ç‰¹å¾$X$åŠ å…¥å™ªå£°å¹²æ‰°(å°±å¯ä»¥éšæœºçš„æ”¹å˜æ ·æœ¬åœ¨ç‰¹å¾Xå¤„çš„å€¼)ï¼Œå†æ¬¡è®¡ç®—å®ƒçš„è¢‹å¤–æ•°æ®è¯¯å·®ï¼Œè®°ä¸º$err_{OOB2}$ã€‚

    - å‡è®¾éšæœºæ£®æ—ä¸­æœ‰$N$æ£µæ ‘ï¼Œé‚£ä¹ˆå¯¹äºç‰¹å¾$X$çš„é‡è¦æ€§ä¸º$(err_{OOB2}-err_{OOB1}/N)$ï¼Œä¹‹æ‰€ä»¥å¯ä»¥ç”¨è¿™ä¸ªè¡¨è¾¾å¼æ¥ä½œä¸ºç›¸åº”ç‰¹å¾çš„é‡è¦æ€§çš„åº¦é‡å€¼æ˜¯å› ä¸ºï¼šè‹¥ç»™æŸä¸ªç‰¹å¾éšæœºåŠ å…¥å™ªå£°ä¹‹åï¼Œè¢‹å¤–çš„å‡†ç¡®ç‡å¤§å¹…åº¦é™ä½ï¼Œåˆ™è¯´æ˜è¿™ä¸ªç‰¹å¾å¯¹äºæ ·æœ¬çš„åˆ†ç±»ç»“æœå½±å“å¾ˆå¤§ï¼Œä¹Ÿå°±æ˜¯è¯´å®ƒçš„é‡è¦ç¨‹åº¦æ¯”è¾ƒé«˜ã€‚

- Feature importance is calculated as the **decrease in node impurity** weighted by the *probability of samples in node*s that are reaching that node.
  - The node probability can be calculated by the **number of samples that reach the node**, divided by the total number of samples.
  - æœ‰æ—¶å€™ä¼šé€‰å°‘çš„number of trees ç‰¹åˆ«æ˜¯æå‡ä¸æ˜¾è‘—çš„æ—¶å€™ï¼Œæ¯”å¦‚100ä¸ªtreeå¯èƒ½åªç”¨äº†5ä¸ªfeaturesï¼Œé‚£ä¹ˆåé¢æˆ‘å°±å¯ä»¥åªmaintainè¿™5ä¸ªfeatures
- The higher the value the more important the feature.

<img src="../images/(null)-20220725101112347.(null)" alt="img" style="width:50%;" />

- SKLearnä¸­çš„ `warm_start`
  - When fitting an estimator repeatedly on the same dataset, but for multiple parameter values (such as to find the value maximizing performance as in [grid search](https://scikit-learn.org/stable/modules/grid_search.html#grid-search)), it may be possible to reuse aspects of the model learned from the previous parameter value, saving time. When `warm_start` is true, the existing [fitted](https://scikit-learn.org/stable/glossary.html#term-fitted) model [attributes](https://scikit-learn.org/stable/glossary.html#term-attributes) are used to initialize the new model in a subsequent call to [fit](https://scikit-learn.org/stable/glossary.html#term-fit).
  - Note that this is only applicable for some models and some parameters, and even some orders of parameter values. For example, `warm_start` may be used when building random forests to add more trees to the forest (increasing `n_estimators`) but not to reduce their number.



### Adaptive Boosting

Adaboostç®—æ³•åˆ©ç”¨åŒä¸€ç§åŸºåˆ†ç±»å™¨ï¼ˆå¼±åˆ†ç±»å™¨ï¼‰ï¼ŒåŸºäºåˆ†ç±»å™¨çš„é”™è¯¯ç‡åˆ†é…ä¸åŒçš„æƒé‡å‚æ•°ï¼Œæœ€åç´¯åŠ åŠ æƒçš„é¢„æµ‹ç»“æœä½œä¸ºè¾“å‡ºã€‚

**æµç¨‹**ï¼š

- æ ·æœ¬èµ‹äºˆæƒé‡ï¼Œå¾—åˆ°ç¬¬ä¸€ä¸ªåˆ†ç±»å™¨ã€‚

  Initially, a decision stump classiï¬er (just splits the data into two regions) is ï¬t to the data

- è®¡ç®—è¯¥åˆ†ç±»å™¨çš„é”™è¯¯ç‡ï¼Œæ ¹æ®**é”™è¯¯ç‡èµ‹äºˆåˆ†ç±»å™¨æƒé‡ï¼ˆ**æ³¨æ„è¿™é‡Œæ˜¯åˆ†ç±»å™¨çš„æƒé‡ï¼‰

- <u>å¢åŠ åˆ†é”™æ ·æœ¬çš„æƒé‡ï¼Œå‡å°åˆ†å¯¹æ ·æœ¬çš„æƒé‡</u>ï¼ˆæ³¨æ„è¿™é‡Œæ˜¯æ ·æœ¬çš„æƒé‡ï¼‰

  The data points correctly classiï¬ed are given less weightage while misclassiï¬ed data points are given higher weightage in the next iteration

- ç„¶åå†ç”¨æ–°çš„æ ·æœ¬æƒé‡è®­ç»ƒæ•°æ®ï¼Œå¾—åˆ°æ–°çš„åˆ†ç±»å™¨

  A decision stump classiï¬er is now ï¬t to the data with weights determined in previous iteration

- å¤šæ¬¡è¿­ä»£ï¼Œç›´åˆ°**åˆ†ç±»å™¨é”™è¯¯ç‡ä¸º0æˆ–è€…æ•´ä½“å¼±åˆ†ç±»å™¨é”™è¯¯ä¸º0ï¼Œæˆ–è€…åˆ°è¾¾è¿­ä»£æ¬¡æ•°ã€‚**

- å°†**æ‰€æœ‰å¼±åˆ†ç±»å™¨çš„ç»“æœåŠ æƒæ±‚å’Œ**ï¼Œå¾—åˆ°ä¸€ä¸ªè¾ƒä¸ºå‡†ç¡®çš„åˆ†ç±»ç»“æœã€‚é”™è¯¯ç‡ä½çš„åˆ†ç±»å™¨è·å¾—æ›´é«˜çš„å†³å®šç³»æ•°ï¼Œä»è€Œåœ¨å¯¹æ•°æ®è¿›è¡Œé¢„æµ‹æ—¶èµ·å…³é”®ä½œç”¨ã€‚

  Weights (ğ†) for each classiï¬er (estimated during the training process) are used to combine the outputs and make the ï¬nal prediction.

<img src="../images/(null)-20220725101132539.(null)" alt="img" style="width: 50%;" />

**ç®—æ³•ï¼š**

1. Initialize the observation weights $w_{i}=1 / N, i=1,2, \ldots, N$.

   æœ€å¼€å§‹æ‰€æœ‰è§‚æµ‹éƒ½æ˜¯equal weights

2. For $m=1$ to $M$ è®­ç»ƒMä¸ªclassifier:

   1. Fit a classifier $G_{m}(x)$ to the training data using weights $w_{i}$.

   2. è®¡ç®—å®ƒçš„weighted error = $\frac{é”™è¯¯æ ·æœ¬çš„æ€»æƒé‡}{æ€»æƒé‡}$ï¼š

      $$\operatorname{err}_{m}=\frac{\sum_{i=1}^{N} w_{i} I\left(y_{i} \neq G_{m}\left(x_{i}\right)\right)}{\sum_{i=1}^{N} w_{i}} $$

     3. Compute $\alpha_{m}=\log \left(\left(1-\operatorname{err}_{m}\right) / \operatorname{err}_{m}\right)$ å¾—åˆ°classifierçš„æƒé‡


     4. Set $w_{i} \leftarrow w_{i} \cdot \exp \left[\alpha_{m} \cdot I\left(y_{i} \neq G_{m}\left(x_{i}\right)\right)\right], i=1,2, \ldots, N$


3. Output $G(x)=\operatorname{sign}\left[\sum_{m=1}^{M} \alpha_{m} G_{m}(x)\right]$: æ‰€æœ‰çš„Classifierçš„ç»“æœæ ¹æ®$$\alpha_m$$ä¸ºæƒåŠ æƒå¹³å‡ï¼

**æ•°å­¦ç†è§£**

- ç®—æ³•çš„Assumed Formula $$G(x)=\sum_{m} \alpha_{m} G_{m}(x)$$

  - Assume the Loss function is a exponentially loss function:$$L_{e x p}(x, y)=\exp (-y G(x))$$

  - æ‰€ä»¥ç›®æ ‡å˜æˆäº†$E=\operatorname{Min}_{\alpha_{m}, G_{m}}\left(\sum_{i} \exp \left(-y_{i} \sum_{m} \alpha_{m} G_{m}\left(x_{i}\right)\right)\right)$

    - æ±‚$$\frac{\partial E}{\partial \alpha_{m}}=0$$:

      $$\alpha_{m}=\ln \left(\frac{1-e r r_{m}}{e r r_{m}}\right)$$

      $$\operatorname{err}_{m}=\frac{\sum_{i=1}^{N} w_{i} I\left(y_{i} \neq G_{m}\left(x_{i}\right)\right)}{\sum_{i=1}^{N} w_{i}}$$

**è¶…å‚æ•°**

- Classiï¬cation:
  - \# estimators
  - learning rateï¼šæ¯æ¬¡åŠ å…¥a fraction of the value
  - base estimatorï¼šå¯ä»¥æ¢æˆåˆ«çš„æ¨¡å‹ï¼Œè€Œä¸æ˜¯`decision stump classiï¬er`
- Regressionï¼š
  - loss function
  - learning rate
  - \# of estimators
  - base estimator



### Gradient Boosting

- åˆ†ç±»å›å½’éƒ½å¯ä»¥ï¼ˆåˆ†ç±»çš„è¯ ä¹Ÿæ˜¯æ‹¿probabilityå»regressionï¼‰
- Trains regression trees in a sequential manner on **modiï¬ed versions of the datasets.** 
- Every tree is trained on the residuals of the data points obtained by subtracting the predictions from the previous tree
  - **weights** for each classiï¬er (estimated during the training process) are used to combine the outputs and make the ï¬nal prediction.

<img src="../images/(null)-20220725101152441.(null)" alt="img" style="width:50%;" />

**ç®—æ³•**

Input: training set $\left\{\left(x_{i}, y_{i}\right)\right\}_{i=1}^{n}$, a differentiable loss function $L(y, F(x))$, number of iterations $M$.
Algorithm:
1. Initialize model with a constant value ç¬¬ä¸€æ­¥å…ˆç”¨ä¸€ä¸ªlossæœ€å°çš„å¸¸æ•°æ¥é¢„æµ‹:
$$
F_{0}(x)=\underset{\gamma}{\arg \min } \sum_{i=1}^{n} L\left(y_{i}, \gamma\right) .
$$
2. For $m=1$ to $M$ :
   1. Compute so-called pseudo-residuals è®¡ç®—ä¸€ä¸ªæ®‹å·® å…¶å®ä¹Ÿå°±æ˜¯æ¢¯åº¦ï¼:
$$
r_{i m}=-\left[\frac{\partial L\left(y_{i}, F\left(x_{i}\right)\right)}{\partial F\left(x_{i}\right)}\right]_{F(x)=F_{m-1}(x)} \text { for } i=1, \ldots, n \text {. }
$$
2. Fit a base learner (or weak learner, e.g. tree) $h_{m}(x)$ to pseudo-residuals, i.e. train it using the training set $\left\{\left(x_{i}, r_{i m}\right)\right\}_{i=1}^{n}$
3. Compute multiplier $\gamma_{m}$ by solving the following one-dimensional optimization problem:
$$
\gamma_{m}=\underset{\gamma}{\arg \min } \sum_{i=1}^{n} L\left(y_{i}, F_{m-1}\left(x_{i}\right)+\gamma h_{m}\left(x_{i}\right)\right) .
$$
4. Update the model:
$$
F_{m}(x)=F_{m-1}(x)+\gamma_{m} h_{m}(x) .
$$
3. Output $F_{M}(x)$.

**ä¸ºä»€ä¹ˆå«gradient boostingï¼Ÿ**

- Gradient Descent

<img src="../images/(null)-20220725101152792.(null)" alt="img" style="width:50%;" />

- Gradient Boostingï¼šThe gradient in Gradient boosting is nothing but the residual. As every tree we are boosting the residual (fit a model that does well on that that residual), we are actually boosting the gradient

- ç›®æ ‡å‡½æ•°$E=\operatorname{Min}_{\gamma_{m}, F_{m}}\left(\frac{1}{2} \sum_{i}\left(y_{i}-F\left(x_{i}\right)\right)^{2}\right)$
  - å‡è®¾Functionçš„å½¢å¼æ˜¯$$F(x)=\sum_{m} \gamma_{m} F_{m}(x)$$ï¼Œ ç”¨squared error$$L(x, y)=\frac{1}{2}(y-F(x))^{2}$$
  - å¯ä»¥è®¡ç®—åœ¨mè¿™ä¸ªæ¨¡å‹å‡ºæ¥çš„æ—¶å€™è®¡ç®—çš„Gradient = $$ \frac{\partial E}{\partial F_{m-1}(x)} = - (y - F_{m-1}(x))=$$ç¬¬m-1è½®çš„residualç›¸åæ•°
  - ç„¶åå°±å¾—åˆ°äº†æ›´æ–°è§„åˆ™ï¼š$$F_{m}(x)=F_{m-1}(x)-\gamma \frac{\partial E}{\partial F_{m-1}(x)}$$
    - Gradient = $$ \frac{\partial E}{\partial F_{m-1}(x)} = y - F_{m-1}(x) =$$ç¬¬mè½®è®­ç»ƒæ—¶é¢å¯¹çš„ä¸Šä¸€è½®çš„residual
- åœ¨MSEçš„æƒ…æ³ä¸‹ï¼Œè´ŸGradientåˆšå¥½æ˜¯residualï¼Œè€Œåœ¨å…¶å®ƒæƒ…å†µä¸‹ï¼Œgradient descentçš„æ—¶å€™ä¾ç„¶æ˜¯åœ¨æ²¿ç€gradientçš„æ–¹å‘å­¦ä¹ ä¼˜åŒ–

**è¶…å‚æ•°**

- \# of estimators
- Learning rate
- Decision tree parameters (max depth, min number of samples etc.)
- Regularization parameters
- Row sampling / Column sampling: Pass tree from one to anotherçš„æ—¶å€™å¯ä»¥åªpass a sample of samples.

---

å„ç§Implementation

---

#### **GBDT**

- é¦–å…ˆä»‹ç»Adaboost Treeï¼Œæ˜¯ä¸€ç§boostingçš„æ ‘é›†æˆæ–¹æ³•ã€‚åŸºæœ¬æ€è·¯æ˜¯ä¾æ¬¡è®­ç»ƒå¤šæ£µæ ‘ï¼Œæ¯æ£µæ ‘è®­ç»ƒæ—¶å¯¹åˆ†é”™çš„æ ·æœ¬è¿›è¡ŒåŠ æƒã€‚æ ‘æ¨¡å‹ä¸­å¯¹æ ·æœ¬çš„åŠ æƒå®é™…æ˜¯å¯¹æ ·æœ¬é‡‡æ ·å‡ ç‡çš„åŠ æƒï¼Œåœ¨è¿›è¡Œæœ‰æ”¾å›æŠ½æ ·æ—¶ï¼Œåˆ†é”™çš„æ ·æœ¬æ›´æœ‰å¯èƒ½è¢«æŠ½åˆ°
- GBDTæ˜¯Adaboost Treeçš„æ”¹è¿›ï¼Œæ¯æ£µæ ‘éƒ½æ˜¯CARTï¼ˆåˆ†ç±»å›å½’æ ‘ï¼‰ï¼Œæ ‘åœ¨å¶èŠ‚ç‚¹è¾“å‡ºçš„æ˜¯ä¸€ä¸ªæ•°å€¼ï¼Œåˆ†ç±»è¯¯å·®å°±æ˜¯çœŸå®å€¼å‡å»å¶èŠ‚ç‚¹çš„è¾“å‡ºå€¼ï¼Œå¾—åˆ°æ®‹å·®ã€‚GBDTè¦åšçš„å°±æ˜¯ä½¿ç”¨æ¢¯åº¦ä¸‹é™çš„æ–¹æ³•å‡å°‘åˆ†ç±»è¯¯å·®å€¼ã€‚
- åœ¨GBDTçš„è¿­ä»£ä¸­ï¼Œå‡è®¾æˆ‘ä»¬å‰ä¸€è½®è¿­ä»£å¾—åˆ°çš„å¼ºå­¦ä¹ å™¨æ˜¯ftâˆ’1(x), æŸå¤±å‡½æ•°æ˜¯L(y,ftâˆ’1(x)), æˆ‘ä»¬æœ¬è½®è¿­ä»£çš„ç›®æ ‡æ˜¯æ‰¾åˆ°ä¸€ä¸ªCARTå›å½’æ ‘æ¨¡å‹çš„å¼±å­¦ä¹ å™¨ht(x)ï¼Œè®©æœ¬è½®çš„æŸå¤±æŸå¤±L(y,ft(x)=L(y,ftâˆ’1(x)+ht(x))æœ€å°ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæœ¬è½®è¿­ä»£æ‰¾åˆ°å†³ç­–æ ‘ï¼Œè¦è®©æ ·æœ¬çš„æŸå¤±å°½é‡å˜å¾—æ›´å°ã€‚
- å¾—åˆ°å¤šæ£µæ ‘åï¼Œæ ¹æ®æ¯é¢—æ ‘çš„åˆ†ç±»è¯¯å·®è¿›è¡ŒåŠ æƒæŠ•ç¥¨
- GBDTçš„æ€æƒ³å¯ä»¥ç”¨ä¸€ä¸ªé€šä¿—çš„ä¾‹å­è§£é‡Šï¼Œå‡å¦‚æœ‰ä¸ªäºº30å²ï¼Œæˆ‘ä»¬é¦–å…ˆç”¨20å²å»æ‹Ÿåˆï¼Œå‘ç°æŸå¤±æœ‰10å²ï¼Œè¿™æ—¶æˆ‘ä»¬ç”¨6å²å»æ‹Ÿåˆå‰©ä¸‹çš„æŸå¤±ï¼Œå‘ç°å·®è·è¿˜æœ‰4å²ï¼Œç¬¬ä¸‰è½®æˆ‘ä»¬ç”¨3å²æ‹Ÿåˆå‰©ä¸‹çš„å·®è·ï¼Œå·®è·å°±åªæœ‰ä¸€å²äº†ã€‚å¦‚æœæˆ‘ä»¬çš„è¿­ä»£è½®æ•°è¿˜æ²¡æœ‰å®Œï¼Œå¯ä»¥ç»§ç»­è¿­ä»£ä¸‹é¢ï¼Œæ¯ä¸€è½®è¿­ä»£ï¼Œæ‹Ÿåˆçš„å²æ•°è¯¯å·®éƒ½ä¼šå‡å°ã€‚

#### GradientBoostingClassiï¬er

Early implementation of Gradient Boosting in sklearn

- Most important parametersï¼š
  - of estimators
  - learning rate
- å¥½ç”¨æ€§ï¼š
  - Supports both binary & multi-class classiï¬cation
  - Supports sparse data
- ç¼ºç‚¹ï¼š
  - Typical slow on large datasets
- ç‰¹å¾é‡è¦æ€§ï¼š
  - æ‰€æœ‰å›å½’æ ‘ä¸­é€šè¿‡ç‰¹å¾iåˆ†è£‚å**å¹³æ–¹æŸå¤±çš„å‡å°‘å€¼**çš„å’Œ/å›å½’æ ‘æ•°é‡ å¾—åˆ°ç‰¹å¾é‡è¦æ€§ã€‚
  - åœ¨sklearnä¸­ï¼ŒGBDTå’ŒRFçš„ç‰¹å¾é‡è¦æ€§è®¡ç®—æ–¹æ³•æ˜¯ç›¸åŒçš„ï¼Œéƒ½æ˜¯åŸºäºå•æ£µæ ‘è®¡ç®—æ¯ä¸ªç‰¹å¾çš„é‡è¦æ€§ï¼Œæ¢ç©¶æ¯ä¸ªç‰¹å¾åœ¨æ¯æ£µæ ‘ä¸Šåšäº†å¤šå°‘çš„è´¡çŒ®ï¼Œå†å–ä¸ªå¹³å‡å€¼ã€‚

#### HistGradientBoostingClassiï¬er

- Orders of magnitude **faster** than GradientBoostingClassiï¬er on large datasets 
- Inspired by LightGBM implementation
- Histogram-based split ï¬nding in tree learning
- ç¼ºç‚¹ï¼š
  - Does not support sparse data
  - Does not support monotonicity constraintsï¼šæ¯”å¦‚enforceä¸€ä¸ªå˜é‡çš„ç³»æ•°ä¸ºæ­£/è´Ÿçš„ï¼
    -  the true relationship has some quality, constraints can be used to improve the predictive performance of the model.
- ä¼˜ç‚¹ï¼š
  - Supports both binary & multi-class classiï¬cation
  - Natively supports categorical featuresï¼ˆä¸éœ€è¦Preprocessï¼‰
  - Bin the value into less bins (1000 uniqueæ•°å€¼ -> 10)

<img src="../images/(null)-20220725101152501.(null)" alt="img" style="width:50%;" />



### XGBoost

XGBoosté‡‡ç”¨çš„æ˜¯level-wiseï¼ˆBFSï¼‰ç”Ÿé•¿ç­–ç•¥ï¼Œèƒ½å¤ŸåŒæ—¶åˆ†è£‚åŒä¸€å±‚çš„å¶å­ï¼Œä»è€Œè¿›è¡Œå¤šçº¿ç¨‹ä¼˜åŒ–ã€‚

- åœ¨å†³ç­–æ ‘çš„ç”Ÿé•¿è¿‡ç¨‹ä¸­ï¼Œä¸€ä¸ªéå¸¸å…³é”®çš„é—®é¢˜æ˜¯å¦‚ä½•æ‰¾åˆ°å¶å­çš„èŠ‚ç‚¹çš„æœ€ä¼˜åˆ‡åˆ†ç‚¹"Xgboost æ”¯æŒä¸¤ç§åˆ†è£‚èŠ‚ç‚¹çš„æ–¹æ³•â€”â€”è´ªå¿ƒç®—æ³•å’Œè¿‘ä¼¼ç®—æ³•

  - è´ªå¿ƒç®—æ³•ï¼šé’ˆå¯¹æ¯ä¸ªç‰¹å¾ï¼ŒæŠŠå±äºè¯¥èŠ‚ç‚¹çš„è®­ç»ƒæ ·æœ¬æ ¹æ®è¯¥ç‰¹å¾å€¼è¿›è¡Œå‡åºæ’åˆ—ï¼Œé€šè¿‡çº¿æ€§æ‰«æçš„æ–¹å¼æ¥å†³å®šè¯¥ç‰¹å¾çš„æœ€ä½³åˆ†è£‚ç‚¹ï¼Œå¹¶è®°å½•è¯¥ç‰¹å¾çš„åˆ†è£‚æ”¶ç›Š
  - è¿‘ä¼¼ç®—æ³•ï¼šå¯¹äºæ¯ä¸ªç‰¹å¾ï¼Œé¦–å…ˆæ ¹æ®ç‰¹å¾åˆ†å¸ƒçš„åˆ†ä½æ•°æå‡ºå€™é€‰åˆ’åˆ†ç‚¹ï¼Œç„¶åå°†è¿ç»­å‹ç‰¹å¾æ˜ å°„åˆ°ç”±è¿™äº›å€™é€‰ç‚¹åˆ’åˆ†çš„æ¡¶ä¸­ï¼Œç„¶åèšåˆç»Ÿè®¡ä¿¡æ¯æ‰¾åˆ°æ‰€æœ‰åŒºé—´çš„æœ€ä½³åˆ†è£‚ç‚¹

- ä¼˜ç‚¹

  - æŸå¤±å‡½æ•°è¿›è¡Œäº†äºŒé˜¶æ³°å‹’å±•å¼€ï¼š

    - æ³°å‹’äºŒé˜¶è¿‘ä¼¼æ¯”GBDTä¸€é˜¶è¿‘ä¼¼æ›´æ¥è¿‘çœŸå®çš„Loss Fnctionï¼Œè‡ªç„¶ä¼˜åŒ–çš„æ›´å½»åº•äºŒé˜¶ä¿¡æ¯èƒ½å¤Ÿè®©æ¢¯åº¦æ”¶æ•›çš„æ›´å¿«ï¼Œç±»ä¼¼ç‰›é¡¿æ³•æ¯”SGDæ”¶æ•›æ›´å¿«ã€‚

      äºŒé˜¶ä¿¡æ¯æœ¬èº«å°±èƒ½è®©æ¢¯åº¦æ”¶æ•›æ›´å¿«æ›´å‡†ç¡®ã€‚è¿™ä¸€ç‚¹åœ¨ä¼˜åŒ–ç®—æ³•é‡Œçš„**ç‰›é¡¿æ³•**ä¸­å·²ç»è¯å®ã€‚å¯ä»¥ç®€å•è®¤ä¸ºä¸€é˜¶å¯¼æŒ‡å¼•æ¢¯åº¦æ–¹å‘ï¼ŒäºŒé˜¶å¯¼æŒ‡å¼•æ¢¯åº¦æ–¹å‘å¦‚ä½•å˜åŒ–ã€‚ç®€å•æ¥è¯´ï¼Œç›¸å¯¹äºGBDTçš„ä¸€é˜¶æ³°å‹’å±•å¼€ï¼ŒXGBoosté‡‡ç”¨äºŒé˜¶æ³°å‹’å±•å¼€ï¼Œå¯ä»¥æ›´ä¸ºç²¾å‡†çš„é€¼è¿‘çœŸå®çš„æŸå¤±å‡½æ•°ã€‚

    - èƒ½å¤Ÿè‡ªå®šä¹‰æŸå¤±å‡½æ•°ï¼ŒäºŒé˜¶æ³°å‹’å±•å¼€å¯ä»¥è¿‘ä¼¼å¤§é‡æŸå¤±å‡½æ•°ï¼›

    - æ³¨æ„ï¼šGBDT+MSEçš„æ—¶å€™boostingæ‹Ÿåˆçš„æ‰æ˜¯æ®‹å·®ï¼Œ**XGBoostæ‹Ÿåˆçš„ä¸æ˜¯æ®‹å·®è€Œæ˜¯ç›´æ¥åˆ©ç”¨äº†äºŒé˜¶å¯¼æ•°ä½œä¸ºæ‹Ÿåˆå¯¹è±¡ï¼Œæ‰¾åˆ°è¯¯å·®å‡½æ•°objå‡å°çš„å¹…åº¦**

  - å¯ä»¥åœ¨ç‰¹å¾é¢—ç²’åº¦å¹¶è¡Œè®­ç»ƒ

    - ä¸æ˜¯è¯´æ¯æ£µæ ‘å¯ä»¥å¹¶è¡Œè®­ç»ƒï¼Œ$XGBoost$æœ¬è´¨ä¸Šä»ç„¶é‡‡ç”¨$Boosting$æ€æƒ³ï¼Œæ¯æ£µæ ‘è®­ç»ƒå‰éœ€è¦ç­‰å‰é¢çš„æ ‘è®­ç»ƒå®Œæˆæ‰èƒ½å¼€å§‹è®­ç»ƒã€‚
    - å†³ç­–æ ‘çš„å­¦ä¹ æœ€è€—æ—¶çš„ä¸€ä¸ªæ­¥éª¤å°±æ˜¯å¯¹ç‰¹å¾çš„å€¼è¿›è¡Œæ’åºï¼ˆå› ä¸ºè¦ç¡®å®šæœ€ä½³åˆ†å‰²ç‚¹ï¼‰ï¼ŒXGBooståœ¨è®­ç»ƒä¹‹å‰ï¼Œæ¯ä¸ªç‰¹å¾æŒ‰ç‰¹å¾å€¼å¯¹æ ·æœ¬è¿›è¡Œé¢„æ’åº**å¹¶å­˜å‚¨ä¸ºblockç»“æ„**
    - åœ¨åé¢æŸ¥æ‰¾ç‰¹å¾åˆ†å‰²ç‚¹æ—¶å¯ä»¥é‡å¤ä½¿ç”¨block
    - åªä¸è¿‡åœ¨è¿›è¡ŒèŠ‚ç‚¹çš„åˆ†è£‚æ—¶ï¼Œéœ€è¦è®¡ç®—æ¯ä¸ªç‰¹å¾çš„å¢ç›Šï¼Œæœ€ç»ˆé€‰å¢ç›Šæœ€å¤§çš„é‚£ä¸ªç‰¹å¾å»åšåˆ†è£‚ï¼Œè¿™é‡Œå„ä¸ªç‰¹å¾çš„å¢ç›Šè®¡ç®—ä¹Ÿå¯ä»¥å¤šçº¿ç¨‹è¿›è¡Œ

  - ç›´æ–¹å›¾ï¼šFast approximate split ï¬nding based on histograms

    - **xgbooståœ¨æ¯ä¸€å±‚éƒ½åŠ¨æ€æ„å»ºç›´æ–¹å›¾**ï¼Œåˆ†æ¡¶çš„ä¾æ®æ˜¯æ ·æœ¬çš„äºŒçº§æ¢¯åº¦ï¼Œæ¯ä¸€å±‚éƒ½è¦é‡æ–°æ„å»º

      lightgbmä¸­å¯¹æ¯ä¸ªç‰¹å¾éƒ½æœ‰ä¸€ä¸ªç›´æ–¹å›¾ï¼Œæ‰€ä»¥æ„å»ºä¸€æ¬¡ç›´æ–¹å›¾å°±å¤Ÿäº†

  - åŠ å…¥æ­£åˆ™é¡¹`Adds l1 and l2 penalties on leaf weights`: åŠ å…¥äº†æ­£åˆ™é¡¹ï¼Œç”¨äºæ§åˆ¶æ¨¡å‹çš„å¤æ‚åº¦ã€‚æ­£åˆ™é¡¹é‡ŒåŒ…å«äº†æ ‘çš„å¶å­èŠ‚ç‚¹ä¸ªæ•°ã€å¶å­èŠ‚ç‚¹æƒé‡çš„ L2 èŒƒå¼ã€‚æ­£åˆ™é¡¹é™ä½äº†æ¨¡å‹çš„æ–¹å·®ï¼Œä½¿å­¦ä¹ å‡ºæ¥çš„æ¨¡å‹æ›´åŠ ç®€å•ï¼Œæœ‰åŠ©äºé˜²æ­¢è¿‡æ‹Ÿåˆï¼›

  - **Shrinkageï¼ˆç¼©å‡ï¼‰ï¼š**ç›¸å½“äºå­¦ä¹ é€Ÿç‡ã€‚XGBoost åœ¨è¿›è¡Œå®Œä¸€æ¬¡è¿­ä»£åï¼Œä¼šå°†å¶å­èŠ‚ç‚¹çš„æƒé‡ä¹˜ä¸Šè¯¥ç³»æ•°ï¼Œä¸»è¦æ˜¯ä¸ºäº†å‰Šå¼±æ¯æ£µæ ‘çš„å½±å“ï¼Œè®©åé¢æœ‰æ›´å¤§çš„å­¦ä¹ ç©ºé—´ï¼›

  - å¥½ç”¨çš„ç‚¹ï¼š

    - Supports `GPU training`å—ç»“æ„å¯ä»¥å¾ˆå¥½çš„æ”¯æŒå¹¶è¡Œè®¡ç®—ï½œsparse dataï½œmissing valuesï½œWorks well with pipelines in sklearn due to a compatible interface
      - ç¼ºå¤±å€¼çš„å¤„ç†ï¼ˆLight GBMä¸€æ ·ï¼‰ï¼šå…ˆä¸å¤„ç†é‚£äº›å€¼ç¼ºå¤±çš„æ ·æœ¬ï¼Œé‡‡ç”¨é‚£äº›æœ‰å€¼çš„æ ·æœ¬æå‡ºåˆ†è£‚ç‚¹ï¼Œåœ¨éå†æ¯ä¸ªæœ‰å€¼ç‰¹å¾çš„æ—¶å€™ï¼Œå°è¯•å°†ç¼ºå¤±æ ·æœ¬åˆ’å…¥å·¦å­æ ‘å’Œå³å­æ ‘ï¼Œé€‰æ‹©ä½¿æŸå¤±æœ€ä¼˜çš„å€¼ä½œä¸ºåˆ†è£‚ç‚¹
    - Monotonicity & feature interaction constraints
      - **feature interaction constraints:** when you consider one feature, you don't want to consider another feature in that branch itself. So we can impose such feature interaction constraints as well, in addition to monotonic relationships.

- ç¼ºç‚¹

  - è™½ç„¶åˆ©ç”¨é¢„æ’åºå’Œè¿‘ä¼¼ç®—æ³•å¯ä»¥é™ä½å¯»æ‰¾æœ€ä½³åˆ†è£‚ç‚¹çš„è®¡ç®—é‡ï¼Œä½†åœ¨èŠ‚ç‚¹åˆ†è£‚è¿‡ç¨‹ä¸­ä»éœ€è¦éå†æ•°æ®é›†ï¼›
  - é¢„æ’åºè¿‡ç¨‹çš„ç©ºé—´å¤æ‚åº¦è¿‡é«˜ï¼Œä¸ä»…éœ€è¦å­˜å‚¨ç‰¹å¾å€¼ï¼Œè¿˜éœ€è¦å­˜å‚¨ç‰¹å¾å¯¹åº”æ ·æœ¬çš„æ¢¯åº¦ç»Ÿè®¡å€¼çš„ç´¢å¼•ï¼Œç›¸å½“äºæ¶ˆè€—äº†ä¸¤å€çš„å†…å­˜ã€‚
  - ä½†ä¸åŠ åŒºåˆ†çš„å¯¹å¾…åŒä¸€å±‚çš„å¶å­ï¼Œå¸¦æ¥äº†å¾ˆå¤šæ²¡å¿…è¦çš„å¼€é”€

  - Does not support categorical variables natively

#### **Feature importanceï¼š**

- importance_type=weightï¼ˆé»˜è®¤å€¼ï¼‰ï¼Œç‰¹å¾é‡è¦æ€§ä½¿ç”¨ç‰¹å¾åœ¨æ‰€æœ‰æ ‘ä¸­ä½œä¸ºåˆ’åˆ†å±æ€§çš„æ¬¡æ•°ã€‚

- mportance_type=gainï¼Œç‰¹å¾é‡è¦æ€§ä½¿ç”¨ç‰¹å¾åœ¨ä½œä¸ºåˆ’åˆ†å±æ€§æ—¶losså¹³å‡çš„é™ä½é‡ã€‚

- importance_type=coverï¼Œç‰¹å¾é‡è¦æ€§ä½¿ç”¨ç‰¹å¾åœ¨ä½œä¸ºåˆ’åˆ†å±æ€§æ—¶å¯¹æ ·æœ¬çš„è¦†ç›–åº¦

- `shap value`: Shapley Additive explanationsçš„ç¼©å†™
  - è¾“å‡ºçš„å½¢å¼ï¼š

    - æ¯ä¸ªæ ·æœ¬: å¯ä»¥çœ‹åˆ°æ¯ä¸ªç‰¹å¾çš„shap_valueè´¡çŒ®ï¼ˆæœ‰æ­£è´Ÿï¼‰
    - æ¯ä¸ªç‰¹å¾ï¼šå¯ä»¥çœ‹åˆ°æ•´ä½“æ ·æœ¬ä¸Šçš„Shapç»å¯¹å€¼å–å¹³å‡å€¼æ¥ä»£è¡¨è¯¥ç‰¹å¾çš„é‡è¦æ€§â€”â€”shapå‡å€¼è¶Šå¤§ï¼Œåˆ™ç‰¹å¾è¶Šé‡è¦
  - è¾“å‡ºçš„å›¾æ ‡
    - dependency
    - individual

#### **å¤„ç†è¿‡æ‹Ÿåˆçš„æƒ…å†µ**

é¦–å…ˆXGBæ˜¯åŸºäºBFSçš„ï¼Œå…¶å®æ²¡é‚£ä¹ˆå®¹æ˜“è¿‡æ‹Ÿåˆ

- ç›®æ ‡å‡½æ•°ä¸­å¢åŠ äº†æ­£åˆ™é¡¹ï¼šä½¿ç”¨å¶å­ç»“ç‚¹çš„æ•°ç›®å’Œå¶å­ç»“ç‚¹æƒé‡çš„$L2$æ¨¡çš„å¹³æ–¹ï¼Œæ§åˆ¶æ ‘çš„å¤æ‚åº¦ã€‚

- è®¾ç½®ç›®æ ‡å‡½æ•°çš„**å¢ç›Šé˜ˆå€¼**ï¼šå¦‚æœåˆ†è£‚åç›®æ ‡å‡½æ•°çš„å¢ç›Šå°äºè¯¥é˜ˆå€¼ï¼Œåˆ™ä¸åˆ†è£‚ã€‚

- è®¾ç½®**æœ€å°æ ·æœ¬æƒé‡å’Œ**çš„é˜ˆå€¼ï¼šå½“å¼•å…¥ä¸€æ¬¡åˆ†è£‚åï¼Œé‡æ–°è®¡ç®—æ–°ç”Ÿæˆçš„å·¦ã€å³ä¸¤ä¸ªå¶å­ç»“ç‚¹çš„æ ·æœ¬æƒé‡å’Œã€‚å¦‚æœä»»ä¸€ä¸ªå¶å­ç»“ç‚¹çš„æ ·æœ¬æƒé‡ä½äºæŸä¸€ä¸ªé˜ˆå€¼ï¼ˆæœ€å°æ ·æœ¬æƒé‡å’Œï¼‰ï¼Œä¹Ÿä¼šæ”¾å¼ƒæ­¤æ¬¡åˆ†è£‚ã€‚

- è®¾ç½®æ ‘çš„æœ€å¤§æ·±åº¦ï¼š$XGBoost$ å…ˆä»é¡¶åˆ°åº•å»ºç«‹æ ‘ç›´åˆ°æœ€å¤§æ·±åº¦ï¼Œå†ä»åº•åˆ°é¡¶åå‘æ£€æŸ¥æ˜¯å¦æœ‰**ä¸æ»¡è¶³åˆ†è£‚æ¡ä»¶çš„ç»“ç‚¹ï¼Œè¿›è¡Œå‰ªæã€‚**

- **shrinkage**: å­¦ä¹ ç‡æˆ–æ­¥é•¿é€æ¸ç¼©å°ï¼Œç»™åé¢çš„è®­ç»ƒç•™å‡ºæ›´å¤šçš„å­¦ä¹ ç©ºé—´

- å­é‡‡æ ·ï¼š<u>*æ¯è½®è®¡ç®—å¯ä»¥ä¸ä½¿ç”¨å…¨éƒ¨æ ·æœ¬ï¼Œä½¿ç®—æ³•æ›´åŠ ä¿å®ˆ*</u>

- **åˆ—æŠ½æ ·**ï¼šè®­ç»ƒçš„æ—¶å€™åªç”¨ä¸€éƒ¨åˆ†ç‰¹å¾ï¼ˆä¸è€ƒè™‘å‰©ä½™çš„blockå—å³å¯ï¼‰

**å‚æ•°ï¼š**

- ç¬¬ä¸€ç±»å‚æ•°ï¼šç”¨äºç›´æ¥æ§åˆ¶å½“ä¸ªæ ‘æ¨¡å‹çš„å¤æ‚åº¦ã€‚åŒ…æ‹¬max_depthï¼Œmin_child_weightï¼Œgamma ç­‰å‚æ•°
  - gammaï¼šåœ¨èŠ‚ç‚¹åˆ†è£‚æ—¶ï¼Œåªæœ‰åˆ†è£‚åæŸå¤±å‡½æ•°çš„å€¼ä¸‹é™äº†ï¼Œæ‰ä¼šåˆ†è£‚è¿™ä¸ªèŠ‚ç‚¹ã€‚GammaæŒ‡å®šäº†èŠ‚ç‚¹åˆ†è£‚æ‰€éœ€çš„æœ€å°æŸå¤±å‡½æ•°ä¸‹é™å€¼ã€‚ è¿™ä¸ªå‚æ•°çš„å€¼è¶Šå¤§ï¼Œç®—æ³•è¶Šä¿å®ˆ

- ç¬¬äºŒç±»å‚æ•°ï¼šç”¨äºå¢åŠ éšæœºæ€§ï¼Œä»è€Œä½¿å¾—æ¨¡å‹åœ¨è®­ç»ƒæ—¶å¯¹äºå™ªéŸ³ä¸æ•æ„Ÿã€‚åŒ…æ‹¬ï¼š
  - subsample - æ¯æ£µæ ‘ï¼Œéšæœºé‡‡æ ·çš„æ¯”ä¾‹
  - colsample_bytree - æ§åˆ¶æ¯æ£µéšæœºé‡‡æ ·çš„åˆ—æ•°çš„å æ¯”

- è¿˜æœ‰å°±æ˜¯ç›´æ¥å‡å°learning rateï¼Œä½†éœ€è¦åŒæ—¶å¢åŠ estimator å‚æ•°ã€‚



### LightGBM

ä» LightGBM åå­—æˆ‘ä»¬å¯ä»¥çœ‹å‡ºå…¶æ˜¯è½»é‡çº§ï¼ˆLightï¼‰çš„æ¢¯åº¦æå‡æœºï¼ˆGBMï¼‰ï¼Œå…¶ç›¸å¯¹ XGBoost å…·æœ‰è®­ç»ƒé€Ÿåº¦å¿«ã€å†…å­˜å ç”¨ä½çš„ç‰¹ç‚¹ã€‚

LightGBMé‡‡ç”¨leaf-wiseç”Ÿé•¿ç­–ç•¥ï¼ˆDFSï¼‰ï¼šæ¯æ¬¡ä»å½“å‰æ‰€æœ‰å¶å­ä¸­æ‰¾åˆ°åˆ†è£‚å¢ç›Šæœ€å¤§ï¼ˆä¸€èˆ¬ä¹Ÿæ˜¯æ•°æ®é‡æœ€å¤§ï¼‰çš„ä¸€ä¸ªå¶å­ï¼Œç„¶ååˆ†è£‚ï¼Œå¦‚æ­¤å¾ªç¯ï¼›ä½†ä¼šç”Ÿé•¿å‡ºæ¯”è¾ƒæ·±çš„å†³ç­–æ ‘ï¼Œäº§ç”Ÿè¿‡æ‹Ÿåˆã€‚

- ä¼˜ç‚¹

  - **Histogram**ï¼šç›´æ–¹å›¾ç®—æ³•çš„åŸºæœ¬æ€æƒ³æ˜¯å…ˆæŠŠè¿ç»­çš„æµ®ç‚¹ç‰¹å¾å€¼ç¦»æ•£åŒ–æˆkä¸ªæ•´æ•°ï¼ˆå…¶å®åˆæ˜¯åˆ†æ¡¶çš„æ€æƒ³ï¼Œè€Œè¿™äº›æ¡¶ç§°ä¸ºbinï¼Œæ¯”å¦‚[0,0.1)â†’0, [0.1,0.3)â†’1ï¼‰ï¼ŒåŒæ—¶æ„é€ ä¸€ä¸ªå®½åº¦ä¸ºkçš„ç›´æ–¹å›¾

    å°†å±äºè¯¥ç®±å­çš„æ ·æœ¬æ•°æ®æ›´æ–°ä¸ºç®±å­çš„å€¼ï¼Œç”¨ç›´æ–¹å›¾è¡¨ç¤º

    - **å¯ä»¥å‡å°‘å†…å­˜æ¶ˆè€—ï¼š**å› ä¸ºä¸ç”¨é¢å¤–å­˜å‚¨é¢„æ’åºçš„ç»“æœï¼Œå¯ä»¥åªä¿å­˜ç‰¹å¾ç¦»æ•£åŒ–åçš„å€¼
    - **è®¡ç®—ä»£ä»·æ›´å°**ï¼š
      - é¢„æ’åºç®—æ³•æ¯éå†ä¸€ä¸ªç‰¹å¾å€¼å°±è¦è®¡ç®—ä¸€æ¬¡åœ¨è¿™é‡Œåˆ†è£‚çš„information gainï¼Œä½†ç›´æ–¹å›¾åªéœ€è¦è®¡ç®—kä¸ªç»Ÿçš„æ•°
      - åŒæ—¶ï¼Œä¸€ä¸ªå¶å­çš„ç›´æ–¹å›¾å¯ä»¥ç”±å®ƒçš„çˆ¶äº²èŠ‚ç‚¹çš„ç›´æ–¹å›¾ä¸å®ƒå…„å¼Ÿçš„ç›´æ–¹å›¾åšå·®å¾—åˆ°

  - **å•è¾¹æ¢¯åº¦é‡‡æ ·** `Gradient-based One-Sided Sampling (GOSS)` 

    - GBDT ç®—æ³•çš„æ¢¯åº¦å¤§å°å¯ä»¥ååº”æ ·æœ¬çš„æƒé‡ï¼Œæ¢¯åº¦è¶Šå°è¯´æ˜æ¨¡å‹æ‹Ÿåˆçš„è¶Šå¥½ï¼Œå•è¾¹æ¢¯åº¦æŠ½æ ·ç®—æ³•åˆ©ç”¨è¿™ä¸€ä¿¡æ¯å¯¹æ ·æœ¬è¿›è¡Œ**<u>æŠ½æ ·</u>**ï¼Œå‡å°‘äº†å¤§é‡æ¢¯åº¦å°çš„æ ·æœ¬ï¼Œåœ¨æ¥ä¸‹æ¥çš„è®¡ç®—é”…ä¸­åªéœ€å…³æ³¨æ¢¯åº¦é«˜çš„æ ·æœ¬ï¼Œæå¤§çš„å‡å°‘äº†è®¡ç®—é‡
      - åœ¨å¯¹æ¯ä¸ªtreeåšsamplingä»è€ŒåŠ é€Ÿçš„æ—¶å€™ï¼š**å› ä¸ºæ¯ä¸€æ­¥çš„gradientå°±æ˜¯residualï¼Œæˆ‘å°±å¯ä»¥sample based on this residual. æŠŠæ¢¯åº¦å¤§çš„é€‰å‡ºæ¥ï¼Œ**æ¢¯åº¦å°çš„sample it out. å¯ä»¥è®¾ç½®ä¸€ä¸ªthresholdæŠŠä½çš„ç­›æ‰
    - è¿™ä¸ªæ“ä½œåé¢ä¹Ÿä¼šç”¨æƒé‡å¹³è¡¡å›æ¥ï¼Œè®©**ä¸€æ–¹é¢ç®—æ³•å°†æ›´å¤šçš„æ³¨æ„åŠ›æ”¾åœ¨è®­ç»ƒä¸è¶³çš„æ ·æœ¬ä¸Šï¼Œå¦ä¸€æ–¹é¢é€šè¿‡ä¹˜ä¸Šæƒé‡æ¥é˜²æ­¢é‡‡æ ·å¯¹åŸå§‹æ•°æ®åˆ†å¸ƒé€ æˆå¤ªå¤§çš„å½±å“**

  - **äº’æ–¥ç‰¹å¾æ†ç»‘Exclusive feature `bundling`** to handle sparse features

    - å¦‚æœä¸¤ä¸ªç‰¹å¾å¹¶ä¸å®Œå…¨äº’æ–¥ï¼ˆå¦‚åªæœ‰ä¸€éƒ¨åˆ†æƒ…å†µä¸‹æ˜¯ä¸åŒæ—¶å–éé›¶å€¼ï¼‰ï¼Œå¯ä»¥ç”¨äº’æ–¥ç‡è¡¨ç¤ºäº’æ–¥ç¨‹åº¦ã€‚äº’æ–¥ç‰¹å¾æ†ç»‘ç®—æ³•ï¼ˆExclusive Feature Bundling, EFBï¼‰æŒ‡å‡ºå¦‚æœå°†ä¸€äº›ç‰¹å¾è¿›è¡Œèåˆç»‘å®šï¼Œåˆ™å¯ä»¥é™ä½ç‰¹å¾æ•°é‡ã€‚
    - speed up the process of splitting
    - åœ¨å®é™…åº”ç”¨ä¸­ï¼Œé«˜ç»´åº¦ç‰¹å¾å…·æœ‰ç¨€ç–æ€§ï¼Œè¿™æ ·å¯ä»¥è®¾è®¡ä¸€ä¸ªå‡å°‘æœ‰æ•ˆç‰¹å¾æ•°é‡çš„æ— æŸçš„æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¨€ç–ç‰¹å¾ä¸­ï¼Œè®¸å¤šç‰¹å¾æ˜¯äº’æ–¥çš„ï¼Œå‡ºç°å¤§é‡0ï¼Œä¾‹å¦‚one-hotã€‚æˆ‘ä»¬å¯ä»¥æ†ç»‘äº’æ–¥çš„ç‰¹å¾ã€‚æœ€åæˆ‘ä»¬è¿˜åŸæ†ç»‘äº’æ–¥é—®é¢˜ä¸ºå›¾ç€è‰²é—®é¢˜ï¼Œä½¿ç”¨è´ªå¿ƒç®—æ³•è¿‘ä¼¼æ±‚è§£ã€‚

  - **<u>LightGBM åŸç”Ÿæ”¯æŒç±»åˆ«ç‰¹å¾ï½œ</u>**Supports GPU training, sparse data & missing valuesï½œGenerally faster than XGBoost on CPUsï½œSupports **distributed training** on diï¬€erent frameworks like Ray, Spark, Dask etc.

  - ç¼ºå¤±å€¼å¤„ç†ï¼šæ¯æ¬¡åˆ†å‰²çš„æ—¶å€™ï¼Œåˆ†åˆ«æŠŠç¼ºå¤±å€¼æ”¾åœ¨å·¦å³ä¸¤è¾¹å„è®¡ç®—ä¸€æ¬¡ï¼Œç„¶åæ¯”è¾ƒä¸¤ç§æƒ…å†µçš„å¢ç›Šï¼Œæ‹©ä¼˜å½•å–
  
  - å¸¦æ·±åº¦é™åˆ¶ä¸è¿‡æ‹Ÿåˆçš„æƒ…å†µä¸‹ï¼ŒLeaf-wiseçš„å¶å­ç”Ÿé•¿ç­–ç•¥æ•ˆç‡æ›´é«˜
  
    - åœ¨XGBoostä¸­ï¼Œæ ‘æ˜¯æŒ‰å±‚ç”Ÿé•¿çš„ï¼Œç§°ä¸ºLevel-wise tree growthï¼ŒåŒä¸€å±‚çš„æ‰€æœ‰èŠ‚ç‚¹éƒ½åšåˆ†è£‚ï¼Œæœ€åå‰ªæ
  
    - Level-wiseè¿‡ä¸€æ¬¡æ•°æ®å¯ä»¥åŒæ—¶åˆ†è£‚åŒä¸€å±‚çš„å¶å­ï¼Œå®¹æ˜“è¿›è¡Œå¤šçº¿ç¨‹ä¼˜åŒ–ï¼Œä¹Ÿå¥½æ§åˆ¶æ¨¡å‹å¤æ‚åº¦ï¼Œä¸å®¹æ˜“è¿‡æ‹Ÿåˆã€‚ä½†å®é™…ä¸ŠLevel-wiseæ˜¯ä¸€ç§ä½æ•ˆçš„ç®—æ³•ï¼Œå› ä¸ºå®ƒä¸åŠ åŒºåˆ†çš„å¯¹å¾…åŒä¸€å±‚çš„å¶å­ï¼Œå¸¦æ¥äº†å¾ˆå¤šæ²¡å¿…è¦çš„å¼€é”€ï¼Œå› ä¸ºå®é™…ä¸Šå¾ˆå¤šå¶å­çš„åˆ†è£‚å¢ç›Šè¾ƒä½ï¼Œæ²¡å¿…è¦è¿›è¡Œæœç´¢å’Œåˆ†è£‚ã€‚
  
    - åœ¨Histogramç®—æ³•ä¹‹ä¸Šï¼ŒLightGBMè¿›è¡Œè¿›ä¸€æ­¥çš„ä¼˜åŒ–ã€‚é¦–å…ˆå®ƒæŠ›å¼ƒäº†å¤§å¤šæ•°GBDTå·¥å…·ä½¿ç”¨çš„æŒ‰å±‚ç”Ÿé•¿ (level-wise) çš„å†³ç­–æ ‘ç”Ÿé•¿ç­–ç•¥ï¼Œè€Œä½¿ç”¨äº†å¸¦æœ‰æ·±åº¦é™åˆ¶çš„æŒ‰å¶å­ç”Ÿé•¿ (leaf-wise)ç®—æ³•ã€‚
  
    - Leaf-wiseåˆ™æ˜¯ä¸€ç§æ›´ä¸ºé«˜æ•ˆçš„ç­–ç•¥ï¼Œæ¯æ¬¡ä»å½“å‰æ‰€æœ‰å¶å­ä¸­ï¼Œæ‰¾åˆ°åˆ†è£‚å¢ç›Šæœ€å¤§çš„ä¸€ä¸ªå¶å­ï¼Œç„¶ååˆ†è£‚ï¼Œå¦‚æ­¤å¾ªç¯ã€‚å› æ­¤åŒLevel-wiseç›¸æ¯”ï¼Œ**åœ¨åˆ†è£‚æ¬¡æ•°ç›¸åŒçš„æƒ…å†µä¸‹ï¼ŒLeaf-wiseå¯ä»¥é™ä½æ›´å¤šçš„è¯¯å·®ï¼Œå¾—åˆ°æ›´å¥½çš„ç²¾åº¦**ã€‚Leaf-wiseçš„ç¼ºç‚¹æ˜¯å¯èƒ½ä¼šé•¿å‡ºæ¯”è¾ƒæ·±çš„å†³ç­–æ ‘ï¼Œäº§ç”Ÿè¿‡æ‹Ÿåˆã€‚å› æ­¤LightGBMåœ¨Leaf-wiseä¹‹ä¸Šå¢åŠ äº†ä¸€ä¸ªæœ€å¤§æ·±åº¦çš„é™åˆ¶ï¼Œåœ¨ä¿è¯é«˜æ•ˆç‡çš„åŒæ—¶é˜²æ­¢è¿‡æ‹Ÿåˆã€‚
  
  - ç›´æ¥æ”¯æŒç±»åˆ«ç‰¹å¾(Categorical Feature)



### XGBoostå’ŒLightGBMçš„åŒºåˆ«

- *ï¼ˆmodelç»“æ„çš„ä¸åŒï¼‰*

  - æ ‘ç”Ÿé•¿ç­–ç•¥ä¸åŒ
    - XGBé‡‡ç”¨level-wiseçš„åˆ†è£‚ç­–ç•¥ï¼šXGBå¯¹æ¯ä¸€å±‚æ‰€æœ‰èŠ‚ç‚¹åšæ— å·®åˆ«åˆ†è£‚ï¼Œä½†æ˜¯å¯èƒ½æœ‰äº›èŠ‚ç‚¹å¢ç›Šéå¸¸å°ï¼Œå¯¹ç»“æœå½±å“ä¸å¤§ï¼Œå¸¦æ¥ä¸å¿…è¦çš„å¼€é”€ã€‚

    - LGBé‡‡ç”¨leaf-wiseçš„åˆ†è£‚ç­–ç•¥ï¼šLeaf-wiseæ˜¯åœ¨æ‰€æœ‰å¶å­èŠ‚ç‚¹ä¸­é€‰å–åˆ†è£‚æ”¶ç›Šæœ€å¤§çš„èŠ‚ç‚¹è¿›è¡Œçš„ï¼Œä½†æ˜¯å¾ˆå®¹æ˜“å‡ºç°è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œæ‰€ä»¥éœ€è¦å¯¹æœ€å¤§æ·±åº¦åšé™åˆ¶ 

  - æ ‘å¯¹ç‰¹å¾åˆ†å‰²ç‚¹æŸ¥æ‰¾ç®—æ³•ä¸åŒï¼š
    - XGBä½¿ç”¨ç‰¹å¾é¢„æ’åºç®—æ³•

    - LGBä½¿ç”¨åŸºäºç›´æ–¹å›¾çš„åˆ‡åˆ†ç‚¹ç®—æ³•ï¼š
      - å‡å°‘å†…å­˜å ç”¨ï¼Œæ¯”å¦‚ç¦»æ•£ä¸º256ä¸ªbinæ—¶ï¼Œåªéœ€è¦ç”¨8ä½æ•´å½¢å°±å¯ä»¥ä¿å­˜ä¸€ä¸ªæ ·æœ¬è¢«æ˜ å°„ä¸ºå“ªä¸ªbin(è¿™ä¸ªbinå¯ä»¥è¯´å°±æ˜¯è½¬æ¢åçš„ç‰¹å¾)ï¼Œå¯¹æ¯”é¢„æ’åºçš„exact greedyç®—æ³•æ¥è¯´ï¼ˆç”¨int_32æ¥å­˜å‚¨ç´¢å¼•+ ç”¨float_32ä¿å­˜ç‰¹å¾å€¼ï¼‰ï¼Œå¯ä»¥èŠ‚çœ7/8çš„ç©ºé—´ã€‚

      - è®¡ç®—æ•ˆç‡æé«˜ï¼Œé¢„æ’åºçš„Exact greedyå¯¹æ¯ä¸ªç‰¹å¾éƒ½éœ€è¦éå†ä¸€éæ•°æ®ï¼Œå¹¶è®¡ç®—å¢ç›Šã€‚è€Œç›´æ–¹å›¾ç®—æ³•åœ¨å»ºç«‹å®Œç›´æ–¹å›¾åï¼Œåªéœ€è¦å¯¹æ¯ä¸ªç‰¹å¾éå†ç›´æ–¹å›¾å³å¯

    - è¿™ä¸ªç‰¹å¾åˆ†å‰²ç‚¹æŸ¥æ‰¾çš„åŒºåˆ«å…¶å®ä¹Ÿè·Ÿåˆ†è£‚æ–¹å¼æœ‰å…³
      - XGB åœ¨æ¯ä¸€å±‚éƒ½åŠ¨æ€æ„å»ºç›´æ–¹å›¾ï¼Œ å› ä¸ºXGBçš„ç›´æ–¹å›¾ç®—æ³•ä¸æ˜¯é’ˆå¯¹æŸä¸ªç‰¹å®šçš„featureï¼Œè€Œæ˜¯æ‰€æœ‰featureå…±äº«ä¸€ä¸ªç›´æ–¹å›¾(æ¯ä¸ªæ ·æœ¬çš„æƒé‡æ˜¯äºŒé˜¶å¯¼)ï¼Œæ‰€ä»¥**æ¯ä¸€å±‚éƒ½è¦é‡æ–°æ„å»ºç›´æ–¹å›¾ã€‚**

      - LGBä¸­å¯¹æ¯ä¸ªç‰¹å¾éƒ½æœ‰ä¸€ä¸ªç›´æ–¹å›¾ï¼Œæ‰€ä»¥æ„å»ºä¸€æ¬¡ç›´æ–¹å›¾å°±å¤Ÿäº†ï¼Œè€Œä¸”LGBè¿˜å¯ä»¥ä½¿ç”¨ç›´æ–¹å›¾åšå·®åŠ é€Ÿï¼Œä¸€ä¸ªèŠ‚ç‚¹çš„ç›´æ–¹å›¾å¯ä»¥é€šè¿‡çˆ¶èŠ‚ç‚¹çš„ç›´æ–¹å›¾å‡å»å…„å¼ŸèŠ‚ç‚¹çš„ç›´æ–¹å›¾å¾—åˆ°ï¼Œä»è€ŒåŠ é€Ÿè®¡ç®—

- *ï¼ˆnçš„ä¸åŒï¼‰*æ ·æœ¬é€‰æ‹©ï¼šä¼šåšå•è¾¹æ¢¯åº¦é‡‡æ ·ï¼ŒLightGBMä¼šå°†æ›´å¤šçš„æ³¨æ„åŠ›æ”¾åœ¨è®­ç»ƒä¸è¶³çš„æ ·æœ¬ä¸Š

- *ï¼ˆrunçš„ä¸åŒï¼‰*å¹¶è¡Œç­–ç•¥æœ‰å·®å¼‚ï¼š

  - XGBæ˜¯ç‰¹å¾å¹¶è¡Œ

    - XGBæ¯ä¸ªworkerèŠ‚ç‚¹ä¸­ä»…æœ‰éƒ¨åˆ†çš„åˆ—æ•°æ®ï¼Œä¹Ÿå°±æ˜¯å‚ç›´åˆ‡åˆ†ï¼Œæ¯ä¸ªworkerå¯»æ‰¾å±€éƒ¨æœ€ä½³åˆ‡åˆ†ç‚¹ï¼Œworkerä¹‹é—´ç›¸äº’é€šä¿¡ï¼Œç„¶ååœ¨å…·æœ‰æœ€ä½³åˆ‡åˆ†ç‚¹çš„workerä¸Šè¿›è¡ŒèŠ‚ç‚¹åˆ†è£‚ï¼Œå†ç”±è¿™ä¸ªèŠ‚ç‚¹å¹¿æ’­ä¸€ä¸‹è¢«åˆ‡åˆ†åˆ°å·¦å³èŠ‚ç‚¹çš„æ ·æœ¬ç´¢å¼•å·ï¼Œå…¶ä»–workeræ‰èƒ½å¼€å§‹åˆ†è£‚ã€‚

    - LGBç‰¹å¾å¹¶è¡Œçš„å‰ææ˜¯æ¯ä¸ªworkerç•™æœ‰ä¸€ä»½å®Œæ•´çš„æ•°æ®é›†ï¼Œä½†æ˜¯æ¯ä¸ªworkerä»…åœ¨ç‰¹å¾å­é›†ä¸Šè¿›è¡Œæœ€ä½³åˆ‡åˆ†ç‚¹çš„å¯»æ‰¾ï¼›workerä¹‹é—´éœ€è¦ç›¸äº’é€šä¿¡ï¼Œé€šè¿‡æ¯”å¯¹æŸå¤±æ¥ç¡®å®šæœ€ä½³åˆ‡åˆ†ç‚¹ï¼›ç„¶åå°†è¿™ä¸ªæœ€ä½³åˆ‡åˆ†ç‚¹çš„ä½ç½®è¿›è¡Œå…¨å±€å¹¿æ’­ï¼Œæ¯ä¸ªworkerè¿›è¡Œåˆ‡åˆ†å³å¯ã€‚

  - LihgtGBMæ˜¯æ•°æ®å¹¶è¡Œ

    - LGBä¸­å…ˆå¯¹æ•°æ®æ°´å¹³åˆ‡åˆ†ï¼Œæ¯ä¸ªworkerä¸Šçš„æ•°æ®å…ˆå»ºç«‹èµ·å±€éƒ¨çš„ç›´æ–¹å›¾ï¼Œç„¶ååˆå¹¶æˆå…¨å±€çš„ç›´æ–¹å›¾ï¼Œé‡‡ç”¨ç›´æ–¹å›¾ç›¸å‡çš„æ–¹å¼ï¼Œå…ˆè®¡ç®—æ ·æœ¬é‡å°‘çš„èŠ‚ç‚¹çš„æ ·æœ¬ç´¢å¼•ï¼Œç„¶åç›´æ¥ç›¸å‡å¾—åˆ°å¦ä¸€å­èŠ‚ç‚¹çš„æ ·æœ¬ç´¢å¼•ï¼Œè¿™ä¸ªç›´æ–¹å›¾ç®—æ³•ä½¿å¾—workeré—´çš„é€šä¿¡æˆæœ¬é™ä½ä¸€å€ï¼Œå› ä¸ºåªç”¨é€šä¿¡ä»¥æ­¤æ ·æœ¬é‡å°‘çš„èŠ‚ç‚¹

    - XGBä¸­çš„æ•°æ®å¹¶è¡Œä¹Ÿæ˜¯æ°´å¹³åˆ‡åˆ†ï¼Œç„¶åå•ä¸ªworkerå»ºç«‹å±€éƒ¨ç›´æ–¹å›¾ï¼Œå†åˆå¹¶ä¸ºå…¨å±€ï¼Œä¸åŒåœ¨äºæ ¹æ®å…¨å±€ç›´æ–¹å›¾è¿›è¡Œå„ä¸ªworkerä¸Šçš„èŠ‚ç‚¹åˆ†è£‚æ—¶ä¼šå•ç‹¬è®¡ç®—å­èŠ‚ç‚¹çš„æ ·æœ¬ç´¢å¼•

- è¿˜æœ‰ä¸€äº›å°çš„åŒºåˆ«ï¼š

  - ç¦»æ•£å˜é‡å¤„ç†ä¸Šï¼šXGBæ— æ³•ç›´æ¥è¾“å…¥ç±»åˆ«å‹å˜é‡å› æ­¤éœ€è¦äº‹å…ˆå¯¹ç±»åˆ«å‹å˜é‡è¿›è¡Œç¼–ç ï¼ˆä¾‹å¦‚ç‹¬çƒ­ç¼–ç ï¼‰ï¼ŒLGBå¯ä»¥ç›´æ¥å¤„ç†ç±»åˆ«å‹å˜é‡
  - è¯†åˆ«ä¸€äº›äº’æ–¥çš„ç‰¹å¾ä¸Šï¼ŒLightGBMå¯ä»¥bundlingç­‰ç­‰

### CatBoost

- Optimized for **categorical** features
- Uses `target encoding` to handle categorical features
- Uses ordered boosting to build "symmetirc" trees
  - ç»™æ¯ä¸ª sampleç¼–ä¸€ä¸ªtime ï¼ˆ incorporate a sense of time on the data itself, which means that these samples have occurred before and these samples have occurred later.ï¼‰
  - Every three trains on a portion of the data based on that time and then it makes a prediction on the other part.
- Overfitting dertector
- Supports GPU training, sparse data & missing values
- Monotonicity constraints



## æ—¶é—´åºåˆ—ç±»

### Prophet

å¸¸è§çš„æ—¶é—´åºåˆ—åˆ†è§£æ–¹æ³•

å°†æ—¶é—´åºåˆ—åˆ†æˆå­£èŠ‚é¡¹$S_t$ï¼Œè¶‹åŠ¿é¡¹$T_t$ï¼Œå‰©ä½™é¡¹$R_t$ï¼Œå³å¯¹æ‰€æœ‰çš„$tâ‰¥0$
$$
y_{t}=S_{t}+T_{t}+R_{t}	
$$

$$
y_{t}=S_{t} \times T_{t} \times R_{t}
$$

$$
\ln y_{t}=\ln S_{t}+\ln T_{t}+\ln R_{t}
$$

fbprophet çš„åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæ·»åŠ äº†èŠ‚æ—¥é¡¹ï¼š
$$
y(t)=g(t)+s(t)+h(t)+\epsilon_{t}
$$

- `è¶‹åŠ¿é¡¹æ¨¡å‹`: åŸºäºé€»è¾‘å›å½’

  - prophetåœ¨é€»è¾‘å›å½’çš„åŸºç¡€ä¸Šæ·»åŠ äº†éšæ—¶é—´å˜åŒ–çš„å‚æ•°ï¼Œé‚£ä¹ˆé€»è¾‘å›å½’å°±å¯ä»¥æ”¹å†™æˆï¼š

    $$
    f(x)=\frac{C(t)}{\left(1+e^{-k(t)(x-m(t))}\right)}
    $$
    
    -  $C$ ç§°ä¸ºæ›²çº¿çš„æœ€å¤§æ¸è¿‘å€¼ï¼Œ $k$ è¡¨ç¤ºæ›²çº¿çš„å¢é•¿ç‡ï¼Œ$m$  è¡¨ç¤ºæ›²çº¿çš„ä¸­ç‚¹
    - å½“ $$
      C=1, k=1, m=0
      $$æ—¶ï¼Œæ°å¥½å°±æ˜¯å¤§å®¶å¸¸è§çš„ sigmoid å‡½æ•°çš„å½¢å¼$\sigma(x)=1 /\left(1+e^{-x}\right)$

  * **åŸºäºåˆ†æ®µçº¿æ€§å‡½æ•°**
    $$
    g(t)=\frac{C(t)}{1+\exp \left(-\left(k+\boldsymbol{a}(t)^{t} \boldsymbol{\delta}\right) \cdot\left(t-\left(m+\boldsymbol{a}(t)^{T} \boldsymbol{\gamma}\right)\right.\right.}
    $$
    $k$è¡¨ç¤ºå˜åŒ–é‡

    $a_{j}(t)$è¡¨ç¤ºæŒ‡ç¤ºå‡½æ•°ï¼š
    $$
    a_{j}(t)=\left\{\begin{array}{l}1, \text { if } t \geq s_{j} \\ 0, \text { otherwise }\end{array}\right.
    $$
    $\delta_{j}$è¡¨ç¤ºåœ¨æ—¶é—´æˆ³$s_{j}$ä¸Šçš„å¢é•¿ç‡çš„å˜åŒ–é‡

    $\gamma_{j}$ç¡®å®šçº¿æ®µè¾¹ç•Œ
    $$
    \gamma_{j}=\left(s_{j}-m-\sum_{\ell<j} \gamma_{\ell}\right) \cdot\left(1-\frac{k+\sum_{\ell<j} \delta_{\ell}}{k+\sum_{\ell \leq j} \delta_{\ell}}\right)
    $$
    å…¶ä¸­ï¼š
    $$
    \boldsymbol{a}(t)=\left(a_{1}(t), \cdots, a_{S}(t)\right)^{T}, \boldsymbol{\delta}=\left(\delta_{1}, \cdots, \delta_{S}\right)^{T}, \boldsymbol{\gamma}=\left({\gamma}_{1}, \cdots, \gamma_{S}\right)^{T}
    $$

- å­£èŠ‚æ€§è¶‹åŠ¿ï¼š

  - æ—¶é—´åºåˆ—é€šå¸¸ä¼šéšç€å¤©ï¼Œå‘¨ï¼Œæœˆï¼Œå¹´ç­‰å­£èŠ‚æ€§çš„å˜åŒ–è€Œå‘ˆç°å­£èŠ‚æ€§çš„å˜åŒ–ï¼Œä¹Ÿç§°ä¸ºå‘¨æœŸæ€§çš„å˜åŒ–

  - prophetç®—æ³•ä½¿ç”¨å‚…ç«‹å¶çº§æ•°æ¥æ¨¡æ‹Ÿæ—¶é—´åºåˆ—çš„å‘¨æœŸæ€§

  - å‚…ç«‹å¶çº§æ•°çš„å½¢å¼ï¼š
    $$
    s(t)=\sum_{n=1}^{N}\left(a_{n} \cos \left(\frac{2 \pi n t}{P}\right)+b_{n} \sin \left(\frac{2 \pi n t}{P}\right)\right)
    $$
    

    - $P$è¡¨ç¤ºæ—¶é—´åºåˆ—çš„å‘¨æœŸï¼Œ $P = 365.25$è¡¨ç¤ºä»¥å¹´ä¸ºå‘¨æœŸï¼Œ$P = 7$è¡¨ç¤ºä»¥å‘¨ä¸ºå‘¨æœŸã€‚	

- èŠ‚å‡æ—¥æ•ˆåº”ï¼ˆholidays and eventsï¼‰

  é™¤äº†å‘¨æœ«ï¼ŒåŒæ ·æœ‰å¾ˆå¤šèŠ‚å‡æ—¥ï¼Œè€Œä¸”ä¸åŒçš„å›½å®¶æœ‰ç€ä¸åŒçš„å‡æœŸï¼Œä¸åŒçš„èŠ‚å‡æ—¥å¯ä»¥çœ‹æˆç›¸äº’ç‹¬ç«‹çš„æ¨¡å‹ï¼Œå¹¶ä¸”å¯ä»¥ä¸ºä¸åŒçš„èŠ‚å‡æ—¥è®¾ç½®ä¸åŒçš„å‰åçª—å£å€¼ï¼Œè¡¨ç¤ºè¯¥èŠ‚å‡æ—¥ä¼šå½±å“å‰åä¸€æ®µæ—¶é—´çš„æ—¶é—´åºåˆ—

  $$h(t)=Z(t) \boldsymbol{\kappa}=\sum_{i=1}^{L} \kappa_{i} \cdot 1_{\left\{t \in D_{i}\right\}}$$

  - å…¶ä¸­ï¼š$Z(t)=\left(1_{\left\{t \in D_{1}\right\}}, \cdots, 1_{\left\{t \in D_{L}\right\}}\right), \boldsymbol{\kappa}=\left(\kappa_{1}, \cdots, \kappa_{L}\right)^{T}$ï¼Œ$\boldsymbol{\kappa} \sim \operatorname{Normal}\left(0, v^{2}\right)$
  - è¯¥æ­£æ€åˆ†å¸ƒæ˜¯å—åˆ°$v$ = holidays_prior_scale è¿™ä¸ªæŒ‡æ ‡å½±å“çš„ã€‚é»˜è®¤å€¼æ˜¯ 10ï¼Œå½“å€¼è¶Šå¤§æ—¶ï¼Œè¡¨ç¤ºèŠ‚å‡æ—¥å¯¹æ¨¡å‹çš„å½±å“è¶Šå¤§ï¼›å½“å€¼è¶Šå°æ—¶ï¼Œè¡¨ç¤ºèŠ‚å‡æ—¥å¯¹æ¨¡å‹çš„æ•ˆæœè¶Šå°

- å˜ç‚¹çš„é€‰æ‹©ï¼šåœ¨ Prophet ç®—æ³•ä¸­ï¼Œéœ€è¦ç»™å‡ºå˜ç‚¹çš„ä½ç½®ã€ä¸ªæ•°åŠå¢é•¿çš„å˜åŒ–ç‡ï¼š

  - changepoint_range

    changepoint_range æŒ‡çš„æ˜¯ç™¾åˆ†æ¯”ï¼Œéœ€è¦åœ¨å‰ changepoint_range é‚£ä¹ˆé•¿çš„æ—¶é—´åºåˆ—ä¸­è®¾ç½®å˜ç‚¹


  - n_changepoint

    n_changepoint è¡¨ç¤ºå˜ç‚¹çš„ä¸ªæ•°ï¼Œåœ¨é»˜è®¤çš„å‡½æ•°ä¸­æ˜¯ n_changepoint = 25


  - changepoint_prior_scaleã€‚

    changepoint_prior_scale è¡¨ç¤ºå˜ç‚¹å¢é•¿ç‡çš„åˆ†å¸ƒæƒ…å†µ
    $$
    \delta_{j} \sim \operatorname{Laplace}(0, \tau)
    $$
    $\mathcal{T}$å°±æ˜¯ change_point_scale


**é¢„ä¼°çš„æ–¹å¼**

- å¯¹äºå·²çŸ¥çš„æ—¶é—´åºåˆ—ï¼Œå¯ä»¥æ‰‹åŠ¨è®¾ç½®sä¸ªå˜ç‚¹
- å¯¹äºé¢„æµ‹çš„æ•°æ®æ¨¡å‹ä½¿ç”¨Poissonåˆ†å¸ƒæ‰¾åˆ°æ–°å¢çš„å˜ç‚¹ï¼Œç„¶åä¸å·²çŸ¥çš„å˜ç‚¹è¿›è¡Œæ‹¼æ¥

**æ¨¡å‹å‚æ•°**

1. Capacityï¼šåœ¨å¢é‡å‡½æ•°æ˜¯é€»è¾‘å›å½’å‡½æ•°çš„æ—¶å€™ï¼Œéœ€è¦è®¾ç½®çš„å®¹é‡å€¼ã€‚

2. Change Pointsï¼šå¯ä»¥é€šè¿‡ n_changepoints å’Œ changepoint_range æ¥è¿›è¡Œç­‰è·çš„å˜ç‚¹è®¾ç½®ï¼Œä¹Ÿå¯ä»¥é€šè¿‡äººå·¥è®¾ç½®çš„æ–¹å¼æ¥æŒ‡å®šæ—¶é—´åºåˆ—çš„å˜ç‚¹ã€‚

3. å­£èŠ‚æ€§å’ŒèŠ‚å‡æ—¥ï¼šå¯ä»¥æ ¹æ®å®é™…çš„ä¸šåŠ¡éœ€æ±‚æ¥æŒ‡å®šç›¸åº”çš„èŠ‚å‡æ—¥ã€‚

4. å…‰æ»‘å‚æ•°ï¼š

   $\tau$ = changepoint_prior_scale å¯ä»¥ç”¨æ¥æ§åˆ¶è¶‹åŠ¿çš„çµæ´»åº¦

   $\sigma$ = seasonality_prior_scale ç”¨æ¥æ§åˆ¶å­£èŠ‚é¡¹çš„çµæ´»åº¦ï¼Œ

   $v$ =  holidays prior scale ç”¨æ¥æ§åˆ¶èŠ‚å‡æ—¥çš„çµæ´»åº¦ã€‚



# Reference

[çŸ¥ä¹|å°±æ˜¯æ¨å®—|SVMçš„æ ¸å‡½æ•°å¦‚ä½•é€‰å–ï¼Ÿ](https://www.zhihu.com/question/21883548/answer/205191440)

[çŸ¥ä¹|é˜¿æ³½|ã€æœºå™¨å­¦ä¹ ã€‘å†³ç­–æ ‘ï¼ˆä¸‹ï¼‰â€”â€”XGBoostã€LightGBMï¼ˆéå¸¸è¯¦ç»†ï¼‰](https://zhuanlan.zhihu.com/p/87885678)

