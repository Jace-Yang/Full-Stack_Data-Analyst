
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Transformer &#8212; Towards a Full-stack DA</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet">
  <link href="../../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="BERT" href="BERT.html" />
    <link rel="prev" title="Self-Attention" href="Self-attention.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/logo2.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Towards a Full-stack DA</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../intro.html">
   Jace六边形DA笔记库
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  数据ETL流程
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../Data_ETL/README.html">
   DE基础
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../Data_ETL/Data_cleaning/README.html">
   数据清洗与处理
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Data_ETL/Data_cleaning/Regex.html">
     正则表达式
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Data_ETL/Data_cleaning/pandas.html">
     Pandas
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  统计分析
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../STAT/README.html">
   基础
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  因果推断
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../Causal_Inference/README.html">
   基础
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../Causal_Inference/1_AB_testing.html">
   AB Test
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  深度学习
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Basics/README.html">
   基础
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Basics/DL_hyperparameter.html">
     DL常见超参及调整策略
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Basics/Optimizer.html">
     优化器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Basics/Trained_by_GPU.html">
     GPU训练
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="README.html">
   NLP
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="basics.html">
     基础概念
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Self-attention.html">
     Self-Attention
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Transformer
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="BERT.html">
     BERT
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../NN_compression/README.html">
   神经网络压缩
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../NN_compression/KD.html">
     知识蒸馏
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
    <label for="toctree-checkbox-5">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../NN_compression/Distill_Bert.html">
       Distill Bert
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../NN_compression/Quantize.html">
     量化
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
    <label for="toctree-checkbox-6">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../NN_compression/QBert.html">
       Q-BERT
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/Deep_Learning/NLP/Transformer.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/Jace-Yang/Full-Stack_Data-Analyst"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/Jace-Yang/Full-Stack_Data-Analyst/issues/new?title=Issue%20on%20page%20%2FDeep_Learning/NLP/Transformer.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        <a class="edit-button" href="https://github.com/Jace-Yang/Full-Stack_Data-Analyst/edit/master/Deep_Learning/NLP/Transformer.md"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   引言
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   李宏毅机器学习笔记
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sequence-to-sequence-seq2seq">
     Sequence-to-sequence (Seq2seq)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id3">
       例子
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id4">
     Transformer架构
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#encoder">
       Encoder
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#decoder-autoregressive">
       Decoder - AutoRegressive
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#encoder-decoder">
       Encoder-Decoder
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id5">
     训练
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id6">
     评估结果
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id7">
     拓展
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#attention-is-all-you-need">
   Attention Is All You Need读论文笔记
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#introduction">
     1. Introduction
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#backgound">
     2. Backgound
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model-architecture">
     3. Model Architecture
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id8">
       总体结构
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id9">
       Encoder层
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#embedding">
         Embedding层
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#self-attention">
         Self-attention层
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#multi-headed-attention">
         Multi-Headed Attention
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#the-residuals-and-layer-normalization">
         The Residuals and Layer normalization
        </a>
       </li>
      </ul>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#decoder">
     Decoder层
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#outputembedding">
       Output的embedding
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#masked-multi-head-attention">
       Masked Multi-Head Attention
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#multi-head-attention">
       第二个Multi-head Attention
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Transformer</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   引言
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   李宏毅机器学习笔记
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sequence-to-sequence-seq2seq">
     Sequence-to-sequence (Seq2seq)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id3">
       例子
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id4">
     Transformer架构
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#encoder">
       Encoder
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#decoder-autoregressive">
       Decoder - AutoRegressive
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#encoder-decoder">
       Encoder-Decoder
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id5">
     训练
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id6">
     评估结果
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id7">
     拓展
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#attention-is-all-you-need">
   Attention Is All You Need读论文笔记
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#introduction">
     1. Introduction
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#backgound">
     2. Backgound
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model-architecture">
     3. Model Architecture
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id8">
       总体结构
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id9">
       Encoder层
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#embedding">
         Embedding层
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#self-attention">
         Self-attention层
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#multi-headed-attention">
         Multi-Headed Attention
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#the-residuals-and-layer-normalization">
         The Residuals and Layer normalization
        </a>
       </li>
      </ul>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#decoder">
     Decoder层
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#outputembedding">
       Output的embedding
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#masked-multi-head-attention">
       Masked Multi-Head Attention
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#multi-head-attention">
       第二个Multi-head Attention
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="transformer">
<h1>Transformer<a class="headerlink" href="#transformer" title="Permalink to this headline">¶</a></h1>
<div class="section" id="id1">
<h2>引言<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p>RNN: feed forward neural networks rolled out over time, 对input有顺序的很有用！比如Time Series</p>
<center><img src="https://cdn.mathpix.com/snip/images/WL5kkhCnNXSpG3439U-a0ip1cbjfUeRaQo4rJ8qspNM.original.fullsize.png" width="65%"/></center>
<p>由于其顺序结构训练速度常常受到限制（precludes parallelization within training examples），而且由于梯度下降/explode，它难以处理长Sequence中较远处的信息。</p>
<p>因此，LSTM中让memory可以retain，但是LSTM比RNN更慢！</p>
<p>既然Attention模型本身可以看到全局的信息， 那么一个自然的疑问是我们能不能去掉RNN结构，仅仅依赖于Attention模型，这样我们可以使训练并行化，同时拥有全局信息？</p>
<p>你可能听说过不同的著名Transformer模型，如 BERT、GPT。在这篇文章中，我们将精读谷歌的这篇 <a class="reference external" href="https://arxiv.org/abs/1706.03762">Attention is All you need</a> 论文来回顾一下仅依赖于Self-Attention机制的Transformer架构。</p>
<p>Transformer的优势在于，它可以不受梯度消失的影响，能够保留<strong>任意长</strong>的长期记忆。而RNN的记忆窗口很短；LSTM和GRU虽然解决了一部分梯度消失的问题，但是它们的记忆窗口也是有限的。</p>
<blockquote>
<div><p>Recurrent neural networks (RNN) are also capable of looking at previous inputs too. But the power of the attention mechanism is that it doesn’t suffer from short term memory. RNNs have a shorter window to reference from, so when the story gets longer, RNNs can’t access words generated earlier in the sequence. This is still true for Gated Recurrent Units (GRU) and Long-short Term Memory (LSTM) networks, although they do a bigger capacity to achieve longer-term memory, therefore, having a longer window to reference from. The attention mechanism, in theory, and given enough compute resources, have an infinite window to reference from, therefore being capable of using the entire context of the story while generating the text.</p>
</div></blockquote>
</div>
<div class="section" id="id2">
<h2>李宏毅机器学习笔记<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<div class="section" id="sequence-to-sequence-seq2seq">
<h3>Sequence-to-sequence (Seq2seq)<a class="headerlink" href="#sequence-to-sequence-seq2seq" title="Permalink to this headline">¶</a></h3>
<p>Transformer是一个<code class="docutils literal notranslate"><span class="pre">Sequence-to-sequence</span></code>会写做<code class="docutils literal notranslate"><span class="pre">Seq2seq</span></code>——input是一个sequence不知道应该要output多长,由机器自己决定output的长度,即Seq2seq</p>
<div class="section" id="id3">
<h4>例子<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p><strong>语音辨识</strong>:</p></li>
<li><p><strong>机器翻译</strong>:</p></li>
<li><p><strong>语音翻译</strong>:</p>
<ul>
<li><p>因為<strong>世界上有很多语言,他根本连文字都没有</strong>,所以不能直接先做一个语音辨识,再做一个机器翻译,把语音辨识系统跟机器翻译系统,接起来</p></li>
</ul>
</li>
<li><p><strong>语音合成</strong></p>
<ul>
<li><p>语音辨识反过来就是<strong>语音合成</strong>Text-to-Speech (TTS) Synthesis</p></li>
</ul>
</li>
<li><p><strong>聊天机器人</strong></p></li>
</ul>
<p>那事实上Seq2Seq model,在NLP的领域,在natural language processing的领域的使用,是比你想像的更為广泛,其实很多<strong>natural language processing的任务,都可以想成是<code class="docutils literal notranslate"><span class="pre">question</span> <span class="pre">answering,QA</span></code>的任务</strong>,而<strong>QA的问题,就可以用Seq2Seq model来解</strong></p>
<ul class="simple">
<li><p><strong>文法剖析</strong>：Seq2seq for Syntactic Parsing</p>
<ul>
<li><p>给机器一段文字，机器生成的<strong>文法的剖析树</strong>也可以是看做是一个一个Sequence</p></li>
</ul>
</li>
<li><p>multi-label classification</p>
<ul>
<li><p><strong>同一个东西,它可以属於多个class</strong>,比如文章分类：</p></li>
<li><p>每一篇文章对应的class的数目根本不一样，所以不能直接输出前三名之类的</p></li>
<li><p>可以用seq2seq硬做,<strong>输入一篇文章</strong> <strong>输出就是class</strong> 就结束了,机器自己决定 它要输出几个class</p></li>
</ul>
</li>
<li><p>Object Detection</p>
<ul>
<li><p>这个看起来跟seq2seq model,应该八竿子打不著的问题,它也可以用seq2seq’s model硬解</p></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="section" id="id4">
<h3>Transformer架构<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<p>一般的seq2seqmodel会分成两块Encoder和Decoder：input一个sequence有Encoder,负责处理这个sequence,再把处理好的结果丢给Decoder,由Decoder决定,它要输出什麼样的sequence</p>
<div class="section" id="encoder">
<h4>Encoder<a class="headerlink" href="#encoder" title="Permalink to this headline">¶</a></h4>
<p>seq2seq model <code class="docutils literal notranslate"><span class="pre">Encoder</span></code>要做的事情,就是<strong>给一排向量，输出另外一排向量</strong></p>
<center><img src="../../images/DL_Transformer_1.png" width="65%"/></center>
<ul>
<li><p>Transformer中的Block：</p>
<p>核心还是self-attention</p>
  <center><img src="../../images/DL_Transformer_2.png" width="70%"/></center>
<ol>
<li><p>注意: 这里做的是Layer Normalization（For a sample, across each feature！）</p>
 <center><img src="../../images/DL_Transformer_15.png" width="50%"/></center>
</li>
<li><p>注意：Transformer的Multi-head内部是把原先的一句<span class="math notranslate nohighlight">\((n个词, 每个词d_{\text {model }}=512维embedding)\)</span>的这么个array拆成8个head（而不是同样大小的head做8次：</p></li>
</ol>
<div class="math notranslate nohighlight">
\[ \operatorname{MultiHead}(Q, K, V)= \text{Concat} (\operatorname{head}_{1}, \ldots \operatorname{head}_{\mathrm{h}}) W^{O} \]</div>
<p>where $<span class="math notranslate nohighlight">\(head_{\mathrm{i}}=\operatorname{Attention}\left(Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V}\right)\)</span>$</p>
<div class="math notranslate nohighlight">
\[\operatorname{Attention}(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V\]</div>
<ul>
<li><p>the projections are parameter matrices <span class="math notranslate nohighlight">\(W_{i}^{Q} \in \mathbb{R}^{d_{\text {model }} \times d_{k}}, W_{i}^{K} \in \mathbb{R}^{d_{\text {model }} \times d_{k}}, W_{i}^{V} \in \mathbb{R}^{d_{\text {model }} \times d_{v}}\)</span> and <span class="math notranslate nohighlight">\(W^{O} \in \mathbb{R}^{h d_{v} \times d_{\text {model }}}\)</span>.</p>
<p>In this work we employ <span class="math notranslate nohighlight">\(h=8\)</span> parallel attention layers, or heads. For each of these we use <span class="math notranslate nohighlight">\(d_{k}=d_{v}=d_{\text {model }} / h=64\)</span>. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.</p>
<p>也就是说每次headi的<span class="math notranslate nohighlight">\(W_{i}^{Q}、W_{i}^{K}、W_{i}^{V}\)</span>是分别把一个<span class="math notranslate nohighlight">\((n个词, 每个词d_{\text {model }}=512维embedding)\)</span>的句子 给映射到<span class="math notranslate nohighlight">\((n个词, d_{k}=\frac{d_{\text {model }}}{h}=\frac{512}{8}=32维)的array\)</span>，最后再concat出来的<span class="math notranslate nohighlight">\(\text{Concat} (\operatorname{head}_{1}, \ldots \operatorname{head}_{\mathrm{h}})\)</span>才是<span class="math notranslate nohighlight">\((n个词, d_{k} \times h=d_{\text {model}})\)</span>维度的array，所以最后<span class="math notranslate nohighlight">\(W_O\)</span>才会是跟前面不一样的<span class="math notranslate nohighlight">\(d_{\text {model }} \times d_{\text {model }}\)</span>维度，<span class="math notranslate nohighlight">\(W_O\)</span>出来的东西就跟原先embeddings的一样了！</p>
</li>
</ul>
</li>
</ul>
<p>整体：
<center><img src="../../images/DL_Transformer_3.png" width="70%"/></center></p>
<p>注意：<strong>原始论文的设计 不代表它是最好的,最optimal的设计</strong></p>
<ul class="simple">
<li><p>有一篇文章叫,<a class="reference external" href="https://arxiv.org/abs/2002.04745">on layer normalization in the transformer architecture</a>，它问的问题就是 為什麼,layer normalization是放在那个地方呢,為什麼我们是先做,residual再做layer normalization,能不能够把layer normalization,放到每一个block的input,也就是说 你做residual以后,再做layer normalization,再加进去 你可以看到说左边这个图,是原始的transformer,右边这个图是稍微把block,更换一下顺序以后的transformer,更换一下顺序以后 结果是会比较好的,这就代表说,原始的transformer 的架构,并不是一个最optimal的设计,你永远可以思考看看,有没有更好的设计方式</p></li>
<li><p>再来还有一个问题就是,為什麼是layer norm 為什麼是别的,不是别的,為什麼不做batch normalization,也许这篇paper可以回答你的问题,这篇paper是<a class="reference external" href="https://arxiv.org/abs/2003.07845">Power Norm：,Rethinking Batch Normalization In Transformers</a>,它首先告诉你说 為什麼,batch normalization不如,layer normalization,在Transformers裡面為什麼,batch normalization不如,layer normalization,接下来在说,它提出来一个power normalization,一听就是很power的意思,都可以比layer normalization,还要performance差不多或甚至好一点</p></li>
</ul>
</div>
<div class="section" id="decoder-autoregressive">
<h4>Decoder - AutoRegressive<a class="headerlink" href="#decoder-autoregressive" title="Permalink to this headline">¶</a></h4>
<p>Decoder整体做的事情是一个一个地输出，也就是auto- regressive（consuming the previously generated symbols as <strong>additional input</strong> when generating the next）</p>
<center><img src="../../images/DL_Transformer_4.png" width="70%"/></center>
<ul class="simple">
<li><p>里面会有特殊的开始的符号 和 断开的符号，让decoder可以输出“END”的概率，如果最高的话就会停下来！比如OUTPUT：<code class="docutils literal notranslate"><span class="pre">机</span> <span class="pre">器</span> <span class="pre">学</span> <span class="pre">习</span> <span class="pre">END</span></code></p></li>
<li><p>解决一步错，步步错的话：可以看scheduled sampling，给模型一些错误的输入</p></li>
</ul>
<p>和Encoder的区别在于4个：</p>
<center><img src="../../images/DL_Transformer_5.png" width="70%"/></center>
<ol>
<li><p><code class="docutils literal notranslate"><span class="pre">Masked</span> <span class="pre">Self-attention</span></code>: 不能看前面</p>
 <center><img src="../../images/DL_Transformer_6.png" width="60%"/></center>
 <center><img src="../../images/DL_Transformer_7.png" width="60%"/></center>
<ul class="simple">
<li><p>原因：现在decoder是一个一个输出，才看得到下一个的</p></li>
</ul>
</li>
<li><p>中间加入了一个Cross attention：接下来仔细讲</p></li>
<li><p>在结果的地方接了一个Softmax来转换矩阵！</p></li>
</ol>
</div>
<div class="section" id="encoder-decoder">
<h4>Encoder-Decoder<a class="headerlink" href="#encoder-decoder" title="Permalink to this headline">¶</a></h4>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">Cross</span> <span class="pre">attention</span></code>:</p>
  <center><img src="../../images/DL_Transformer_8.png" width="60%"/></center>
<p>这个地方比如说 原先input时English的embedding，Decoder来的是French的embedding，这边学习的就是实现English和French的word mapping</p>
<ul>
<li><p>输出两个字的过程细节：</p>
  <center><img src="../../images/DL_Transformer_9.png" width="50%"/><img src="../../images/DL_Transformer_10.png" width="45.3%"/></center>
</li>
</ul>
</li>
<li><p>最后接一个<code class="docutils literal notranslate"><span class="pre">Linear</span></code>：这一层的# Neurons，比如在英语翻译法语的时候数量就会是法语words的个数</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Softmax</span></code>：得到probability</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Output</span></code>：最高的！然后再把这个output shift right传给的coder继续output下一个单词，直到这个步骤输出<code class="docutils literal notranslate"><span class="pre">End</span></code></p></li>
</ul>
</div>
</div>
<div class="section" id="id5">
<h3>训练<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h3>
<p>每次训练其实就类似进行了一个分类问题：目标是minimize总的cross entropy <span class="math notranslate nohighlight">\(\mathbb{H}(p)=-\sum_{k} p_{k} \log p_{k}\)</span></p>
<center><img src="../../images/DL_Transformer_11.png" width="60%"/></center>
<ul>
<li><p>这里可以做一个<code class="docutils literal notranslate"><span class="pre">Teacher</span> <span class="pre">Forcing</span></code>: Ssing the ground truth as input</p>
  <center><img src="../../images/DL_Transformer_13.png" width="60%"/></center>
</li>
</ul>
</div>
<div class="section" id="id6">
<h3>评估结果<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>BLEU score：decoder出来的句子和ground true对比</p></li>
</ul>
</div>
<div class="section" id="id7">
<h3>拓展<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Copy Mechanism: 从输入中复制东西到输出里，如聊天机器人、文章摘要生成</p></li>
<li><p>Guided Attention：要求机器做attention的时候有固定的方式</p></li>
</ul>
</div>
</div>
<div class="section" id="attention-is-all-you-need">
<h2>Attention Is All You Need读论文笔记<a class="headerlink" href="#attention-is-all-you-need" title="Permalink to this headline">¶</a></h2>
<div class="section" id="introduction">
<h3>1. Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h3>
<p>RNN (LSTM)的问题很多，并行计算能力很差，因为 T 时刻的计算依赖 T-1 时刻的隐层计算结果。</p>
<ul class="simple">
<li><p>precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples</p></li>
</ul>
<p>怎么改进也没用，所以我们另起炉灶放弃recurrence，而是relying <strong>entirely on an attention mechanism</strong> to draw global dependencies between input and output</p>
<p>对比RNN系列，Transformer的特征抽取能更好，使用了self-attention和多头机制来让源序列和目标序列自身的embedding表示所蕴含的信息更加丰富。</p>
</div>
<div class="section" id="backgound">
<h3>2. Backgound<a class="headerlink" href="#backgound" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Self-attention</span></code> an attention mechanism relating different <strong>positions</strong> of a single sequence in order to compute <strong>a representation of the sequence</strong></p></li>
<li><p>Transformer特性：rely entirely on self-attention to compute representations of its input and output without using sequence aligned RNNs or convolution</p></li>
</ul>
</div>
<div class="section" id="model-architecture">
<h3>3. Model Architecture<a class="headerlink" href="#model-architecture" title="Permalink to this headline">¶</a></h3>
<div class="section" id="id8">
<h4>总体结构<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h4>
<p>Transformer的结构采用 Encoder-Decoder 架构。论文中Encoder层由6个Encoder堆叠在一起，Decoder层也是6层。</p>
<p><img alt="img" src="https://pic3.zhimg.com/v2-93645bf35687d3065e766127d70c73a6_b.png" /></p>
<p>先从整体来看：</p>
<ul class="simple">
<li><p>Encoder将输入序列做了一个复杂的、高阶的embedding；</p></li>
<li><p>Decoder使用了Encoder的这个高阶embedding，同时还根据之前Decoder的输出，一步一步地产生输出。</p></li>
</ul>
<p>每一个Encoder和Decoder的内部结构如下图：</p>
<center><img src="../../images/DL_Transformer_14.png" width="90%"/></center>
<ul class="simple">
<li><p>Encoder的每个identical layer里包含两个Sublayers：</p>
<ul>
<li><p>多头Self-attention层，帮助当前节点不仅仅只关注当前的词，从而能获取到上下文的语义。</p></li>
<li><p>前馈神经网络层(feed forward)，提供非线性。</p></li>
</ul>
</li>
<li><p>Decoder里面有三个sublayer</p>
<ul>
<li><p>masked Self-attention 保证不看到后面的序列</p></li>
<li><p>跟encoder交叉的multi-head attention</p></li>
<li><p>FC</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="id9">
<h4>Encoder层<a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h4>
<p>首先，模型需要对输入的数据进行一个embedding操作，并输入到Self-attention层，处理完数据后把数据送给前馈神经网络，前馈神经网络的计算可以并行，得到的输出会输入到下一个Encoder。大致结构如下：</p>
<p><img alt="img" src="https://pic1.zhimg.com/v2-4cefc7dcda98b0d26dc04c618cc49dcc_b.png" /></p>
<p><img alt="x_1,x_2" src="https://www.zhihu.com/equation?tex=x_1%2Cx_2" />就是embedding， <img alt="z_1, z_2" src="https://www.zhihu.com/equation?tex=z_1%2C%20z_2" />是经过self-attention之后的输出，<img alt="r_1, r_2" src="https://www.zhihu.com/equation?tex=r_1%2C%20r_2" /> 是经过feed forward网络之后的输出，它们会被输入到下一层encoder中去。</p>
<div class="section" id="embedding">
<h5>Embedding层<a class="headerlink" href="#embedding" title="Permalink to this headline">¶</a></h5>
<p>Transformer模型中缺少一种解释输入序列中单词<strong>顺序</strong>的方法。为了处理这个问题，Transformer给Encoder层和Decoder层的输入添加了一个额外的向量Positional Encoding，维度和embedding的维度一样。</p>
<p>这个位置向量的具体计算方法有很多种，论文中的计算方法是在偶数位置，使用正弦编码，在奇数位置，使用余弦编码：</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned} P E_{(p o s, 2 i)} &amp;=\sin \left(p o s / 10000^{2 i / d_{\mathrm{model}}}\right) \\ P E_{(p o s, 2 i+1)} &amp;=\cos \left(p o s / 10000^{2 i / d_{\mathrm{model}}}\right) \end{aligned}\end{split}\]</div>
<ul>
<li><p>pos: 当前词在句子中的位置</p></li>
<li><p>i：是指向量中每个值的 index</p>
<p>(实际上 用any reasonable function应该都可以！)</p>
</li>
</ul>
<p>所以，最终一个词的embedding，就是它的语义信息embedding+序列信息embedding (positional encoding):</p>
<img src="https://cdn.mathpix.com/snip/images/0FJrJ5ltV6QjNhlsvmEHphlq4fHDIbbM-obl3nddnwI.original.fullsize.png" />
</div>
<div class="section" id="self-attention">
<h5>Self-attention层<a class="headerlink" href="#self-attention" title="Permalink to this headline">¶</a></h5>
<p>让我们从宏观视角看自注意力机制，精炼一下它的工作原理。</p>
<p>例如，下列句子是我们想要翻译的输入句子：</p>
<blockquote>
<div><p>The animal didn’t cross the street because it was too tired.</p>
</div></blockquote>
<p>​    这个“it”在这个句子是指什么呢？它指的是street还是这个animal呢？这对于人类来说是一个简单的问题，但是对于算法则不是。</p>
<p>​    当模型处理这个单词“it”的时候，自注意力机制会允许“it”与“animal”建立联系。</p>
<p>​    随着模型处理输入序列的每个单词，自注意力会关注整个输入序列的<strong>所有单词</strong>，帮助模型对本单词更好地进行编码(embedding)。</p>
<p><img alt="img" src="https://pic3.zhimg.com/v2-db1893bb9039b565ed6c7e9415069a12_b.png" /></p>
<p>​    如上图所示，当我们在编码器#5（栈中最上层编码器）中编码“it”这个单词的时，注意力机制的部分会去关注“The Animal”，将它的表示的一部分编入“it”的编码中。接下来我们看一下Self-Attention详细的处理过程。</p>
<ul>
<li><p><strong>step 1</strong>：对于输入序列的每个单词，它都有三个向量编码，分别为：<span class="math notranslate nohighlight">\(Query、Key、Value\)</span>。这三个向量是用embedding向量与三个矩阵（ <span class="math notranslate nohighlight">\(W^Q, W^K, W^V\)</span> )相乘得到的结果。这三个矩阵的值在BP的过程中会一直进行更新。</p></li>
<li><p><strong>step 2</strong>：计算Self-Attention的分数值，该分数值决定了当我们在某个位置encode一个词时，对输入句子的其他部分的关注程度。这个分数值的计算方法是用该词语的<span class="math notranslate nohighlight">\(Query\)</span>与句子中其他词语的<span class="math notranslate nohighlight">\(Key\)</span>做点乘。以下图为例，假设我们在为这个例子中的第一个词“Thinking”计算自注意力向量，我们需要拿输入句子中的每个单词对“Thinking”打分。这些分数决定了在编码单词“Thinking”的过程中重视句子其它token的程度。</p>
<p><img alt="img" src="https://pic2.zhimg.com/v2-0fd261302748ff1e312ccb84f1716191_b.png" /></p>
</li>
<li><p><strong>step 3</strong>：对每个分数除以 <span class="math notranslate nohighlight">\(\sqrt{d}\)</span> （d是embedding维度），之后做softmax进行归一化。</p>
<p><img alt="img" src="https://pic3.zhimg.com/v2-9146baeb7e334246f4fd880bd270158e_b.png" /></p>
<p>注意！这里是跟Self- attention不同的地方，除这个d的原因是：</p>
<blockquote>
<div><p>Transformer在对比dot product和additive（element wise相加）的时候，发现后者的效果更好！We suspect that for large values of <span class="math notranslate nohighlight">\(d_{k}\)</span>, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients. To counteract this effect, we scale the dot products by <span class="math notranslate nohighlight">\(\frac{1}{\sqrt{d_{k}}}\)</span>. （<span class="math notranslate nohighlight">\(d_{k}\)</span>是embedding的维度）。</p>
</div></blockquote>
</li>
<li><p><strong>step 4</strong>：把每个<span class="math notranslate nohighlight">\(Value\)</span>向量和softmax得到的权重值进行相乘，进行加权求和，得到的结果即是一个词语的self-attention embedding值。这个embedding值已经是融合了句子中其他token的了。</p>
<p><img alt="img" src="https://pic1.zhimg.com/v2-958a90545951330591468e064fb0a0b0_b.png" /></p>
</li>
</ul>
<p>这样，自注意力的计算就完成了。得到的向量就可以传递给前馈神经网络。</p>
<p>值得注意的是，这个地方的self-attention是可以并行化进行的！</p>
</div>
<div class="section" id="multi-headed-attention">
<h5>Multi-Headed Attention<a class="headerlink" href="#multi-headed-attention" title="Permalink to this headline">¶</a></h5>
<p>​    通过增加一种叫做”多头”注意力（”multi-headed”attention）的机制，论文进一步完善了自注意力层。</p>
<p>​    接下来我们将看到，对于“多头”注意力机制，我们有多个Query/Key/Value权重矩阵集 <img alt="W^Q,W^K,W^V" src="https://www.zhihu.com/equation?tex=W%5EQ%2CW%5EK%2CW%5EV" /> (Transformer使用八个注意力头)。</p>
<p><img alt="img" src="https://pic4.zhimg.com/v2-1e328407bef98b2168803ab80a0e133f_b.png" /></p>
<p>​    现在对于每一个词语，我们有了八个向量 <img alt=" z_0,....,z_7" src="https://www.zhihu.com/equation?tex=%20z_0%2C....%2Cz_7" />，它们分别由八个head产生。但是对于下一个feed-forward层，我们应该把每个词语都用一个向量来表示。所以下一步，我们需要把这八个向量压缩成一个向量。可以直接把这些矩阵拼接在一起，然后用一个附加的权重矩阵<span class="math notranslate nohighlight">\(W^O\)</span>与它们相乘:</p>
<p><img alt="img" src="https://pic1.zhimg.com/v2-cda2410ace7ffb238770c6a9165de914_b.png" /></p>
<p>那么，<strong>Transformer为什么需要进行Multi-head Attention</strong>：原因是将模型分为多个头，形成多个子空间，可以让模型去关注不同方面的信息，最后再将各个方面的信息综合起来。多次attention综合的结果至少能够起到增强模型的作用，可以类比CNN中同时使用多个卷积核的作用，直观上讲，多头的注意力有助于网络捕捉到更丰富的特征/信息。</p>
</div>
<div class="section" id="the-residuals-and-layer-normalization">
<h5>The Residuals and Layer normalization<a class="headerlink" href="#the-residuals-and-layer-normalization" title="Permalink to this headline">¶</a></h5>
<p>​    在继续进行下去之前，我们需要提到一个encoder中的细节：在每个encoder中都有一个残差连接，并且都跟随着一个Layer Normalization（层-归一化）步骤。</p>
<p><img alt="img" src="https://pic1.zhimg.com/v2-8fe26c64e4dc0d15631e699ca3453aa0_b.png" /></p>
<p>Layer-Norm也是归一化数据的一种方式，不过 Layer-Norm 是在<strong>每一个样本上</strong>计算均值和方差，而不是 Batch-Norm 那种在<strong>批</strong>方向计算均值和方差！</p>
<p><img alt="img" src="https://pic2.zhimg.com/v2-003de84ad4e5816ab9a40f62f71b1005_b.png" /></p>
<p>总而言之，Encoder就是用来给input一个比较好的embedding，使用self-attention来使一个词的embedding包含了<strong>上下文</strong>的信息，而不是简单的查look-up table。Transformer使用了多层(6层)的Encoder是为了把握一些<strong>高阶</strong>的信息。</p>
</div>
</div>
</div>
<div class="section" id="decoder">
<h3>Decoder层<a class="headerlink" href="#decoder" title="Permalink to this headline">¶</a></h3>
<p>Transformer的Decoder作用和普通seq2seq一样：从&lt;Start&gt;开始，基于<strong>之前的Decoder输出</strong>，以及<strong>Encoder的embedding</strong>，来预测<strong>下一个词的概率分布</strong>。</p>
<p>下面我们来详细介绍Decoder的内部结构。</p>
<p><img alt="img" src="https://pic2.zhimg.com/v2-2264bc124efa6de7d498f499a420c291_b.jpeg" /></p>
<div class="section" id="outputembedding">
<h4>Output的embedding<a class="headerlink" href="#outputembedding" title="Permalink to this headline">¶</a></h4>
<p>和Encoder一样，Decoder中的Output先经过embedding变成数字向量然后+positional encoding之后得到了一个embedding</p>
</div>
<div class="section" id="masked-multi-head-attention">
<h4>Masked Multi-Head Attention<a class="headerlink" href="#masked-multi-head-attention" title="Permalink to this headline">¶</a></h4>
<p>和前面不同的是，Decoder的self-attention层其实是<strong>masked</strong> multi-head attention。mask表示掩码，它对某些值进行掩盖。这是为了防止Decoder在计算某个词的attention权重时“看到”这个词后面的词语。</p>
<blockquote>
<div><p>Since the decoder is <strong>auto-regressive</strong> and generates the sequence word by word, you need to prevent it from conditioning to future tokens. For example, when computing attention scores on the word “am”, you should not have access to the word “fine”, because that word is a future word that was generated after. <strong>The word “am” should only have access to itself and the words before it</strong>. This is true for all other words, where they can only attend to previous words.</p>
</div></blockquote>
<p><img alt="img" src="https://pic3.zhimg.com/v2-b32f466acd31b74cf07d892ac85f3362_b.png" /></p>
<p>Mask 是为了使得 decoder 不能看见未来的信息。也就是对于一个序列，在 time_step 为 t 的时刻，我们的Decoder只能依赖于 t 时刻之前的输出，而不能依赖 t 之后的输出。因此我们需要想一个办法，把 t 之后的信息给隐藏起来。</p>
<p>那么具体怎么做呢？也很简单：产生一个上三角矩阵，上三角的值全为-inf。</p>
<p><img alt="img" src="https://pic4.zhimg.com/v2-2e56261653655af539869dd73cd3c92f_b.png" /></p>
<p>加上-inf的目的是，做softmax之后-inf会变成0：</p>
<p><img alt="img" src="https://pic4.zhimg.com/v2-bce63dfed73f5c11cb46029b81568e33_b.png" /></p>
<p>这个mask是Decoder中self-attention和Encoder中的self-attention唯一有区别的地方。  经过了masked self-attention之后，decoder的每一个词都得到了<strong>包含其以前信息的高阶融合embedding</strong>。</p>
</div>
<div class="section" id="multi-head-attention">
<h4>第二个Multi-head Attention<a class="headerlink" href="#multi-head-attention" title="Permalink to this headline">¶</a></h4>
<blockquote>
<div><p>For this layer, the encoder’s outputs are keys and values, and the first multi-headed attention layer outputs are the queries. This process matches the encoder’s input to the decoder’s input, allowing the decoder to decide which encoder input is relevant to put a focus on.</p>
</div></blockquote>
<p>每个decoder的masked self-attention输出的token embedding都去和encoder输出的高阶融合embedding做self-attention：</p>
<p><img alt="img" src="https://pic2.zhimg.com/v2-8f9e956a0244a30163c5a4279fee6929_b.gif" /></p>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./Deep_Learning/NLP"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="Self-attention.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Self-Attention</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="BERT.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">BERT</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Jace Yang<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>