# Q-BERT

> [Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT](https://arxiv.org/pdf/1909.05840.pdf)

- æ•ˆæœ

    Q-BERT achieves 13Ã— compression ratio in weights, 4Ã— smaller activation size, and 4Ã— smaller embedding size, within at most 2.3% accuracy loss.


- ç ”ç©¶è´¡çŒ®ï¼š

    - mixed precision quantization based on Hessian informationâ€”â€”å¯¹äºŒé˜¶ä¿¡æ¯ï¼ˆå³ Hessian ä¿¡æ¯ï¼‰è¿›è¡Œå¤§é‡é€å±‚åˆ†æï¼Œè¿›è€Œå¯¹BERTæ‰§è¡Œæ··åˆç²¾åº¦é‡åŒ–ã€‚ç ”ç©¶å‘ç°ï¼Œä¸è®¡ç®—æœºè§†è§‰é¢†åŸŸä¸­çš„ç¥ç»ç½‘ç»œç›¸æ¯”ï¼ŒBERTçš„Hessian è¡Œä¸ºå­˜åœ¨æå¤§çš„ä¸åŒã€‚å› æ­¤ï¼Œè¯¥ç ”ç©¶æå‡ºä¸€ç§åŸºäºtopç‰¹å¾å€¼å‡å€¼å’Œæ–¹å·®çš„æ•æ„Ÿåº¦åº¦é‡æŒ‡æ ‡ï¼Œä»¥å®ç°æ›´å¥½çš„æ··åˆç²¾åº¦é‡åŒ–ã€‚

    - the group-wise quantizing schemeâ€”â€”ç ”ç©¶è€…æå‡ºæ–°çš„é‡åŒ–æœºåˆ¶â€”â€”ç»„é‡åŒ–ï¼ˆgroup-wise quantizationï¼‰ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿç¼“è§£å‡†ç¡®ç‡ä¸‹é™é—®é¢˜ï¼ŒåŒæ—¶ä¸ä¼šå¯¼è‡´ç¡¬ä»¶å¤æ‚åº¦æ˜¾è‘—ä¸Šå‡ã€‚å…·ä½“è€Œè¨€ï¼Œç»„é‡åŒ–æœºåˆ¶å°†æ¯ä¸ªçŸ©é˜µåˆ†å‰²ä¸ºä¸åŒçš„ç»„ï¼Œæ¯ä¸ªç»„æ‹¥æœ‰ç‹¬ç«‹çš„é‡åŒ–èŒƒå›´å’ŒæŸ¥æ‰¾è¡¨ã€‚

    - ç ”ç©¶è€…è°ƒæŸ¥äº†BERTé‡åŒ–ä¸­çš„ç“¶é¢ˆï¼Œå³ä¸åŒå› ç´ å¦‚ä½•å½±å“NLPæ€§èƒ½å’Œæ¨¡å‹å‹ç¼©ç‡ä¹‹é—´çš„æƒè¡¡ï¼Œè¿™äº›å› ç´ åŒ…æ‹¬é‡åŒ–æœºåˆ¶ï¼Œä»¥åŠåµŒå…¥ã€è‡ªæ³¨æ„åŠ›å’Œå…¨è¿æ¥å±‚ç­‰æ¨¡å—ã€‚

## èƒŒæ™¯

Ultra low precision quantization can lead to significant accuracy degradation. Mixed precision quantization and multi-stage quantization have been proposed to solve/alleviate this problem.

- Mixed precision quantizationçš„é—®é¢˜ï¼šæŒ‡æ•°åŒ–çš„æœç´¢ç©ºé—´ï¼Œæ¯”å¦‚æˆ‘ä»¬å¯ä»¥å¯¹ä¸€ä¸ª12å±‚çš„BERTåš2ã€4ã€æˆ–è€…8bitçš„ä¸‰ç§é‡åŒ–ï¼Œé‚£ä¹ˆå°±ä¸€å…±æœ‰$3^{12} \approx 5.3 \times 10^{5}$ç§ä¸åŒçš„é‡åŒ–æ–¹æ³•ï¼

## æ ¸å¿ƒæ–¹æ³•

å¯¹äºä¸€ä¸ªfine-tunedçš„BERTï¼šæŸå¤±å‡½æ•°æ˜¯

$$
L(\theta)=\sum_{(x_{i}, y_{i})} \operatorname{CE}(\operatorname{softmax}(W_{c}(W_{n}(\ldots W_{1}(W_{e}(x_{i}))))), y_{i})
$$

- CEï¼šCross Entropyæˆ–è€…å…¶ä»–åˆé€‚çš„loss function
- BERTä¸‰ç±»ä¸»è¦çš„layersï¼š
    - æœ€å†…å±‚çš„$W_e$æ˜¯embedding tableï¼Œ$BERT_{BASE}$ä¸­éœ€è¦91MB.
    - å¤–é¢çš„$W_{1}, W_{2}, \ldots, W_{n}$æ˜¯Encoderå±‚ï¼Œ$BERT_{BASE}$ä¸­éœ€è¦325MB
    - $W_{c}$æ˜¯output layerï¼Œ$BERT_{BASE}$ä¸­éœ€è¦0.01MB.

        QBERTæ²¡æœ‰é‡åŒ–output layerï¼Œè€Œæ˜¯ç”¨ä¸¤ç§ä¸åŒçš„æ–¹æ³•é‡åŒ–embeddingå’ŒEncoderçš„parameters

### é‡åŒ–

å¦‚æœæŠŠä¸€ä¸ªNNä¸­çš„activationæˆ–è€…æƒé‡real value $z$ç”¨quantization operator $Q$æ¥é‡åŒ–çš„è¯ï¼Œé‚£ä¹ˆå¯¹æ¯ä¸€ä¸ªåœ¨ $(t_{j}, t_{j+1}]$ ($j=0, \ldots, 2^{k}-1$ï¼Œkæ˜¯è¿™ä¸€å±‚çš„quantization precision)è¿™ä¸ªåŒºé—´çš„$z$æ¥è¯´ï¼Œéƒ½ä¼šæ˜ å°„åˆ°ä¸€ä¸ªå›ºå®šçš„æ•°$q_j$ï¼Œä¹Ÿå°±æ˜¯ï¼š
$$
Q(z)=q_{j}, \quad \text { for } z \in(t_{j}, t_{j+1}]
$$

- Uniformly quantization function: $(t_{j}, t_{j+1}]$è¿™ä¸ªåŒºé—´çš„å¤§å°éƒ½æ˜¯ä¸€æ ·çš„ï¼ŒQBERTç”¨çš„ä¹Ÿæ˜¯è¿™ç§æ–¹æ³•

æ•´ä½“æ¥è¯´ï¼ŒQBERTå®ç°çš„æ˜¯ä¸€ä¸ªéå¯¹ç§°çš„quantization-aware fine-tuningï¼Œå±äºä¹‹å‰æåˆ°çš„QATä½†æ˜¯æ˜¯åœ¨fine-tuneçš„é˜¶æ®µå®æ–½çš„ï¼š

- åœ¨å‰å‘ä¼ æ’­çš„è¿‡ç¨‹é‡Œï¼Œå¯¹æ¯ä¸€ä¸ªweightæˆ–è€…æ¿€æ´»å‡½æ•°tensor $X$ä¸­çš„æ¯ä¸€ä¸ªelementéƒ½é‡‡å–

    1. $$X^{\prime}=\operatorname{Clamp}(X, q_{0}, q_{2^{k}-1}) $$

        - Clamp(X, min, max)è¿™ä¸ªå‡½æ•°çš„æ„æ€æ˜¯æŠŠaé‡Œé¢æ¯”bå°çš„è®¾æˆbï¼Œæ¯”cå¤§çš„è®¾æˆc
        - è¿™ä¸€æ­¥ä¹‹åå¾—åˆ°çš„$X^{\prime}$ç›¸å½“äºæ˜¯è¢«clipåˆ°äº†$[q_{0}, q_{2^{k}-1}]$è¿™ä¸ªèŒƒå›´é‡Œé¢
        - é€šå¸¸è¿™ä¸ªèŒƒå›´æ˜¯[min, max]çš„ä¸€ä¸ªsubintervalï¼Œè¿™æ ·åšçš„ç›®çš„æ˜¯get rid of outliers and better represent the majority of the given tensor
    2. $$X^{I}=\lfloor\frac{X^{\prime}-q_{0}}{\Delta}\rceil, \text { where } \Delta=\frac{q_{2^{k}-1}-q_{0}}{2^{k}-1} $$
        - $\lfloor Â· \rceil$: å–æ•´å‡½æ•°
        - $\Delta$: ä¸¤ä¸ªquantizedçš„ç‚¹çš„è·ç¦»
        
            è¿™ä¸ªfunctionå¾—åˆ°çš„$X^{I}$ç›¸å½“äºå‡æ‰äº†$X^{\prime}$çš„æœ€å°å€¼ä¹‹åç®—ç¦»æœ€å°å€¼å¤šå°‘ä¸ªintervalï¼Œç„¶åå†è¢«å–ä¸ªæ•´ï¼Œå¾—åˆ°çš„matrixå°±æ˜¯ä¸€ä¸ªæ•´æ•°çŸ©é˜µäº†ï¼
    3. $$DQ(X)=\Delta X^{I}+q_{0}$$

        è¿™æ˜¯åŸæ–‡å†™çš„æ˜¯Qï¼Œä½†è¯´äº†æ˜¯ä¸€ä¸ªdequantization oeprationï¼Œä¼šæŠŠä¹‹å‰çš„floatå†æ¬¡æ˜ å°„å›åˆ°pre-trainedå¥½çš„ç½‘ç»œç†Ÿæ‚‰çš„å€¼åŸŸ!

    ç»è¿‡è¿™ä¸‰ä¸ªæ­¥éª¤ä¹‹åï¼Œè¿™ä¸ªQ DQçš„blockçš„è¾“å‡ºå¤§æ¦‚æ˜¯è¿™ä¹ˆä¸€å›äº‹ï¼š

    <center><img src="../../images/DL_QBert_1.png" width="45%"/></center>
    
    ç„¶è€Œé—®é¢˜æ¥äº†ï¼Œè¿™æ ·çš„activationè¾“å‡ºæ ¹æœ¬æ²¡æœ‰æ¢¯åº¦å¯è¨€å‘€ï¼
    

- æ‰€ä»¥ï¼Œåœ¨back-propagationçš„æ—¶å€™æˆ‘ä»¬è¦ç”¨Straight-through Estimatorï¼ˆ[STE](https://arxiv.org/abs/1308.3432)ï¼‰æ–¹æ³•ç»•è¿‡è¿™ä¸€å±‚activationï¼å®ç°ä¸€ä¸ªâ€œFake quantization forward and backward passâ€çš„é”™è§‰è®©å…¶ä»–å±‚æ­£å¸¸å»è®­ç»ƒ

    
    > A straight-through estimator is exactly what it sounds like. It estimates the gradients of a function. Specifically it ignores the derivative of the threshold function and passes on the incoming gradient as if the function was an identity function.<center><img src="../../images/DL_QBert_2.png" width="45%"/></center>


    é‚£ä¹ˆIntuitionå¾ˆç®€å•ï¼Œæˆ‘ä»¬æŠŠåˆšåˆšå¯¹ä¸€ä¸ªå±‚çš„$w$å˜æˆ$[w_{min}, w_{max}]$ä¹‹é—´çš„integerçš„è¿™ä¸ªè¿‡ç¨‹è®°ä¸º$w arrow \hat{w}$ï¼Œé‚£ä¹ˆå¯¹è¿™ä¸ªä¸å¯å¯¼çš„forward layerï¼Œæˆ‘ä»¬åªéœ€è¦æŠŠå®ƒåŸå…ˆåˆ°å¤„ä¸å¯å¯¼çš„å¯¼æ•°$\frac{\partial \hat{w}_{L}}{\partial w_{L}}$ç»™ç›´æ¥è®¾æˆ1ï¼Œå®ƒå°±ä¸å½±å“chain ruleä¸Šçš„å…¶ä»–back probagationäº†ï¼
    
    ä½†å”¯ä¸€è¦æ³¨æ„çš„ç»†èŠ‚æ˜¯ï¼Œåä¼ å›æ¥çš„æ¢¯åº¦æ˜¯æœ‰å¯èƒ½åœ¨æˆ‘ä»¬ä¹‹å‰è®¾å®šçš„$[w_{min}, w_{max}]$èŒƒå›´ä¹‹å¤–çš„ï¼Œå¯¹äºè¿™éƒ¨åˆ†çš„æƒ…å†µæˆ‘ä»¬ä¹‹é—´ä¸æ›´æ–°å‚æ•°ï¼Œæ‰€ä»¥å¯¼æ•°ç›´æ¥è®¾æˆ0ï¼
    
    æ•´ç†æ•ˆæœæ¥çœ‹ï¼Œç”±äº$\frac{\partial \hat{\boldsymbol{w}}_{L}}{\partial \boldsymbol{w}_{L}} \approx 1$ï¼Œè®©$\frac{\partial \mathcal{L}}{\partial \hat{w}} \approx \frac{\partial \mathcal{L}}{\partial w}$ æ˜¯æ²¡ä»€ä¹ˆé—®é¢˜çš„ï¼
    
- Forwardå’ŒBackwardçš„æ•´ä½“æ•ˆæœç”¨[Towards Energy-efficient Quantized Deep Spiking Neural Networks for Hyperspectral Image Classification](https://arxiv.org/abs/2107.11979)è¿™ç¯‡paperé‡Œçš„ä¸€å¼ å›¾è¡¨ç¤ºå°±æ˜¯ï¼š

<center><img src="../../images/DL_QBert_3.png" width="55%"/></center>


### Mixed precision quantization

#### Motivation

Different encoder layers are attending to different structures, and it is expected that they exhibit different sensitivity. Thus, assigning the same number of bits to all the layers is sub-optimal. However, a brute force approach is not feasible for deep networks, as the search space for mixed-precision is exponential in the number of layers!

æ¯”å¦‚ä»ä¸‹é¢è¿™å¼ å›¾å±•ç¤ºäº†4ä¸ªä¸åŒçš„fine-tunedä¹‹åçš„BERTå±‚çš„Loss Landscape

<center><img src="../../images/DL_QBert_4.png" width="75%"/></center>

- è§£é‡Šï¼š
    - x, y åæ ‡æ˜¯äºŒé˜¶å¯¼ç»„æˆçš„HessiançŸ©é˜µä¸­ï¼Œç‰¹å¾å€¼æœ€å¤§çš„ä¸¤ä¸ªç‰¹å¾å‘é‡
    
    - zè½´æ˜¯è¿™ä¸€å±‚çš„loss function
    - ç°è‰²å°çƒæ˜¯convergeçš„åœ°æ–¹ï¼Œè¿™é‡Œæ³¨æ„QBERTå‡è®¾ åœ¨è¿›è¡Œquantizationä¹‹å‰çš„ fine-tunedåBERTå·²ç»è·‘äº†è¶³å¤Ÿçš„iterationsè¾¾åˆ°äº†local minimaï¼Œå› æ­¤æ¢¯åº¦æ¥è¿‘0ï¼Œå¹¶ä¸”æœ‰æ­£çš„curvatureï¼ˆpositive Hessian eigenvalueï¼‰
        - è¿™ä¸ªå‡è®¾
    - x yçš„å˜åŒ–â‡’zçš„å˜åŒ–ï¼Œå¯ä»¥è¿‘ä¼¼weightå˜åŒ–â‡’lossçš„å˜åŒ–ï¼Œä¹Ÿå°±æ˜¯æŸå¤±å‡½æ•°å¯¹weightçš„sensitivity
- ä»è¿™å¼ å›¾å¯ä»¥è§‚å¯Ÿå‡ºæ¥ï¼šæ¯ä¸€å±‚çš„sensitivityå·®åˆ«å¾ˆå¤§ï¼Œå¦‚æœå¯¹(a)çš„å±‚åŠ¨ä¸€ç‚¹weightçš„è¯ï¼Œlossä¼šè¿…é€Ÿincreaseï¼Œè€Œå¯¹æœ€åä¸€å±‚å¾ˆflatçš„æ¥è¯´ï¼Œç”±äºå®ƒéå¸¸ä¸sensitiveæ‰€ä»¥æˆ‘å°±ç®—åŠ¨äº†å¾ˆå¤§çš„weightï¼Œlossä¹Ÿä¸ä¼šåç¦»convergeçš„åœ°æ–¹éå¸¸è¿œ

    è¿™å°±æç¤ºæˆ‘ä»¬åœ¨åé¢çš„å±‚å¯èƒ½å¯ä»¥é‡‡ç”¨æ›´æ¿€è¿›çš„ç­–ç•¥ï¼Œæ¯”å¦‚2 bitï¼Œä¹Ÿä¸ä¼šæŸå¤±å¾ˆå¤§çš„accuracyï¼Œä½†æœ‰äº›æ•åº¦é«˜çš„åœ°æ–¹ï¼Œå°±ä¸èƒ½è¿™ä¹ˆæ“ä½œï¼

å› æ­¤ï¼Œæˆ‘ä»¬ä¸èƒ½é‡‡å–ç»Ÿä¸€çš„ç­–ç•¥ï¼Œç‰¹åˆ«æ˜¯æˆ‘ä»¬æœ€ç»ˆçš„ç›®æ ‡modelå¾ˆå°ã€éœ€è¦æˆ‘ä»¬è¿›è¡Œultra low precision (4-bitsã€2-bits)çš„æ—¶å€™ã€‚æˆ‘ä»¬éœ€è¦çš„ç­–ç•¥æ˜¯ï¼šassign more bits to more sensitive layers in order to retain performance.

#### Hessian AWare Quantization (HAWQ)

[HAWQ](https://arxiv.org/pdf/1905.03696.pdf)æ˜¯QBertä¼¯å…‹åˆ©labçš„å¦å¤–ä¸€ç¯‡ç”¨åœ¨Imageä¸Šçš„é‡åŒ–æ–¹æ³•ï¼ŒQBERTå°±æ˜¯æ”¹è¿›äº†HAWQæ–¹æ³•å¹¶åº”ç”¨åœ¨äº†BERTä¸Šã€‚å› æ­¤æˆ‘ä»¬ç®€å•ä»‹ç»ä¸€ä¸‹HAWQï¼

HAWQå®šä¹‰äº†ä¸€ä¸ªå«Hessian spectrumï¼Œå…¶å®å°±æ˜¯çŸ©é˜µçš„top eigenvaluesï¼æ ¸å¿ƒæ€æƒ³æ˜¯NN layers with higher Hessian spectrum (i.e., larger top eigenvalues) are more sensitive to quantization and require higher precision, as compared to layers with small Hessian spectrum (i.e., smaller top eigenvalues).

è¿™ä¸ªæ–¹æ³•çš„æ€è·¯å¾ˆç®€å•ï¼Œä½†å¯¹äºéå¸¸é«˜ç»´åº¦çš„çŸ©é˜µï¼Œæ˜¾å¼æ±‚ç‰¹å¾å€¼æœ‰æ±‚é€†çš„è¿‡ç¨‹æ˜¯ä¸å¯èƒ½çš„ï¼Œæ¯”å¦‚ä¸€ä¸ª $\text {BERT}_{\text{BASE}}$æœ‰7Mä¸ªå‚æ•°ï¼Œç‰¹å¾å€¼å°±éœ€è¦è§£$7 M \times 7 M$çš„çŸ©é˜µï¼Although it is not possible to explicitly form the Hessian, it is possible to compute the Hessian eigenvalues without explicitly forming it, using a matrix-free power iteration algorithm to calculate `Hessian matvec` â€”â€” the result of multiplication of the Hessian matrix with a given (possibly random) vector v , that is, $H_{i} v$.

-  Denote $g_{i}$ as the gradient of loss $L$ with respect to the $i^{t h}$ block parameters,
    $$
    g_{i}=\frac{\partial L}{\partial W_{i}}
    $$
    
    Then, for a random vector $v$ (which has the same dimension as $g_{i}$ ), we have:
    
    $$\begin{aligned} 
    
        \frac{\partial(g_{i}^{T} v)}{\partial W_{i}}
        
        &=\frac{\partial g_{i}^{T}}{\partial W_{i}} v+g_{i}^{T} \frac{\partial v}{\partial W_{i}} \quad \text{// Product Rule} \\ 
        &=\frac{\partial g_{i}^{T}}{\partial W_{i}} v \quad \text{// $v$ is independent of $W_i$} \\
        &=H_{i} v \end{aligned} $$

- æ¥ç€å°±å¯ä»¥ä½¿ç”¨Power Iterationç®—æ³•æ¥å¾—åˆ°æœ€å¤§çš„ç‰¹å¾æ ¹å’Œç‰¹å¾å‘é‡

   <center><img src="../../images/DL_QBert_6.png" width="55%"/></center>

   - $i$æ˜¯è¿­ä»£çš„è½®æ¬¡ï¼Œä¸€å…±è·‘nä¸ªiteration

   - å½“æˆ‘ä»¬å¯¹$gv$æ±‚å¯¼çš„æ—¶å€™ï¼Œå…¶å®æ˜¯ç»™$v$ ä¹˜äº†ä¸€ä¸ª$H$ï¼Œéšç€è¿™ä¸ªHè¶Šä¹˜è¶Šå¤šï¼Œ$v$å°±ä¼šconverge to dominate eigenvector. è¿™å…¶å®æ˜¯æ•°å€¼ä»£æ•°çš„ä¸€ç§æ–¹æ³•ï¼å¯ä»¥çœ‹[è¿™é‡Œ](http://mlwiki.org/index.php/Power_Iteration)

    <center><img src="../../images/DL_QBert_7.png" width="65%"/></center>


æœ‰äº†è¿™ä¸ªæ–¹æ³•ä¹‹åï¼Œå¯¹æ¯ä¸€å±‚æ¥è¯´ï¼Œæˆ‘ä»¬training dataï¼Œæˆ‘ä»¬éƒ½å¯ä»¥ç®—å‡ºä¸€ä¸ªHeissian matrixå¹¶ä¸”è®¡ç®—å¾—åˆ°top eigenvaluesï¼Œè€Œä¹‹å‰çš„æ–¹æ³•å°±æ˜¯æŠŠè¿™äº›top eigenvaluesåšä¸€ä¸ªå¹³å‡å½¢æˆå‡†åˆ™â€”â€”More aggressive quantization is performed for layers with smaller top eigenvalue.

#### QBERTçš„æ”¹è¿›

QBERTå‘ç°ï¼Œassigning bits based only on the average top eigenvalues is infeasible for many NLP tasksï¼Œæ¯”å¦‚å¯¹BERTè¿›è¡Œè¿™ä¸ªè¿‡ç¨‹æ±‚å¾—çš„ç‰¹å¾å€¼ä¼šå‘ç°ï¼šThe distribution of top Hessian eigenvalue for different layers of $\text {BERT}_{\text{BASE}}$ in different layers exhibit different magnitude of eigenvalues even though all layers have exactly same structure and size.

<center><img src="../../images/DL_QBert_8.png" width="65%"/></center>

- æ¯”å¦‚ä¸Šé¢è¿™å¼ å›¾SQuADçš„ç¬¬ä¸ƒå±‚çš„varianceæ˜¯61.6ï¼Œä½†å‡å€¼æ˜¯1.0ï¼å°½ç®¡è¿™ä¸ªvarianceæ˜¯10ä¸ª 10%æ•°æ®è®¡ç®—å‡ºæ¥çš„ç‰¹å¾å€¼ å¾—åˆ°çš„ï¼

    
é‚£ä¹ˆBertæå‡ºçš„æ–¹æ³•æ˜¯è¿™æ ·çš„

1. åˆ¤æ–­å‡†åˆ™ï¼šå†åŠ ä¸Šè¿™ä¸ªtop eigenvaluesçš„æ ‡å‡†å·®ï¼
    $$\Omega_{i} \triangleq\left|\operatorname{mean}\left(\lambda_{i}\right)\right|+\operatorname{std}\left(\lambda_{i}\right)$$

    - $\lambda_{i}$æ˜¯ç¬¬iå±‚çš„weightçš„$H_{i}$çš„æœ€é«˜å€¼ ç»„æˆçš„å‘é‡ï¼ˆå› ä¸ºæ•°æ®è¢«åˆ†æˆäº†10ä»½æ¥ç®— æ‰€ä»¥æœ‰10ä¸ª top eigenvaluesï¼‰

2. æœ‰äº†è¿™ä¸ªå‡†åˆ™ä¹‹åå°±å¯ä»¥æ¯”è¾ƒäº†ï¼šsort them in descending order, and we use it as a metric to relatively determine the quantization precision. We then perform quantization-aware fine-tuning based on the selected precision setting.

ä½†æ–‡ä¸­å¹¶æ²¡æœ‰è¯¦ç»†ç»™å‡ºè¿™äº›$\Omega_{i}$çš„å…·ä½“æ•°å€¼ï¼Œåªåœ¨æœ€åç»™å‡ºäº†ä»–ä»¬åœ¨ç”¨2/3 bit mixed precision ä»¥åŠ 2/4-bit mixed precisionçš„å…·ä½“æ–¹æ¡ˆï¼š

<center><img src="../../images/DL_QBert_10.jpg" width="75%"/></center>

- æ³¨æ„Embedding layerå› ä¸ºä½œè€…å‘ç°embedding layer is more sensitive to quantization than the encoder layers.

#### SQuADæ²¡æœ‰è¾¾åˆ°local minimaçš„é—®é¢˜

æ³¨æ„ï¼Œè¿™é‡Œä½œè€…è¿˜æå‡ºï¼ŒSQuADåœ¨fine-tunedçš„è¿‡ç¨‹é‡Œæ²¡æœ‰è¾¾åˆ°local minimumï¼Œå› ä¸ºHessiançŸ©é˜µä¸æ˜¯æ­£å®šçš„ã€‚

- ä¸€èˆ¬æ¥è¯´ï¼šbefore performing quantization the trained model has converged to a local minima!ä¹Ÿå°±æ˜¯å·²ç»è·‘äº†è¶³å¤Ÿçš„è¿­ä»£äº†

    è¾¾åˆ°local minimaçš„è¯æœ‰ä¸¤ä¸ªæ¡ä»¶ï¼šnecessary optimality conditions are zero gradientä»¥åŠpositive curvatureï¼ˆpositive Hessian eigenvalue)ï¼Œä½†æ˜¯è¿™é‡Œ

- ç„¶è€Œï¼ŒSQuAD has actually not converged to a local minima!

    <center><img src="../../images/DL_QBert_9.png" width="75%"/></center>

    - ä»è¿™å¼ å›¾ä¹Ÿå¯ä»¥çœ‹å‡ºæ¥ï¼šSQuADçš„fine-tuned BERTæ˜¯convergeåœ¨äº†saddle pointç‚¹ä¸Šï¼

å› æ­¤ï¼Œæ¥ä¸‹æ¥çš„æ–¹æ³•that performing quantization on SQuAD would lead to higher performance degradation as compared to other tasks, and this is indeed the case as will be discussed next.



### Group-wise Quantization ç»„é‡åŒ–

å¾…æ›´æ–°ingğŸ˜­

<center><img src="../../images/DL_QBert_11.png" width="75%"/></center>


## å‚è€ƒèµ„æ–™

- https://www.bilibili.com/video/av458127699?from=search&seid=14421186488406937921&spm_id_from=333.337.0.0
- https://www.youtube.com/watch?v=v1oHf1KV6kM
- https://www.youtube.com/watch?v=aX4Tm1s01wY
- https://zhuanlan.zhihu.com/p/103429033

- [Towards Energy-efficient Quantized Deep Spiking Neural Networks for Hyperspectral Image Classification](https://arxiv.org/abs/2107.11979)