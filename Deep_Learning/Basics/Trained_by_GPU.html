
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>GPU训练 &#8212; Towards a Full-stack DA</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet">
  <link href="../../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="NLP" href="../NLP/README.html" />
    <link rel="prev" title="优化器" href="Optimizer.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/logo2.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Towards a Full-stack DA</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../intro.html">
   Jace的六边形DA笔记库
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  数据ETL流程
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../Data_ETL/README.html">
   DE基础
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../Data_ETL/Data_cleaning/README.html">
   数据清洗与处理
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Data_ETL/Data_cleaning/Regex.html">
     正则表达式
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Data_ETL/Data_cleaning/pandas.html">
     Pandas
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  统计分析
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../STAT/README.html">
   基础
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  因果推断
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../Causal_Inference/README.html">
   基础
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../Causal_Inference/1_AB_testing.html">
   AB Testing
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  深度学习
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="README.html">
   基础
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="DL_hyperparameter.html">
     DL常见超参及调整策略
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Optimizer.html">
     优化器
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     GPU训练
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../NLP/README.html">
   NLP
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../NLP/basics.html">
     基础概念
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../NLP/Self-attention.html">
     Self-Attention
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../NLP/Transformer.html">
     Transformer
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../NLP/BERT.html">
     BERT
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../NN_compression/README.html">
   神经网络压缩
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../NN_compression/KD.html">
     知识蒸馏
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
    <label for="toctree-checkbox-5">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../NN_compression/Distill_Bert.html">
       Distill Bert
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../NN_compression/Quantize.html">
     量化
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
    <label for="toctree-checkbox-6">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../NN_compression/QBert.html">
       Q-BERT
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  备用的模板
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../notebooks.html">
   Content with notebooks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference external" href="https://www.linkedin.com/in/jinhang-yang/">
     LinkedIn
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/Deep_Learning/Basics/Trained_by_GPU.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/Jace-Yang/Full-Stack_Data-Analyst"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/Jace-Yang/Full-Stack_Data-Analyst/issues/new?title=Issue%20on%20page%20%2FDeep_Learning/Basics/Trained_by_GPU.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        <a class="edit-button" href="https://github.com/Jace-Yang/Full-Stack_Data-Analyst/edit/master/Deep_Learning/Basics/Trained_by_GPU.md"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   基础
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#single-node-single-gpu-training">
     Single Node, Single GPU Training
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dl-scaling-with-gpu">
     DL Scaling with GPU
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#commonly-used-gpu-accelerators-in-deep-learning">
       硬件选择：Commonly used GPU Accelerators in Deep Learning
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id2">
       对比单GPU的提升
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#single-node-multi-gpu-training">
     Single Node, Multi-GPU Training
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#distributed-training">
     Distributed Training
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#partition-and-synchronize">
       Partition and synchronize:
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#aggregation">
       Aggregation
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#performance-metric-scaling-efficiency">
       Performance metric: Scaling efficiency：
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#parallelism">
       Parallelism
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#model-parallelism">
         Model Parallelism
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#data-parallelism">
         Data Parallelism
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#parameter-server-ps-based-synchronization">
       Parameter server (PS) based Synchronization
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#synchronous-sgd">
         Synchronous SGD
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sync-vs-async">
         Sync VS Async
        </a>
        <ul class="nav section-nav flex-column">
         <li class="toc-h6 nav-item toc-entry">
          <a class="reference internal nav-link" href="#id3">
           效率的考虑
          </a>
         </li>
         <li class="toc-h6 nav-item toc-entry">
          <a class="reference internal nav-link" href="#stale-gradients">
           Stale Gradients
          </a>
         </li>
        </ul>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#decentralized-aggregation-peer-to-peer">
       Decentralized Aggregation - Peer-to-peer
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#downpour-sgd">
   Downpour SGD
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id4">
     架构
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#stochasticity">
     Stochasticity的来源
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id5">
     表现评估
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#distributed-scaling-speed-up">
   Distributed Scaling Speed up
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#reduction-over-gradients">
     Reduction over gradients的原理
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id6">
     实际效果验证
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#poweral-ddl">
   PowerAl DDL
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#benchmarking">
     Benchmarking的术语
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id7">
     实际结果
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#d-torus-topology-for-inter-gpu-communication">
   2-D Torus Topology for inter-gpu communication
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#meaningful-distributed-dl-system-comparison">
   Meaningful Distributed DL System Comparison
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>GPU训练</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   基础
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#single-node-single-gpu-training">
     Single Node, Single GPU Training
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dl-scaling-with-gpu">
     DL Scaling with GPU
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#commonly-used-gpu-accelerators-in-deep-learning">
       硬件选择：Commonly used GPU Accelerators in Deep Learning
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id2">
       对比单GPU的提升
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#single-node-multi-gpu-training">
     Single Node, Multi-GPU Training
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#distributed-training">
     Distributed Training
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#partition-and-synchronize">
       Partition and synchronize:
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#aggregation">
       Aggregation
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#performance-metric-scaling-efficiency">
       Performance metric: Scaling efficiency：
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#parallelism">
       Parallelism
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#model-parallelism">
         Model Parallelism
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#data-parallelism">
         Data Parallelism
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#parameter-server-ps-based-synchronization">
       Parameter server (PS) based Synchronization
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#synchronous-sgd">
         Synchronous SGD
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sync-vs-async">
         Sync VS Async
        </a>
        <ul class="nav section-nav flex-column">
         <li class="toc-h6 nav-item toc-entry">
          <a class="reference internal nav-link" href="#id3">
           效率的考虑
          </a>
         </li>
         <li class="toc-h6 nav-item toc-entry">
          <a class="reference internal nav-link" href="#stale-gradients">
           Stale Gradients
          </a>
         </li>
        </ul>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#decentralized-aggregation-peer-to-peer">
       Decentralized Aggregation - Peer-to-peer
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#downpour-sgd">
   Downpour SGD
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id4">
     架构
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#stochasticity">
     Stochasticity的来源
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id5">
     表现评估
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#distributed-scaling-speed-up">
   Distributed Scaling Speed up
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#reduction-over-gradients">
     Reduction over gradients的原理
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id6">
     实际效果验证
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#poweral-ddl">
   PowerAl DDL
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#benchmarking">
     Benchmarking的术语
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id7">
     实际结果
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#d-torus-topology-for-inter-gpu-communication">
   2-D Torus Topology for inter-gpu communication
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#meaningful-distributed-dl-system-comparison">
   Meaningful Distributed DL System Comparison
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="gpu">
<h1>GPU训练<a class="headerlink" href="#gpu" title="Permalink to this headline">¶</a></h1>
<div class="section" id="id1">
<h2>基础<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<div class="section" id="single-node-single-gpu-training">
<h3>Single Node, Single GPU Training<a class="headerlink" href="#single-node-single-gpu-training" title="Permalink to this headline">¶</a></h3>
<p>Training throughput depends on:</p>
<ul class="simple">
<li><p>Neural network model (activations, parameters, compute operations)</p></li>
<li><p>Batch size</p></li>
<li><p>Compute hardware: GPU type (e.g., Nvidia M60, K80, P100, V100)</p></li>
<li><p>Floating point precision (FP32 vs FP16)</p>
<ul>
<li><p>换了FP16之后相当于working with reduced precision arithmetic｜Using FP16 can reduce training times and enable larger batch sizes/models without significantly impacting the accuracy of the trained model</p></li>
</ul>
</li>
</ul>
<p>训练时间：Training time with single GPU very large: 6 days with Places dataset (2.5M images) using Alexnet on a single K40.</p>
<p>提高performance的办法一般是Increasing batch size increases throughput</p>
<ul class="simple">
<li><p>然而Batch size is restricted by GPU memory！</p></li>
<li><p>GPU的限制下⇒Small batch size <span class="math notranslate nohighlight">\(=&gt;\)</span> noisier approximation of the gradient <span class="math notranslate nohighlight">\(\Rightarrow\)</span> lower learning rate <span class="math notranslate nohighlight">\(=&gt;\)</span> slower convergence</p></li>
</ul>
</div>
<div class="section" id="dl-scaling-with-gpu">
<h3>DL Scaling with GPU<a class="headerlink" href="#dl-scaling-with-gpu" title="Permalink to this headline">¶</a></h3>
<div class="section" id="commonly-used-gpu-accelerators-in-deep-learning">
<h4>硬件选择：Commonly used GPU Accelerators in Deep Learning<a class="headerlink" href="#commonly-used-gpu-accelerators-in-deep-learning" title="Permalink to this headline">¶</a></h4>
<img src="https://cdn.mathpix.com/snip/images/EgvWEJqCtlVEjyoLDQ1kqDWtRSVN5q_ZW9jcmnqsJFM.original.fullsize.png" />
<ul class="simple">
<li><p>两个K80就可以实现6个TFLOPS</p></li>
</ul>
</div>
<div class="section" id="id2">
<h4>对比单GPU的提升<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h4>
<img src="https://cdn.mathpix.com/snip/images/ym85dUm7bzwccOmHZq03IOOmqBdaK34LrMLl783Nyws.original.fullsize.png" />
<ul class="simple">
<li><p>不同模型的speedup不一样，因为different networks have different structure, they have different amount of floating point operations requirement, they have different memory requirement</p></li>
</ul>
<img src="https://cdn.mathpix.com/snip/images/F8v3Uz9CFYVg0XIHolZnrAKvV4u5H-AE1mapfdeJrMQ.original.fullsize.png" />
- 从batch size提高后的improve来看也是一样的model dependent！
</div>
</div>
<div class="section" id="single-node-multi-gpu-training">
<h3>Single Node, Multi-GPU Training<a class="headerlink" href="#single-node-multi-gpu-training" title="Permalink to this headline">¶</a></h3>
<p>最重要的是communicate的cost！让Scaling not linear的各种影响因素——Synchronization</p>
<ul class="simple">
<li><p>Communication libraries (e.g., NCCL) and supported communication algorithms/collectives (broadcast, all-reduce, gather)</p>
<ul>
<li><p>NCCL (“Nickel”) is library of accelerated collectives that is easily integrated and topology-aware so as to improve the scalability of multi-GPU applications</p></li>
</ul>
</li>
<li><p>Communication link bandwidth: PCle/QPI or NVlink</p></li>
<li><p>Communication algorithms depend on the communication topology (ring, hub-spoke, fully connected) between the GPUs.</p></li>
<li><p>Most collectives amenable to bandwidth-optimal implementation on rings, and many topologies can be interpreted as one or more rings [P. Patarasuk and X. Yuan]</p></li>
</ul>
<img src="https://cdn.mathpix.com/snip/images/XWDeJcrg4WsPOm1CDXkWF005zbjCK7KoXanRu0yo630.original.fullsize.png" />
<ul class="simple">
<li><p>V100优势在于GPU和GPU之间的connectivity很好</p></li>
</ul>
<p>没有NVLINK的时候：</p>
<img src="https://cdn.mathpix.com/snip/images/pwqaKYxymHmAt-5HJ7HR37vmC5lgVfR8ITd73dYZVjE.original.fullsize.png" />
<p>有NVLINK的时候：</p>
<img src="https://cdn.mathpix.com/snip/images/bFRPoR_Y-bsmIVYEzzLtJzSnchkKboILT1Zh3O7HCus.original.fullsize.png" />
<img src="https://cdn.mathpix.com/snip/images/EUtejV4EyMXJtlUlGNuAA6XayYDeicPhJlmuCSw_oH4.original.fullsize.png" />
<img src="https://cdn.mathpix.com/snip/images/6UpUK2ib1ACVqIj-N7a42p-Gs-V2xUZrTcFTkAWHbD8.original.fullsize.png" />
</div>
<div class="section" id="distributed-training">
<h3>Distributed Training<a class="headerlink" href="#distributed-training" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><p>multiple nodes in multiple machines (GPUs): parallelism by distributing your job across multiple machines</p>
</div></blockquote>
<div class="section" id="partition-and-synchronize">
<h4>Partition and synchronize:<a class="headerlink" href="#partition-and-synchronize" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p>Type of Parallelism: Model, Data, Hybrid</p>
<ul>
<li><p>Data Parallelism: partition the data set across multiple machines and then each one of them can do the training using that dataset (但是有整个model)</p></li>
<li><p>Model Parallelism: train portions of a big model on different machines (但用整个dataset)</p></li>
<li><p>Hybrid: partition the model and also partition the data.</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="aggregation">
<h4>Aggregation<a class="headerlink" href="#aggregation" title="Permalink to this headline">¶</a></h4>
<ul>
<li><p>目的</p>
<ul>
<li><p><strong>Average model得不到我们想要的global的model</strong>：partition之后即使每个模型看到的local sub-dataset的分布一样，但因为features的区别还是会train出不一样的parameters. 因为每个local model不会向总体generalize，所以Averaging them之后也不会generalized</p></li>
<li><p>因此，我们想要的是train一些iterations之后就做synchronize, 假设不分batch，做data parallelism的时候：</p>
  <center><img src="../../images/DL_GPU_1.png" width="45%"/></center>
<ul class="simple">
<li><p>开始先initialize with same weight</p></li>
<li><p>forward pas get the loss</p></li>
<li><p>backward get the gradient</p></li>
<li><p>sum the gradient （会<span class="math notranslate nohighlight">\(\iff\)</span>用总loss算的gradient）</p></li>
<li><p>然后让每个submodel从这套parameter再继续迭代</p></li>
</ul>
<p>如果分了batch size n：</p>
<ul class="simple">
<li><p>那就总共就pass <span class="math notranslate nohighlight">\(n \times m\)</span>个samples</p></li>
<li><p>得到<span class="math notranslate nohighlight">\(n \times m\)</span>个gradients</p></li>
<li><p>再计算<span class="math notranslate nohighlight">\(\frac{\partial}{\partial w} \sum_{i=1}^{m} \sum_{j=1}^{n}Loss_{i, j}\)</span></p></li>
</ul>
</li>
</ul>
</li>
<li><p>Type of Aggregation: Centralized, decentralized</p>
<ul class="simple">
<li><p><mark style="background-color:#e1e1e1;">Centralized aggregation: parameter server</mark>——就是把gradient传送到centralized server，center的只maintain model parameter不做training</p></li>
<li><p><mark style="background-color:#e1e1e1;">Decentralized aggregation: P2P, all reduce</mark>——直接让workers之间交流实现aggregation</p></li>
</ul>
   <center><img src="../../images/DL_GPU_2.png" width="45%"/></center>
</li>
</ul>
</div>
<div class="section" id="performance-metric-scaling-efficiency">
<h4>Performance metric: Scaling efficiency：<a class="headerlink" href="#performance-metric-scaling-efficiency" title="Permalink to this headline">¶</a></h4>
<div class="math notranslate nohighlight">
\[\frac{\text{run time of one iteration {\bf on a single GPU}}}{\text{the run time of one iteration when {\bf distributed over $n$ GPUs}}}\]</div>
<ul class="simple">
<li><p>batch size一样的话：对于n个GPU来说 是<span class="math notranslate nohighlight">\(n \times\)</span> Single GPU batch size，但是是parallel进行，所以每个GPU还是处理一样的数据！因此时间in 2 cases应该是一样的 <span class="math notranslate nohighlight">\(\Rightarrow \text{scaling efficiency} = 1\)</span></p></li>
<li><p>然而，GPU中需要communication的时间——Sychronize to get an aggregated gradient，所以会有更长的时间！ <span class="math notranslate nohighlight">\(\Rightarrow \text{scaling efficiency} &lt; 1\)</span></p></li>
</ul>
</div>
<div class="section" id="parallelism">
<h4>Parallelism<a class="headerlink" href="#parallelism" title="Permalink to this headline">¶</a></h4>
<p>Parallel execution of a training job on different compute units through</p>
<ul class="simple">
<li><p>scale-up (single node, multiple and faster GPUs)：让one machine的比如4个GPU 连接很快，实现powerful</p></li>
<li><p>scale-out (multiple nodes distributed training) ：用多个machine（nodes），可能每个都不是很贵但可以堆，不过这样network会成为bottleneck</p></li>
</ul>
<p>方式：</p>
<ul class="simple">
<li><p>Enables working with large models by partitioning model across learners</p></li>
<li><p>Enables efficient training with large datasets using large “effective” batch sizes (batch split across learners)</p></li>
<li><p>Speeds up computation</p></li>
</ul>
<div class="section" id="model-parallelism">
<h5>Model Parallelism<a class="headerlink" href="#model-parallelism" title="Permalink to this headline">¶</a></h5>
<p>Splitting the model across multiple learners，比如下面这个5层神经网络分给4个learner（learner之间连接加粗）</p>
 <center><img src="../../images/DL_GPU_3.png" width="45%"/></center>
<ul class="simple">
<li><p>做Partition的criteria</p>
<ul>
<li><p>minimize这些bold edges</p></li>
<li><p>也就是keep the nodes densely connect to each other in a same machine ⇒ exploit machine 的local compute power</p></li>
</ul>
</li>
</ul>
<p>Performance benefits depend on</p>
<ul class="simple">
<li><p>Connectivity structure</p></li>
<li><p>Compute demand of operations</p></li>
</ul>
<p>Heavy compute and local connectivity -benefit most</p>
<ul class="simple">
<li><p>Each machine handles a subset of computation</p></li>
<li><p>Low network traffic</p></li>
</ul>
</div>
<div class="section" id="data-parallelism">
<h5>Data Parallelism<a class="headerlink" href="#data-parallelism" title="Permalink to this headline">¶</a></h5>
 <center><img src="../../images/DL_GPU_4.png" width="45%"/></center>
<ul class="simple">
<li><p>Model is replicated on different learners</p></li>
<li><p>Data is sharded and each learner work on a different partition</p></li>
<li><p>Helps in efficient training with large amount of data</p></li>
<li><p>Parameters (weights, biases, gradients) from different replicas need to be synchronized</p></li>
</ul>
</div>
</div>
<div class="section" id="parameter-server-ps-based-synchronization">
<h4>Parameter server (PS) based Synchronization<a class="headerlink" href="#parameter-server-ps-based-synchronization" title="Permalink to this headline">¶</a></h4>
<div class="section" id="synchronous-sgd">
<h5>Synchronous SGD<a class="headerlink" href="#synchronous-sgd" title="Permalink to this headline">¶</a></h5>
<blockquote>
<div><p>Synchronous: at any instance of time the model at each of the workers is same (always working on the same model)</p>
</div></blockquote>
 <center><img src="../../images/DL_GPU_5.png" width="45%"/></center>
<ol class="simple">
<li><p>Each learner executes the entire model</p></li>
<li><p>After each mini-batch processing a learner calculates the gradient and sends it to the parameter server</p></li>
<li><p>The parameter server calculates new value of weights and sends them to the model replicas</p></li>
</ol>
<p><code class="docutils literal notranslate"><span class="pre">模型太大的问题</span></code>：partition into model shards，这样一个机器只用responsible for maintaining some parameters</p>
<p><code class="docutils literal notranslate"><span class="pre">沟通问题</span></code>：当模型数量很大的时候，bottleneck可能会变成bandwidth</p>
<p><code class="docutils literal notranslate"><span class="pre">内存问题</span></code>：模型很大的时候没法fit in single machine</p>
<ul class="simple">
<li><p>注意 比如每个模型10000个weight，100个模型，那parameter serve需要先接受100*10000，再average成10000个weight</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">Straggler</span> <span class="pre">problem问题</span></code>：PS needs to wait for updated gradients from all the learners before calculating the model parameters在center收集gradient的时候learner只能idle 等最慢的worker好了然后得到new parameter完成一次iteration</p>
<ul>
<li><p>具体解释：</p>
<ul class="simple">
<li><p>Even though size of mini-batch processed by each learner is same, updates from different learners may be available at different times at the PS</p>
<ul>
<li><p>Randomness in compute time at learners</p></li>
<li><p>Randomness in communication time between learners and Parameter Server</p></li>
</ul>
</li>
<li><p>Waiting for slow and straggling learners diminishes the speed-up offered by parallelizing the training</p></li>
</ul>
</li>
<li><p>解决方案1——Synchronous SGD Variants</p>
  <center><img src="../../images/DL_GPU_6.png" width="100%"/></center>
<blockquote>
<div><p>P: total number of learners<br>
K: number of learners/mini-batches the PS waits for before updating parameters<br>
Lightly shaded arrows indicate straggling gradient computations that are canceled.</p>
</div></blockquote>
<ul class="simple">
<li><p>K-sync SGD: PS waits for gradients from K learners before updating parameters; the remaining learners are canceled</p>
<ul>
<li><p>When K = P, K-sync SGD is same as Fully Sync-SGD</p></li>
</ul>
</li>
<li><p>K-batch sync: PS waits for gradients from K mini-batches before updating parameters; the remaining (unfinished) learners are canceled</p>
<ul>
<li><p>特点</p>
<ul>
<li><p>Irrespective of which learner the gradients come from</p></li>
<li><p>Wherever any learner finishes, it pushes its gradient to the PS, fetches current parameter at PS and starts computing gradient on the next mini-batch based on the same local value of the parameters</p></li>
</ul>
</li>
<li><p>Runtime per iteration reduces with K-batch sync; error convergence is same as K-sync</p></li>
</ul>
</li>
</ul>
</li>
<li><p>解决方案2——Asynchronous SGD and variants</p>
  <center><img src="../../images/DL_GPU_7.png" width="100%"/></center>
<ul class="simple">
<li><p>Async-SGD: do not need to guarantee that all the workers locally are working with the same model</p>
<ul>
<li><p>跟1-sync-SGD的异同：同样只用等一个model的结果，但不会把这个结果update到所有的learner而kill慢learner的结果，而是</p></li>
</ul>
</li>
<li><p>K-async SGD: PS waits for gradients from K learners before updating parameters but the remaining learners are not canceled; each learner may be giving a gradient calculated at stale version of the parameters</p>
<ul>
<li><p>When <span class="math notranslate nohighlight">\(K=1\)</span>, K-async SGD is same as Async-SGD</p></li>
<li><p>When <span class="math notranslate nohighlight">\(K=P\)</span>, K-async SGD is same as Fully Sync-SGD</p></li>
</ul>
</li>
<li><p>K-batch async: PS waits for gradients from K mini-batches before updating parameters; the remaining learners are not canceled</p></li>
<li><p>Wherever any learner finishes, it pushes its gradient to the PS, fetches current parameter at PS and starts computing gradient on the next mini-batch based on the current value of the PS</p></li>
<li><p>Runtime per iteration reduces with K-batch async; error convergence is same as K-async</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="sync-vs-async">
<h5>Sync VS Async<a class="headerlink" href="#sync-vs-async" title="Permalink to this headline">¶</a></h5>
<div class="section" id="id3">
<h6>效率的考虑<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h6>
<p>Sync-SGD的slowdown在研究中的结果：</p>
<div class="proof theorem admonition" id="my-theorem">
<p class="admonition-title"><span>Theorem </span></p>
<div class="theorem-content section" id="proof-content">
<p>Let the wall clock time of each learner to process a single mini-batch be <span class="math notranslate nohighlight">\(i.i.d .\)</span> random variables <span class="math notranslate nohighlight">\(X_{1}, X_{2}, \ldots, X_{P}\)</span>. Then the ratio of the <strong>expected runtimes per iteration</strong> for synchronous and asynchronous <span class="math notranslate nohighlight">\(S G D\)</span> is
$<span class="math notranslate nohighlight">\(
\frac{\mathbb{E}\left[T_{\text {Sync }}\right]}{\mathbb{E}\left[T_{\text {Async }}\right]}=P \frac{\mathbb{E}\left[X_{P: P}\right]}{\mathbb{E}[X]}
\)</span>$</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(X_{(P: P)}\)</span> <strong><span class="math notranslate nohighlight">\(P^{t h}\)</span> order statistic</strong> of <span class="math notranslate nohighlight">\(P\)</span> i.i.d. random variables <span class="math notranslate nohighlight">\(X_{1}, X_{2}, \ldots, X_{P}\)</span>.</p></li>
</ul>
</div>
</div><ul>
<li><p>这个等式在说：快的倍数=个数×最慢的那个除平均的</p></li>
<li><p>For <span class="math notranslate nohighlight">\(X_{i} \sim \exp (\mu) : \frac{\mathbb{E}\left[T_{\text {Sync }}\right]}{\mathbb{E}\left[T_{\text {Async }}\right]} \sim P \log P\)</span> 也就是说Sync的时间是Async的时间的倍数=最慢的名次 * log最慢的名次</p>
  <center><img src="../../images/DL_GPU_9.png" width="70%"/></center>
<ul class="simple">
<li><p>20个Processor的时候 Sync的等最慢的那个 Async的可以跑100次了</p></li>
</ul>
</li>
</ul>
<p>在学习率一样的时候，Async-SGD decay更快但是有higher error floor</p>
<center><img src="../../images/DL_GPU_10.png" width="70%"/></center>
<blockquote>
<div><p>就是说：Async可以在同样的时间做更多的epoch所以converge得更快，但final training error就不一定了</p>
</div></blockquote>
<ul>
<li><p>Trade-off在K不同的时候：</p>
  <center><img src="../../images/DL_GPU_11.png" width="70%"/></center>
<blockquote>
<div><p>Recall: Async等到K个之后不会Kill其他的，而是让他们继续，然后等下一波K个无论是哪个批次的Loss function出来的gradient，因此Async肯定要比Sync快！</p>
</div></blockquote>
<ul class="simple">
<li><p>K=1的时候是一样的</p></li>
<li><p>K很大 一直到样本size的时候，如果batches are independent, it doesn’t matter whether the K gradients are coming from the same or differnt learners，所以两者的convergence接近。但Async的还是快</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="stale-gradients">
<h6>Stale Gradients<a class="headerlink" href="#stale-gradients" title="Permalink to this headline">¶</a></h6>
<p>尽管单位时间 Async更多update会让model收敛更快，但也会遇到新的<code class="docutils literal notranslate"><span class="pre">Stale</span> <span class="pre">Gradients</span></code>的问题</p>
<center><img src="../../images/DL_GPU_8.png" width="80%"/></center>
<ul class="simple">
<li><p>PS updates happen without waiting for all learners ⇒ Weights that a learner uses to evaluate gradients may be old values of the parameters at PS</p>
<ul>
<li><p>Parameter server asynchronously updates weights</p></li>
<li><p>By the time learner gets back to the PS to submit gradient, the weights may have already been updated at the PS (by other learners)</p></li>
<li><p>Gradients returned by this learner are stale (i.e., were evaluated at an older version of the model)</p></li>
</ul>
</li>
<li><p>Stale gradients can make SGD unstable, slowdown convergence, cause sub-optimal convergence (compared to Sync-SGD)</p></li>
</ul>
<p><strong>这个gap有多大呢</strong>?Gupta el al. Staleness-aware Async-SGD for Distributed Deep Learning. 2016这篇论文做了衡量</p>
<center><img src="../../images/DL_GPU_12.png" width="100%"/></center>
<ul class="simple">
<li><p>结论：The gradient is on average <span class="math notranslate nohighlight">\(\frac{N \text{(number of learners)}}{K \text{(等多少个update)}}\)</span> steps out of date by the time they are applied to the global parameter vector.</p></li>
<li><p>结论指导我们 结果不好的时候可能是我们做update基于的learner太少了</p></li>
</ul>
<p>此外，还可以设置Staleness dependent的decreasing learning rate：
$<span class="math notranslate nohighlight">\(
\text { learning rate }(\alpha)=\frac{\text { base learning rate }\left(\alpha_{0}\right)}{\text { average staleness }(\langle\sigma\rangle)}
\)</span>$</p>
<center><img src="../../images/DL_GPU_13.png" width="80%"/></center>
<blockquote>
<div><p>n是多少个soft-sync；Number of learners <span class="math notranslate nohighlight">\(\lambda=30\)</span>; mini-batch size <span class="math notranslate nohighlight">\(\mu=128\)</span>.</p>
</div></blockquote>
<ul class="simple">
<li><p>效果: Dividing the learning rate by the average staleness ⇒ better convergence (achieves lower test error when using the <span class="math notranslate nohighlight">\(n\)</span>-softsync protocol)</p></li>
<li><p>With staleness-dependent learning rate setting Async-SGD can achieve accuracy comparable to Sync-SGD while achieving linear speedup of ASGD</p></li>
</ul>
<p>还有一个方法再考虑进weight的差别有多大</p>
<div class="math notranslate nohighlight">
\[
\eta_{j}=\min \left\{\frac{C}{\left\|\mathbf{w}_{j}-\mathbf{w}_{\tau(j)}\right\|_{2}^{2}}, \eta_{\max }\right\}
\]</div>
<ul>
<li><p><span class="math notranslate nohighlight">\(\eta_{j}\)</span> : learning rate at jth iteration of parameters at PS 注意——<strong>same for different model parameters</strong></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(w_{j}\)</span> : parameter value at jth iteration at <strong>PS</strong></p></li>
<li><p><span class="math notranslate nohighlight">\(w_{\tau(j)}\)</span> : parameter value used by the <strong>local</strong> learner (to calculate gradient) whose gradient pushing triggered jth iteration at PS</p></li>
<li><p>C: hyperparameter</p></li>
</ul>
</li>
<li><p>效果：更stable的convergence</p>
  <center><img src="../../images/DL_GPU_14.png" width="50%"/></center>
</li>
</ul>
</div>
</div>
</div>
<div class="section" id="decentralized-aggregation-peer-to-peer">
<h4>Decentralized Aggregation - Peer-to-peer<a class="headerlink" href="#decentralized-aggregation-peer-to-peer" title="Permalink to this headline">¶</a></h4>
<p>Every one sends to every one:
<center><img src="../../images/DL_GPU_15.png" width="50%"/></center></p>
<p>所以可以locally update weight！好处是parameter server不是bottleneck了，问题是communications比较高</p>
<ul class="simple">
<li><p>解决这个问题：compresse updates</p></li>
</ul>
<hr class="docutils" />
<p>接下来：看一系列解决distributed training的问题的大厂paper</p>
</div>
</div>
</div>
<hr class="docutils" />
<div class="section" id="downpour-sgd">
<h2>Downpour SGD<a class="headerlink" href="#downpour-sgd" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><p>Google</p>
</div></blockquote>
<div class="section" id="id4">
<h3>架构<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<p>Parameter、Model和Data 的partition一起做：</p>
<center><img src="../../images/DL_GPU_16.png" width="50%"/></center>
<blockquote>
<div><p>Model是Tensorflow的前身——DistBelief <br>一个Parameter Server shard只会看一部分的parameter<br>一个Model replica里面的四个小方块儿可以理解为在处理四部分的parameters 但最后一个learner会把所有的weight一起给parameter server（然后各个shard再去负责自己部分的数据接收、新weight计算和返回！）</p>
</div></blockquote>
<ul class="simple">
<li><p>Different PS shards update their parameters asynchronously, they do not need to communicate among themselves：因为每个PSshard只关心自己那part的parameter</p></li>
<li><p>Different model replicas run independently：Their only job is to calculate the gradient, to push the gradient through the parameter server or the shard and to get the updated model. And once they have done model, then they started to start doing the next iteration. 不需要coordination</p></li>
<li><p>Model replicas are permitted to fetch parameters and push gradients in separate threads. 意义：做完一个PS shard的weight update之后可以先开始下载它返回的新model的那部分parameter，等这个model replica更新完最后一个PS shard负责的parameter之后就基本只用等它的下载了</p></li>
<li><p>Tolerates variances in the processing speed of different model replicas：因为不需要wait！如果有个model fail，training也不会失败，但就是我们不会学习到给这个model的那部分数据了！</p></li>
<li><p>Tolerates failure of model replicas; More robust to machine failures than synchronous SGD.：Suppose some replica dies, you can bring back by bringing up another machine and get the model from the parameter server, share to the current model so that they will resume. 只用等待就可以了</p></li>
</ul>
</div>
<div class="section" id="stochasticity">
<h3>Stochasticity的来源<a class="headerlink" href="#stochasticity" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Model replicas will most likely be using “stale” parameters when calculating gradients locally</p></li>
<li><p>No guarantee that at any given moment the parameters on each shard of the parameter server have undergone the same number of updates. 因为K个参数 每个模型push K个 谁先到谁就update 但先到的那个会影响后到的那个的update结果</p>
<ul>
<li><p>Order of updates may be different at different PS shards</p></li>
</ul>
</li>
</ul>
<hr class="docutils" />
<p>Optimizer：<code class="docutils literal notranslate"><span class="pre">Adagrad</span> <span class="pre">learning</span> <span class="pre">rate</span></code>
$<span class="math notranslate nohighlight">\(
        \eta_{i, k}=\frac{\gamma}{\sqrt{\sum_{j=1}^{k} \Delta w_{i, j}^{2}}}\)</span>$</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\eta_{i, k}\)</span> : learning rate of the ith parameter at iteration k</p></li>
<li><p><span class="math notranslate nohighlight">\(\Delta w_{i, k}\)</span> is its gradient</p></li>
<li><p><span class="math notranslate nohighlight">\(\gamma\)</span> is the constant scaling factor</p></li>
</ul>
<hr class="docutils" />
<ul class="simple">
<li><p>improves robustness of Downpour SGD (Adagrad implemented locally within <strong>each parameter</strong> server shard, 所以也不用等！)</p></li>
<li><p>Effective learning rate ‘decreases very fast because of gradient accumulation from the beginning of training</p>
<ul>
<li><p>随着迭代进行，cumulative square sum分母增加所以learning rate 的decrease</p></li>
<li><p>Sparse的时候Adam好用：有些feature一直大，learning rate就会被压低！如果有些没啥用的，learning rate就会增大</p>
<ul>
<li><p>give <strong>frequently</strong> occurring features very <strong>low</strong> learning rates and <strong>infrequent</strong> features <strong>high</strong> learning rates</p></li>
<li><p>the learning rate adaptation facilitates finding and identifying very predictive but comparatively rare features</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<div class="section" id="id5">
<h3>表现评估<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h3>
<p>Scaling with Model Parallelism</p>
 <center><img src="../../images/DL_GPU_17.png" width="80%"/></center>
<blockquote>
<div><p><strong>Average training speed-up:</strong> the ratio of the time taken per iteration using only a single machine to the time taken using <span class="math notranslate nohighlight">\(N\)</span> partitions (machines)</p>
</div></blockquote>
<ul class="simple">
<li><p>Benefit diminishes when adding more machines:</p>
<ul>
<li><p>Less work per machine</p></li>
<li><p>Increased communication between machines</p></li>
</ul>
</li>
<li><p>Speed-up with model parallelism对不同的模型不同：</p>
<ul>
<li><p>Fully connected speech model with 42M parameters: <span class="math notranslate nohighlight">\(2.2 \times 8\)</span> machines</p></li>
<li><p>Locally connected image model with 1.7B parameters: <span class="math notranslate nohighlight">\(12 x\)</span> with 81 machines</p></li>
</ul>
</li>
</ul>
<p>Scaling with Downpour SGD</p>
 <center><img src="../../images/DL_GPU_18.png" width="80%"/></center>
<p>Runtime-Resource Tradeoff in Downpour SGD</p>
  <center><img src="../../images/DL_GPU_19.png" width="80%"/></center>
<ul class="simple">
<li><p>Points closer to the origin are preferable in that they take less time while using fewer resources.</p></li>
</ul>
</div>
</div>
<div class="section" id="distributed-scaling-speed-up">
<h2>Distributed Scaling Speed up<a class="headerlink" href="#distributed-scaling-speed-up" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><p>Facebook</p>
</div></blockquote>
<div class="section" id="reduction-over-gradients">
<h3>Reduction over gradients的原理<a class="headerlink" href="#reduction-over-gradients" title="Permalink to this headline">¶</a></h3>
<p>To synchronize gradients of <span class="math notranslate nohighlight">\(N\)</span> learners, a reduction operation needs to be performed
$<span class="math notranslate nohighlight">\(
\sum_{j=1}^{N} \Delta w_{j}
\)</span>$</p>
<center><img src="../../images/DL_GPU_20.png" width="70%"/></center>
<ul class="simple">
<li><p>之前的办法是parameters server（左）</p></li>
<li><p>现在用新的——logical ring</p>
<ul>
<li><p>Each node has a left neighbor and a right neighbor</p></li>
<li><p>Node only sends data to its right neighbor, and only receives data from its left neighbor</p></li>
</ul>
</li>
<li><p>Both PS and Ring all-reduce involve synchronous parameter updates</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">Ring</span> <span class="pre">All-Reduce</span></code>的Two Step algorithm</p>
<ol>
<li><p>Scatter-reduce: GPUs exchange data such that <em>every GPU ends up with a chunk of the final result</em></p>
 <center><img src="../../images/DL_GPU_21.png" width="75%"/></center>
<ul class="simple">
<li><p>N - 1 rounds，每次send <span class="math notranslate nohighlight">\(\frac{N}{P}\)</span>个parameter，比如这里就是4轮 假设100个参数 每次传20个加到下面的GPU去</p></li>
</ul>
</li>
<li><p>Allgather: GPUs exchange chunks from scatter-reduce such that <em>all GPUs end up with the complete final result.</em></p>
 <center><img src="../../images/DL_GPU_22.png" width="75%"/></center>
<ul class="simple">
<li><p>N - 1 rounds</p></li>
</ul>
</li>
</ol>
<p>Parameter Server和Ring All-Reduce的communication cost比较</p>
<blockquote>
<div><p>P: number of processes  N: total number of model parameters</p>
</div></blockquote>
<ul>
<li><p>PS (centralized reduce)需要 <span class="math notranslate nohighlight">\(2 N(P-1)\)</span></p>
<ul>
<li><p>Amount of data sent to PS by (P-1) learner processes: N(P-1)</p>
<p>After reduce, PS sends back updated parameters to each learner</p>
</li>
<li><p>Amount of data sent by PS to learners: N(P-1)</p></li>
</ul>
</li>
<li><p>Ring All-Reduce (decentralized reduce) 需要 <span class="math notranslate nohighlight">\(2 N(P-1) / P\)</span></p>
<ul class="simple">
<li><p>Scatter-reduce: Each process sends N/P amount of data to (P-1) learners - Total amount sent (per process): N (P-1)/P</p></li>
<li><p>AllGather: Each process again sends N/P amount of data to <span class="math notranslate nohighlight">\((P-1)\)</span> learners</p></li>
</ul>
</li>
</ul>
<p>结论：PS communication cost is proportional to <span class="math notranslate nohighlight">\(P\)</span> whereas ring all-reduce cost is practically independent of <span class="math notranslate nohighlight">\(P\)</span> for large <span class="math notranslate nohighlight">\(P\)</span> (ratio (P-1)/P tends to 1 for large <span class="math notranslate nohighlight">\(P\)</span> )</p>
<p>在DL上的应用：Backpropagation</p>
<ul class="simple">
<li><p>Backpropagation computes gradients starting from the output layer and moving towards in the input layer</p>
<ul>
<li><p>Gradients for output layers are available earlier than inner layers</p></li>
</ul>
</li>
<li><p>Start all reduce on the output layer parameters while other gradients are being computed</p>
<ul>
<li><p>Overlay of <em>communication and local compute</em>: 最终我们需要所有的gradient做all-reduction，但是我们可以不用等，比如Output往前的gradient算好了就开始reduction，然后继续往前backpropagate。相当于gradient计算完成的时候，就只差input layer的all-reduce没做其他都做好了</p></li>
</ul>
</li>
<li><p>Demonstrated training of a ResNet-50 network in one hour on 256 GPUs</p></li>
<li><p>Major techniques:</p>
<ul>
<li><p>Data parallelism （没有model)</p></li>
<li><p>Very large batch sizes</p></li>
<li><p>Learning rate adjustment technique to deal with large batches</p>
<ul>
<li><p>Gradual warm up of learning rate from low to high value in 5 epochs</p></li>
</ul>
</li>
<li><p>MPI_AllReduce for communication between machines</p></li>
<li><p>NCCL communication library between GPUs on a machine connected using NVLink</p></li>
<li><p>Gradient aggregation performed in paralle/ with backprop</p>
<ul>
<li><p>No data dependency between gradients across layers?</p></li>
<li><p>As soon as the gradient for a layer is computed, it is aggregated across workers, while gradient computation for the next layer continues (就是用了overlap of communication and local compute)</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<div class="section" id="id6">
<h3>实际效果验证<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h3>
<p>模型设置</p>
<ul class="simple">
<li><p>Each server contains 8 NVIDIA Tesla P100 GPUs interconnected with NVIDIA NVLink</p>
<ul>
<li><p>32 servers × 每个server 8个 = 256个P100 GPUs</p></li>
</ul>
</li>
<li><p>50 Gbit Ethernet</p></li>
<li><p>Dataset: 1000-way ImageNet classification task; 28 million training images; 50,000 validation images; top- 1 error</p></li>
<li><p>ResNet-50</p></li>
<li><p>Mini-batch size per GPU: 32 (fixed, weak scaling across servers)</p></li>
<li><p>optimizer: Caffe2</p></li>
<li><p>Learning rate: <span class="math notranslate nohighlight">\(\eta=0.1 \cdot \frac{k n}{256}\)</span></p>
<ul>
<li><p>这个例子里 学习率是0.1</p>
<ul>
<li><p>k = batch size per GPU(worker) 就是32</p></li>
<li><p>n = 8个GPU（1个server）</p></li>
</ul>
</li>
<li><p>如果用 2个server, 学习率是0.2</p>
<ul>
<li><p>n = 16</p></li>
<li><p>k = 32</p></li>
</ul>
</li>
<li><p>总之学习率跟n和k是成正比的</p></li>
</ul>
</li>
</ul>
<p>Large Minibatches的Linear Scaling Rule: When the minibatch size is multiplied by <span class="math notranslate nohighlight">\(k\)</span>, multiply the learning rate by <span class="math notranslate nohighlight">\(k\)</span>.</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(w_{t+1}=w_{t}-\eta \frac{1}{n} \sum_{x \in \mathcal{B}} \nabla l\left(x, w_{t}\right)\)</span></p>
<ul class="simple">
<li><p>Average gradient of <span class="math notranslate nohighlight">\(\mathcal{B}\)</span> batches</p></li>
</ul>
</li>
<li><p>Suppose, <span class="math notranslate nohighlight">\(\nabla l\left(x, w_{t}\right) \approx \nabla l\left(x, w_{t+j}\right)\)</span> for <span class="math notranslate nohighlight">\(j&lt;k\)</span>, then <span class="math notranslate nohighlight">\(\hat{w}_{t+1} \approx w_{t+k}\)</span> 这个时候可以偷懒:</p>
<p>从<span class="math notranslate nohighlight">\(w_{t+k}=w_{t}-\eta \frac{1}{n} \sum_{j&lt;k} \sum_{x \in \mathcal{B}_{j}} \nabla l\left(x, w_{t+j}\right)\)</span> 从j到k一个一个算梯度</p>
<p>⇒ <span class="math notranslate nohighlight">\(\hat{w}_{t+1}=w_{t}-\hat{\eta} \frac{1}{k n} \sum_{j&lt;k} \sum_{x \in \mathcal{B}_{j}} \nabla l\left(x, w_{t}\right)\)</span> 用t的梯度近似之前的梯度</p>
<p>⇒ <span class="math notranslate nohighlight">\(\hat{\eta} = k \times \eta \)</span></p>
</li>
</ul>
<p>用32个servers的效果是最好的：</p>
<center><img src="../../images/DL_GPU_23.png" width="75%"/></center>
<p>Runtime scaling的情况：</p>
<center><img src="../../images/DL_GPU_24.png" width="75%"/></center>
<ul class="simple">
<li><p>Why time per iteration increases little with increasing minibatch size?：更多的machine需要communicate the gradients更多</p></li>
<li><p>Why time per epoch decreases with increasing batch size ? ：虽然单个iteration时间小小增加，但是number of iterations减少了很多</p></li>
<li><p>With 44 servers how much time it takes to finish 1 epoch of training ?：352的GPU的话 一个epoch 0.5秒</p></li>
<li><p>Can you get throughput (images/sec) from time per epoch ? Do you need to know batch size ?：能算！</p></li>
<li><p>Can you get throughput (images/sec) from time to process a minibatch (iteration)? Do you need to know batch size?</p></li>
<li><p>If K-batch sync or K-sync (K &lt; number of servers) was applied would the convergence been faster ? What about the final training error ?</p>
<ul>
<li><p>会converge faster但是final error不一定</p></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="section" id="poweral-ddl">
<h2>PowerAl DDL<a class="headerlink" href="#poweral-ddl" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><p>IBM</p>
</div></blockquote>
<img src="https://cdn.mathpix.com/snip/images/7j-0c71K_43IbuIq0fW_CxxOe4YxdFCrtuCCHbJMDvs.original.fullsize.png" />
<div class="section" id="benchmarking">
<h3>Benchmarking的术语<a class="headerlink" href="#benchmarking" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p>Speedup：<span class="math notranslate nohighlight">\(\frac{\text{1 machine的time per iteration}}{\text{n machine的time per iteration}}\)</span></p></li>
<li><p>Scaling efficiency：<span class="math notranslate nohighlight">\(\frac{\text{run time of one iteration on a single GPU}}{\text{run time of one iteration when distributed over N GPUs}}\)</span> ??? <span class="math notranslate nohighlight">\(\frac{\text{n machine的throughput}}{\text{1 machine的throughput}}\)</span></p>
<ul>
<li><p>One can satisfy any given scaling efficiency for any neural network by increasing the batch size and reducing communication overhead</p>
<p>But Too big a batch size will result in converging to an unacceptable accuracy or no convergence at all</p>
</li>
<li><p>throughput：how many images propcessed in 1 sec</p>
<ul class="simple">
<li><p>比如n machine，batchsize/machine=B，那就是<span class="math notranslate nohighlight">\(\frac{n \times B}{\text{one iteration across n machine }T(n)}\)</span></p></li>
<li><p>n machine，batchsize=B，那就是 <span class="math notranslate nohighlight">\(\frac{1 \times B}{\text{one iteration across 1 machine }T(1)}\)</span></p></li>
</ul>
</li>
<li><p>Scaling efficiency：<span class="math notranslate nohighlight">\(\frac{\text{n machine的 throughput}}{\text{1 machine的throughput}}\)</span> = <span class="math notranslate nohighlight">\(\frac{\frac{n \times B}{T(n)}}{\frac{1 \times B}{T(1)}}\)</span>=<span class="math notranslate nohighlight">\(\frac{n \times T(1)}{T(n)}=n\times\)</span> speedup</p></li>
<li><p>Speedup =  <span class="math notranslate nohighlight">\(\frac{m \times \text{time for iteration of 1 machine}}{\text{time for iteration of m machine}}\)</span></p></li>
</ul>
</li>
<li><p>Accuracy and end-to-end training time</p>
<ul class="simple">
<li><p>训练快是一回事，后面的怎么样！</p></li>
</ul>
</li>
<li><p>还有一些DL System performance的factor</p>
<ul class="simple">
<li><p>Neural network</p></li>
<li><p>Deep learning framework <span class="math notranslate nohighlight">\(\quad J P\)</span></p></li>
<li><p>GPU type</p></li>
<li><p>Communication overhead</p>
<ul>
<li><p>当计算时间太长的时候，会dominate！</p></li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Other considerations</p>
<ul>
<li><p>Systems under comparison should train to same accuracy</p></li>
<li><p>Accuracy should be reported on sufficiently large test set</p></li>
<li><p>Compute to communication ratio can vary widely for different neural networks. Using a neural network with high compute to communication ratio can hide the ills of an inferior distributed Deep Learning system.</p>
<ul class="simple">
<li><p>a sub-optimal communication algorithm or low bandwidth interconnect will not matter that much</p></li>
</ul>
</li>
<li><p>Computation time for one Deep Learning iteration can vary by up to <span class="math notranslate nohighlight">\(50 \%\)</span> when different Deep Learning frameworks are being used. This increases the compute to communication ratio and gives the inferior distributed Deep Learning system an unfair uplift to the scaling efficiency.</p></li>
<li><p>A slower GPU increases the compute to communication ratio and again gives the inferior distributed Deep Learning system an unfair uplift to the scaling efficiency.</p>
<ul class="simple">
<li><p>Nvidia P100 GPUs are approximately 3X faster than Nvidia K40 GPUs.</p></li>
<li><p>When evaluating the communication algorithm and the interconnect capability of a Deep Learning system, it is important to use a high performance GPU.</p></li>
</ul>
</li>
<li><p>Communication overhead = run time of one iteration when distributed over N GPUs <span class="math notranslate nohighlight">\(-\)</span> run time of one iteration on a single GPU.</p>
<blockquote>
<div><p>一个machine batch size 256， n个machine的batch size是n \times 256，但是每个machine还是working on 256 batchsize，所以这里的increase of time是来自于communication带来的！</p>
</div></blockquote>
<ul class="simple">
<li><p>Includes the communication latency and the time it takes to send the message (gradients) among the GPUs.</p></li>
<li><p>Communication overhead gives an indication of the quality of the communication algorithm and the interconnect bandwidth.</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="id7">
<h3>实际结果<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h3>
<img src="https://cdn.mathpix.com/snip/images/SAa_QjwyHibznt9gIUCZOb87Oibk1P5mwMLXsjdZg7o.original.fullsize.png" />
<ul class="simple">
<li><p>右边的Torch implementation, the reduction of gradients is not overlapped with compute; scaling efficiency is 84% vs 95% with Caffe</p></li>
</ul>
</div>
</div>
<div class="section" id="d-torus-topology-for-inter-gpu-communication">
<h2>2-D Torus Topology for inter-gpu communication<a class="headerlink" href="#d-torus-topology-for-inter-gpu-communication" title="Permalink to this headline">¶</a></h2>
<img src="https://cdn.mathpix.com/snip/images/hcuF_XqPfIjiji_yCVI7m8qvMDZeWeCThL0FihINalQ.original.fullsize.png" />
<img src="https://cdn.mathpix.com/snip/images/PUWnb1Zm8VJ5rctDHqXXoh4hlvlj_tzDmANLIztNWVQ.original.fullsize.png" />
</div>
<div class="section" id="meaningful-distributed-dl-system-comparison">
<h2>Meaningful Distributed DL System Comparison<a class="headerlink" href="#meaningful-distributed-dl-system-comparison" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>A popular neural network that has been widely trained and tuned</p></li>
<li><p>Use a Deep Learning framework that is computationally-efficient</p></li>
<li><p>Train to best accuracy on high performance GPUs</p></li>
<li><p>Report:</p>
<ul>
<li><p>Accuracy achieved</p></li>
<li><p>Training time to attain the accuracy</p></li>
<li><p>Scaling efficiency</p></li>
<li><p>Communication overhead</p></li>
</ul>
</li>
<li><p>Metric may also include <strong>power and cost</strong>.</p></li>
</ul>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./Deep_Learning/Basics"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="Optimizer.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">优化器</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../NLP/README.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">NLP</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Jace Yang<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>