# æœ‰ç›‘ç£å­¦ä¹ 

## æ•´ä½“æ¡†æ¶

**æœ€ç»ˆè¾“å‡ºï¼šè¶…å‚æ•°è®¾å®šâ•æ¨¡å‹â•æ¨¡å‹è¡¨ç°**

- Development-test splitã€Hyperparameter tuning ã€Optimal model trainingã€Model evaluationã€Model deployment

![img](../images/(null)-20220724153653287.(null))

- evaluationå¾ˆé‡è¦ï¼šå› ä¸ºæˆ‘ä»¬éœ€è¦çŸ¥é“æ¯ä¸ªå¤æ‚åº¦çš„æ¨¡å‹å¯¹åº”çš„è¡¨ç°æ˜¯å¤šå°‘ï¼Œæ¥åˆ¤æ–­æœ‰æ²¡æœ‰å¿…è¦ç”¨å¤æ‚çš„æ¨¡å‹ï¼

  å› æ­¤ï¼šä¸å¯ä»¥ç”¨æ•´ä¸ªdatasetå‡ºè¶…å‚æ•°è€Œæ”¾å¼ƒevaluation

### Data Preprocessing

#### Missing Data

- ä¸å¤„ç†ï¼ˆå½“æˆä¸€ç§å€¼ï¼‰ç„¶åç”¨ç¡®å®ä¸æ•æ„Ÿçš„æ ‘æ¨¡å‹

- å‰”é™¤

  - Drop column (typically used as baseline)ï¼šç¼ºå¤±å¤ªå¤šçš„æ—¶å€™
  - Drop rows (if there are only a few with missing values)

- å¡«å……ï¼ˆImputeï¼‰/ä¼°ç®—(estimation)ï¼š

  - mean or median (SimpleImputer in sklearn API) æ²¡æœ‰å……åˆ†è€ƒè™‘æ•°æ®ä¸­å·²æœ‰çš„ä¿¡æ¯ï¼Œè¯¯å·®å¯èƒ½è¾ƒå¤§
  - kNN (neighbors are found using nan_euclidean_distance  metric)
  - Regression modelsæ ¹æ®è°ƒæŸ¥å¯¹è±¡å¯¹å…¶ä»–é—®é¢˜çš„ç­”æ¡ˆï¼Œé€šè¿‡å˜é‡ä¹‹é—´çš„ç›¸å…³åˆ†ææˆ–é€»è¾‘æ¨è®ºè¿›è¡Œä¼°è®¡ã€‚ä¾‹å¦‚ï¼ŒæŸä¸€äº§å“çš„æ‹¥æœ‰æƒ…å†µå¯èƒ½ä¸å®¶åº­æ”¶å…¥æœ‰å…³ï¼Œå¯ä»¥æ ¹æ®è°ƒæŸ¥å¯¹è±¡çš„å®¶åº­æ”¶å…¥æ¨ç®—æ‹¥æœ‰è¿™ä¸€äº§å“çš„å¯èƒ½æ€§

- Add a binary additional indicator column (è·Ÿä¸Šä¸€æ­¥ä¸€è‡´)

  (often captured by adding missing indicator columns) 

  - Missing in not random! It will add value to the model
  - æ¯”å¦‚ï¼æœ‰ä¸ªclass is always missingï¼å°±åƒ16å²ä»¥ä¸‹çš„è¿™ä¸ªç»„æ²¡æœ‰é©¾ç…§å¹´é™,è¿™ä¸ªå¯ä»¥æ˜¯predictive columnsï¼ï¼

- Matrix factorizationï¼šå°†ä¸€ä¸ªå«*ç¼ºå¤±*å€¼çš„çŸ©é˜µ X åˆ†è§£ä¸ºä¸¤ä¸ª(æˆ–å¤šä¸ª)çŸ©é˜µ,ç„¶åè¿™äº›åˆ†è§£åçš„çŸ©é˜µç›¸ä¹˜å°± å¯ä»¥å¾—åˆ°åŸçŸ©é˜µçš„è¿‘ä¼¼ X

#### categorical data

æ³¨æ„ï¼šéƒ½æ˜¯å¯¹åˆ†å¼€ä¹‹åçš„æ•°æ®ï¼åªé’ˆå¯¹train data æ¥fit

- Ordinal encoding

  - Missing valueå¯ä»¥ç†è§£ä¸ºæœ€ä¸é‡è¦çš„classç„¶åç»™0ï¼Œä¹Ÿå¯ä»¥ç†è§£ä¸ºæœ€é‡è¦çš„ç»™maxï¼æˆ–è€…imputeæˆmode
- One-hot encoding: no information loss.

  - ç‰¹ç‚¹

    - å¤„ç†ç¼ºå¤±ï¼šmissingçš„æ—¶å€™å¯ä»¥æŠŠmissingå½“ä½œä¸€ç§category
    - æµ‹è¯•é›†é‡åˆ°æ–°çš„ç±»åˆ«çš„æ—¶å€™ï¼šåŠ å…¥`handle_unkown = "ignore"`å¯ä»¥
  - åœºæ™¯ï¼š
  
    - One-hot encoding introduces **multi-collinearity**
  
      - For e.g., x3 = 1 - x1 - x2 (in case when we have three categories)
      - Possible to remove one feature, because it's a **linear combination of the other columns**, could be problematic for some non-regularized regression models
      - Has **implications** on model interpretation
        - å¯ä»¥dropè¿™ä¸ªä¹Ÿå¯ä»¥dropåˆ«çš„ï¼Œè¿™æ ·çš„è¯feature importanceå°±ä¸åŒäº†
        - æœ‰äººå¯ä»¥keep all columns, and apply regulariazation to take care of during the training process, then get insights into the model
    - æœ‰çš„æ¨¡å‹ æ¯”å¦‚treesï¼Œå¯ä»¥split on categorical variables, so it will automatically handles categorical variablesï¼š
    
      - Tree-based models
      - Naive Bayes models
  - é—®é¢˜ï¼šLeads to high-dimensional datasets
- Target encodingï¼šä¸æ˜¯introduce 1 column for 1 category, è€Œæ˜¯summarize the information for each category and convert into 1 column

  - Generally applicable for high **cardinality** categorical features 

  - å…·ä½“encodeçš„æ–¹å¼å–å†³äºæ¨¡å‹é—®é¢˜ï¼š

    - `Regression`: Average target value for each category

    - `Classification`: Average of æ¦‚ç‡â€”â€”è¿™ä¸ªæ¯”ç›´æ¥mapåˆ°labelå¥½ï¼Œå› ä¸ºä¾ç„¶å¯ä»¥æ ¹æ®probabilityåŒºåˆ†å‡ºä¸åŒçš„classå¯¹yçš„å½±å“
      - `Binary classiï¬cation`: Probability of being in class 1
      
      - `Multiclass classiï¬cation`: *One feature per class* that gives the probability distribution

#### Numerical Feature Scaling

- scalingä¸ä¼šæ”¹å˜åŸå§‹æ•°æ®ï¼Œä½†æ˜¯ä¼šè®©æ¨¡å‹å˜å¾—å¥½
- è®°å¾—è¦fit_transform(è®­ç»ƒé›†)ï¼Œç„¶åtransform(æµ‹è¯•é›†)è€Œä¸æ˜¯fit_transform(æµ‹è¯•é›†)ï¼Œå› ä¸ºæˆ‘ä»¬ä¸çŸ¥é“æµ‹è¯•é›†çš„meanå’Œstdç­‰

<img src="../images/(null)-20220724215527501.(null)" alt="img" style="width:50%;" /><img src="../images/(null)-20220724215537886.(null)" alt="img" style="width:50%;" />

- å…·ä½“çš„æ–¹å¼ï¼š

  æ ‡å‡†åŒ–ï¼šæœ€å¤§æœ€å°æ ‡å‡†åŒ–ã€zæ ‡å‡†åŒ–â€”â€”StandardScaler()ã€MinMaxScaler()ã€MaxAbsScaler()ï¼ˆé™¤ä»¥æœ€å¤§å€¼çš„è¯è´Ÿæ•°è¿˜ä¼šæ˜¯è´Ÿæ•°ï¼‰ã€RobustScaler()ã€Nomalizer()ï¼ˆå˜æˆåœ†å½¢ï¼‰

  å½’ä¸€åŒ–ï¼šå¯¹äºæ–‡æœ¬æˆ–è¯„åˆ†ç‰¹å¾ï¼Œä¸åŒæ ·æœ¬ä¹‹é—´å¯èƒ½æœ‰æ•´ä½“ä¸Šçš„å·®å¼‚ï¼Œå¦‚aæ–‡æœ¬å…±20ä¸ªè¯ï¼Œbæ–‡æœ¬30000ä¸ªè¯ï¼Œbæ–‡æœ¬ä¸­å„ä¸ªç»´åº¦ä¸Šçš„é¢‘æ¬¡éƒ½å¾ˆå¯èƒ½è¿œè¿œé«˜äºaæ–‡æœ¬

- æ³¨æ„åº”è¯¥åšfitçš„æ•°æ®é›†è·Ÿåº”è¯¥åšfitçš„æ¨¡å‹æ˜¯ä¸€è‡´çš„

  - æ¯”å¦‚hyper parameter tuningçš„æ—¶å€™  scalerä¸åº”è¯¥ç¢°validation data

    <img src="../images/(null)-20220724215547062.(null)" alt="img" style="width:50%;" />

#### Outliers

- æ£€æµ‹æ–¹å¼ï¼šIQRã€LRä¹‹åçœ‹cookè·ç¦»



#### å¤„ç†æ ·æœ¬ä¸å¹³è¡¡

**Change data**

- Random Undersampling

- Random Oversampling

- Ensemble Resampling

  - A random re-sample of majority class is used for training each instance in an ensemble
  - The minority class is retained while training the instance.

- Synthetic Minority Oversampling Technique (SMOTE)

  <img src="../images/image-20220726111730712.png" alt="image-20220726111730712" style="width:50%;" />

  - Synthetic Minority Oversampling Technique (SMOTE) is a popular method to handle training with imbalanced datasets
  - SMOTE **adds synthetic interpolated sample**s to minority class
  - The following procedure is repeated for every original data point in minority class:
    - Pick a neighbor from $k$ nearest neighbors
    - **Sample a point randomly from the line** joining the two data points.
    - Add the point to the minority class
  - Leads to large datasets (due to oversampling)

**Change training procedure**

- assighing Class weightsï¼šmake sure the penalty of predicting minority wrong is more high!
  - Reweight each sample during training
    - Modify the loss function to account for class weights
    - Similar effect as oversampling (except that this is not random)
- ä¿®æ”¹æ¨¡å‹çš„æŸå¤±å‡½æ•°ï¼šä¸æ˜¯F1äº†è€Œæ˜¯æ›´ä¸åŒçš„

**é‡æ–°é€‰æ‹©è¯„ä»·æŒ‡æ ‡**ï¼š

- AP

**é‡æ„é—®é¢˜**

- ä»”ç»†å¯¹ä½ çš„é—®é¢˜è¿›è¡Œåˆ†æä¸æŒ–æ˜ï¼Œæ˜¯å¦å¯ä»¥å°†ä½ çš„é—®é¢˜åˆ’åˆ†æˆå¤šä¸ªæ›´å°çš„é—®é¢˜ï¼Œè€Œè¿™äº›å°é—®é¢˜æ›´å®¹æ˜“è§£å†³ã€‚



### æ¨¡å‹è®­ç»ƒæ­¥éª¤

Development-test split

- Random split

  - æ¯”ä¾‹å–å†³äºå®é™…é—®é¢˜
    - Large Sample Sizeï¼šä¸¤è¾¹éƒ½å¤Ÿ éšä¾¿
    - è®­ç»ƒé›†å°çš„ æ¯”å¦‚åªæœ‰100ä¸ªçš„æ—¶å€™å¯èƒ½éœ€è¦put asideå°‘ä¸€ç‚¹
    - æœ‰æ—¶å€™å¤ªå¤šäº†ï¼Œåªéœ€è¦è®­ç»ƒ50%çš„æ•°æ®å°±å¤Ÿäº†æ¥èŠ‚çœæ—¶é—´ï¼Œåé¢æ‹¿50%å»æµ‹è¯•ï¼Œtraining procures is shorten without comprimising the quality of model
  - æœ€åè¾“å‡ºå„ä¸ªtargetçš„ä¸ä¸€å®šæ˜¯å æ¯”ä¸€æ ·çš„

- Stratiï¬ed Splitting

  - The stratiï¬ed splitting ensures that the **ratio of classes in development** and **test datasets** equals that of the original dataset. 

  - Generally employed when performing classiï¬cation tasks on highly imbalanced datasets

  - indexæ˜¯class

    <img src="../images/(null)-20220724221510415.(null)" alt="img" style="width: 33%;" />

  - æ˜¯SK learnçš„é»˜è®¤å€¼ï¼

- Structured Splitting
  - The structured splitting is generally employed to prevent data leakage.
  - Examplesï¼šStock price predictionsã€Time-series predictions

<img src="../images/(null)-20220724221513024.(null)" alt="img" style="width: 33%;" />

Hyper-parameter tuning

æ ¸å¿ƒç›®æ ‡ï¼šTraining data â‡’ Select Best Parameters

<img src="../images/(null)-20220724221503960.(null)" alt="img" style="width:25%;" />

è¶…å‚çš„æ³¨æ„äº‹é¡¹â€”â€”æ³¨æ„å¤æ‚åº¦

- å¯¹æ¨¡å‹å¤æ‚åº¦çš„ç†è§£ï¼šå¯¹æ¨¡å‹å˜å¤æ‚ï¼Œæˆ‘ä»¬åœ¨åšBias-Varianceçš„Tradeoï¬€.æ¨¡å‹çš„é¢„æµ‹è¯¯å·®å¯ä»¥åˆ†è§£ä¸ºä¸‰ä¸ªéƒ¨åˆ†: åå·®(bias)ï¼Œ æ–¹å·®(variance) å’Œå™ªå£°(noise).

  - the conflict in trying to simultaneously minimize these two sources of [error](https://en.wikipedia.org/wiki/Errors_and_residuals_in_statistics) that prevent supervised learning algorithms from generalizing beyond their training set
    - **The** ***[bias](https://en.wikipedia.org/wiki/Bias_of_an_estimator)*** **error** is an error from erroneous assumptions in the learning [algorithm](https://en.wikipedia.org/wiki/Algorithm). High bias can cause an algorithm to *miss the relevant relations between features and target outputs* (underfitting). åå·®åº¦é‡äº†æ¨¡å‹çš„æœŸæœ›é¢„æµ‹ä¸çœŸå®ç»“æœçš„åç¦»ç¨‹åº¦ï¼Œ å³åˆ»ç”»äº†å­¦ä¹ ç®—æ³•æœ¬èº«çš„æ‹Ÿåˆèƒ½åŠ›ã€‚åå·®åˆ™è¡¨ç°ä¸ºåœ¨ç‰¹å®šåˆ†å¸ƒä¸Šçš„é€‚åº”èƒ½åŠ›ï¼Œåå·®è¶Šå¤§è¶Šåç¦»çœŸå®å€¼ã€‚
    
    - **The** ***[variance](https://en.wikipedia.org/wiki/Variance)*** is an error from **sensitivity** to **small fluctuations in the training set**. High variance may result from an algorithm modeling the random [noise](https://en.wikipedia.org/wiki/Noise_(signal_processing)) in the training data ([overfitting](https://en.wikipedia.org/wiki/Overfitting)). æ–¹å·®åº¦é‡äº†åŒæ ·å¤§å°çš„è®­ç»ƒé›†çš„å˜åŠ¨æ‰€å¯¼è‡´çš„å­¦ä¹ æ€§èƒ½çš„å˜åŒ–ï¼Œ å³åˆ»ç”»äº†æ•°æ®æ‰°åŠ¨æ‰€é€ æˆçš„å½±å“ã€‚æ–¹å·®è¶Šå¤§ï¼Œè¯´æ˜æ•°æ®åˆ†å¸ƒè¶Šåˆ†æ•£
    
    - å™ªå£°ï¼šå™ªå£°è¡¨è¾¾äº†åœ¨å½“å‰ä»»åŠ¡ä¸Šä»»ä½•æ¨¡å‹æ‰€èƒ½è¾¾åˆ°çš„æœŸæœ›æ³›åŒ–è¯¯å·®çš„ä¸‹ç•Œï¼Œ å³åˆ»ç”»äº†å­¦ä¹ é—®é¢˜æœ¬èº«çš„éš¾åº¦ ã€‚
    
      <img src="../images/(null)-20220724221505235.(null)" alt="img" style="width:45%;" /><img src="../images/(null)-20220724221443081.(null)" alt="img" style="width:45%;" />
    
      - æˆ‘ä»¬æƒ³è¦å·¦ä¸Šè§’ï¼šéƒ½å¾ˆå‡†ç¡®

- **è¿‡æ‹Ÿåˆé—®é¢˜**

  <img src="../images/(null)-20220724221442702.(null)" alt="img" style="width: 50%;" />

  - underfittingæ˜¯low variance high biasï¼šæ²¡æœ‰varianceä½†éƒ½é¢„æµ‹å‡ºåå·®äº† 
    - å½“ç®—æ³•ä»æ•°æ®é›†å­¦ä¹ çœŸå®ä¿¡å·çš„**çµæ´»æ€§æœ‰é™**æ—¶ï¼Œå°±ä¼šå‡ºç°åå·®ã€‚( æƒ³çš„å¤ªè¿‡ç®€å•ï¼Œæ¬ æ‹Ÿåˆ), æ‰€ä»¥æ¨¡å‹æ•´ä½“äº§ç”Ÿåå·®ã€‚
    - æ¬ æ‹ŸåˆæŒ‡çš„æ˜¯æ¨¡å‹æ²¡æœ‰å¾ˆå¥½åœ°å­¦ä¹ åˆ°æ•°æ®ç‰¹å¾ï¼Œä¸èƒ½å¤Ÿå¾ˆå¥½åœ°æ‹Ÿåˆæ•°æ®ï¼Œåœ¨è®­ç»ƒæ•°æ®å’ŒæœªçŸ¥æ•°æ®ä¸Šè¡¨ç°éƒ½å¾ˆå·®ã€‚
    - æ¬ æ‹Ÿåˆçš„åŸå› åœ¨äºï¼š
      - ç‰¹å¾é‡è¿‡å°‘ï¼›
      - æ¨¡å‹å¤æ‚åº¦è¿‡ä½


  - è§£å†³ï¼š
    - å¢åŠ æ–°ç‰¹å¾ï¼Œå¯ä»¥è€ƒè™‘åŠ å…¥è¿›ç‰¹å¾ç»„åˆã€é«˜æ¬¡ç‰¹å¾ï¼Œæ¥å¢å¤§å‡è®¾ç©ºé—´ï¼›

    - æ·»åŠ **å¤šé¡¹å¼ç‰¹å¾**ï¼Œè¿™ä¸ªåœ¨æœºå™¨å­¦ä¹ ç®—æ³•é‡Œé¢ç”¨çš„å¾ˆæ™®éï¼Œä¾‹å¦‚å°†çº¿æ€§æ¨¡å‹é€šè¿‡æ·»åŠ äºŒæ¬¡é¡¹æˆ–è€…ä¸‰æ¬¡é¡¹ä½¿æ¨¡å‹æ³›åŒ–èƒ½åŠ›æ›´å¼ºï¼›
    
    - **å‡å°‘æ­£åˆ™åŒ–å‚æ•°**ï¼Œæ­£åˆ™åŒ–çš„ç›®çš„æ˜¯ç”¨æ¥é˜²æ­¢è¿‡æ‹Ÿåˆçš„ï¼Œä½†æ˜¯æ¨¡å‹å‡ºç°äº†æ¬ æ‹Ÿåˆï¼Œåˆ™éœ€è¦å‡å°‘æ­£åˆ™åŒ–å‚æ•°ï¼›
    - ä½¿ç”¨**éçº¿æ€§æ¨¡å‹**ï¼Œæ¯”å¦‚æ ¸SVM ã€å†³ç­–æ ‘ã€æ·±åº¦å­¦ä¹ ç­‰æ¨¡å‹ï¼›
    - è°ƒæ•´**æ¨¡å‹çš„å®¹é‡(capacity)**ï¼Œé€šä¿—åœ°ï¼Œæ¨¡å‹çš„å®¹é‡æ˜¯æŒ‡å…¶æ‹Ÿåˆå„ç§å‡½æ•°çš„èƒ½åŠ›ï¼›
    - å®¹é‡ä½çš„æ¨¡å‹å¯èƒ½å¾ˆéš¾æ‹Ÿåˆè®­ç»ƒé›†ã€‚


  - overfittingæ˜¯high variance low biasï¼šå¹³å‡æ¥çœ‹çš„è¯ æ˜¯center the plot means doing well! ä½†varianceéå¸¸é«˜

    - å¤ªå…³æ³¨è®­ç»ƒé›†ä¸­ä¸ªä½“æ³¢åŠ¨ï¼Œè¿‡æ‹Ÿåˆ
    - é«˜æ–¹å·®æ¨¡å‹ï¼Œå¯¹ç‰¹å®šè®­ç»ƒæ•°æ®é›†çš„çµæ´»æ€§æé«˜ã€‚
    - é«˜æ–¹å·®æ¨¡å‹éå¸¸å…³æ³¨è®­ç»ƒæ•°æ®ï¼Œè€Œå¯¹ä»¥å‰æ²¡æœ‰è§è¿‡çš„æ•°æ®ä¸è¿›è¡Œæ³›åŒ–generalizabilityã€‚å› æ­¤ï¼Œè¿™æ ·çš„æ¨¡å‹åœ¨è®­ç»ƒæ•°æ®ä¸Šè¡¨ç°å¾—å¾ˆå¥½ï¼Œä½†åœ¨æµ‹è¯•æ•°æ®ä¸Šå´æœ‰å¾ˆé«˜çš„é”™è¯¯ç‡ã€‚
    - è¿‡æ‹Ÿåˆçš„åŸå› åœ¨äºï¼š
      - **å‚æ•°å¤ªå¤š**ï¼Œæ¨¡å‹å¤æ‚åº¦è¿‡é«˜ï¼›

      - å»ºæ¨¡**æ ·æœ¬é€‰å–æœ‰è¯¯**ï¼Œå¯¼è‡´é€‰å–çš„æ ·æœ¬æ•°æ®ä¸è¶³ä»¥ä»£è¡¨é¢„å®šçš„åˆ†ç±»è§„åˆ™ï¼›
      
      - **æ ·æœ¬å™ªéŸ³å¹²æ‰°è¿‡å¤§**ï¼Œä½¿å¾—æœºå™¨å°†éƒ¨åˆ†å™ªéŸ³è®¤ä¸ºæ˜¯ç‰¹å¾ä»è€Œæ‰°ä¹±äº†é¢„è®¾çš„åˆ†ç±»è§„åˆ™ï¼›
      
      - å‡è®¾çš„**æ¨¡å‹æ— æ³•åˆç†å­˜åœ¨**ï¼Œæˆ–è€…è¯´æ˜¯å‡è®¾æˆç«‹çš„æ¡ä»¶å®é™…å¹¶ä¸æˆç«‹ã€‚
      - 
    - æ€ä¹ˆè§£å†³è¿‡æ‹Ÿåˆï¼ˆé‡ç‚¹ï¼‰ğŸŒŸğŸŒŸğŸŒŸ
      - è·å–å’Œä½¿ç”¨**æ›´å¤šçš„æ•°æ®**ï¼ˆæ•°æ®é›†å¢å¼ºï¼‰â€”â€”è§£å†³è¿‡æ‹Ÿåˆçš„æ ¹æœ¬æ€§æ–¹æ³•
    
      - **ç‰¹å¾é™ç»´**:äººå·¥é€‰æ‹©ä¿ç•™ç‰¹å¾çš„æ–¹æ³•å¯¹ç‰¹å¾è¿›è¡Œé™ç»´
      - åŠ å…¥æ­£åˆ™åŒ–ï¼Œæ§åˆ¶æ¨¡å‹çš„å¤æ‚åº¦
        - ä¸ºä»€ä¹ˆå‚æ•°è¶Šå°ä»£è¡¨æ¨¡å‹è¶Šç®€å•ï¼Ÿ
          - å› ä¸ºå‚æ•°çš„ç¨€ç–ï¼Œåœ¨ä¸€å®šç¨‹åº¦ä¸Šå®ç°äº†ç‰¹å¾çš„é€‰æ‹©ã€‚
          - è¶Šå¤æ‚çš„æ¨¡å‹ï¼Œè¶Šæ˜¯ä¼šå°è¯•å¯¹æ‰€æœ‰çš„æ ·æœ¬è¿›è¡Œæ‹Ÿåˆï¼Œç”šè‡³åŒ…æ‹¬ä¸€äº›å¼‚å¸¸æ ·æœ¬ç‚¹ï¼Œè¿™å°±å®¹æ˜“é€ æˆåœ¨è¾ƒå°çš„åŒºé—´é‡Œé¢„æµ‹å€¼äº§ç”Ÿè¾ƒå¤§çš„æ³¢åŠ¨ï¼Œè¿™ç§è¾ƒå¤§çš„æ³¢åŠ¨ä¹Ÿåæ˜ äº†åœ¨è¿™ä¸ªåŒºé—´é‡Œçš„å¯¼æ•°å¾ˆå¤§ï¼Œè€Œ**åªæœ‰è¾ƒå¤§çš„å‚æ•°å€¼æ‰èƒ½äº§ç”Ÿè¾ƒå¤§çš„å¯¼æ•°ã€‚å› æ­¤å¤æ‚çš„æ¨¡å‹ï¼Œå…¶å‚æ•°å€¼ä¼šæ¯”è¾ƒå¤§**ã€‚ å› æ­¤å‚æ•°è¶Šå°‘ä»£è¡¨æ¨¡å‹è¶Šç®€å•
      - **Dropout**
      - Early stopping
      - äº¤å‰éªŒè¯
      - å¢åŠ å™ªå£°


å‚æ•°å’Œè¶…å‚æ•°çš„åŒºåˆ«ï¼šparameteræ˜¯learn from dataçš„ hyperparameteræ˜¯ä½ å®šçš„

- ä¹Ÿå¯ä»¥è®©æ•°æ®å‡ºhyperparameterï¼Œä½†è¿™æ ·çš„è¯å°±optimization problemä¼šå˜å¾—å¤æ‚ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªç®€å•çš„å¯ä»¥solveçš„convex optimizationï¼Œæ‰€ä»¥æˆ‘ä»¬ä¼šfixå®ƒ

è¶…å‚æ•°æœç´¢æ–¹æ³•

<img src="../images/(null)-20220724221446494.(null)" alt="img" style="width:33%;" />

- Grid: there are really three distinct values of one parameter and three distinct values of another parameter, so there are only 3 x 3, 6 different values tried.
- Random: probably there are nine different values, and in this case there are nine different values. when you actually doing random search, you're actually **trying different values more than the search itself, which has a finite set of values**

  - å¥½å¤„ï¼š**å¯¹dominatingçš„parameterå¯ä»¥å°è¯•æ›´å¤šçš„å€¼**

- Bayesian optimizationï¼šgiven search, figure out the best next point to search 

  - Bayesian optimization works by constructing a probability distribution of possible functions (gaussian process) that best describe the function you want to optimize.

    - Gaussian processæŠŠæ‰€æœ‰æœç´¢è¿‡çš„ç‚¹æ‹Ÿåˆæˆä¸€ä¸ªå‡½æ•°

  - A utility function helps explore the parameter space by trading between exploration and exploitation.

  - The probability distribution of functions is *updated (bayesian) based on observations so far.*

  - åŒºåˆ«ï¼šä¸æ˜¯pre determinedçš„ï¼Œç„¶ågrid å’Œrandomä¸éœ€è¦ä½¿ç”¨æˆ‘ä»¬è¾“å…¥çš„ç»“æœ

- ä¸¤è€…ç»“åˆâ€”â€”Evolutionary optimization

- è¶…å‚é€‰æ‹©æ–¹æ³•ï¼ˆmodel selectionï¼‰ï¼šå¯¹æ¯ä¸ªè¶…å‚strategyï¼Œæˆ‘ä»¬éœ€è¦çŸ¥é“è¿™ä¸ªè¶…å‚æ•°è¡¨ç°æ€ä¹ˆæ ·

  - å¦‚æœä½¿ç”¨test dataçš„è¯ï¼Œä¼šå¯¼è‡´overfittingï¼Œæ‰€ä»¥æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªvalidation datasetæ¥è¡¡é‡effectiveness of a hyperparameter valueï¼Œä»è€Œå®ç°model selectionï¼

  - Three-way holdoutï¼šè·Ÿä¹‹å‰çš„split testsetçš„æ–¹æ³•ä¸€æ ·do another splitï¼Œå¯ä»¥random stratifiedä¹‹ç±»çš„

    - æ•ˆæœï¼šgive reasonable approximation of test performance on large balanced datasets 

  - K-fold cross validation (CV)ï¼š æ•°æ®åˆ†æˆkä»½ï¼Œæ‰§è¡Œkæ¬¡ï¼ˆk-1ä»½å½“æ¨¡å‹ å‰©ä¸€ä»½è¯„ä¼°ï¼‰â‡’å¹³å‡è¡¨ç°

<img src="../images/(null)-20220724221503787.(null)" alt="img" style="width:50%;" />

- Leave-one-out CVï¼šk = nï¼Œ**æ‰€æœ‰çš„æ ·æœ¬éƒ½å•ç‹¬è¢«æ‹¿èµ°ä¸€æ¬¡**
  - High variance é€‚ç”¨äºå°æ•°æ®ï¼

- Repeated stratiï¬ed K-fold CVï¼šK-foldçš„åŸºç¡€ä¸Š æ¯æ¬¡development data is *shuffled* before creating the training & validation datasets

- Stratiï¬ed K-fold CV

  <img src="../images/(null)-20220724221510330.(null)" alt="img" style="width:33%;" />

  -  Stratiï¬ed sampling is used when working with highly imbalanced datasets ï¼


-  Random permutation CVï¼šgenerate a user defined number of independent train / test dataset splits. Samples are first shuffled and then split into a pair of train and test sets.

  - æ‰€ä»¥ä¼šæ˜¯ä¹±çš„ï¼

**Optimal model training, model evaluation**ï¼šDevelopment data = Training data + validation data â‡’ Model to evaluate

- The purpose of test dataset is to evaluate the **performance of the ï¬nal optimal model**
- Model evaluation is supposed to give a pulse on how the model would perform in the wild.  (æµ‹è¯•é›†çš„è¡¨ç°æ˜¯ä¸ºäº†è¡¡é‡åœ¨unseen data çš„è¡¨ç°ï¼æµ‹è¯•é›†ç›¸å½“äºæ˜¯ä¸€ä¸ªproxy)
  - è¿™å°±æ˜¯ä¸ºä»€ä¹ˆåœ¨training processæˆ‘ä»¬å®Œå…¨ä¸touch test set

<img src="../images/(null)-20220724221510063.(null)" alt="img" style="width:33%;" /><img src="../images/(null)-20220724221509687.(null)" alt="img" style="width:33%;" />

**`Model deployment Dataset`** ï¼šTraining data + validation data + Test data â‡’ Deployed model

<img src="../images/(null)-20220724222627619.(null)" alt="img" style="width:33%;" />



## KNN

> https://blog.csdn.net/sinat_30353259/article/details/80901746

A simple **non-parametric** supervised learning methodï¼š Assigns the value of the nearest neighbor(s) to the unseen data point

- Prediction is computationally expensive, while training is trivial
- Generally performs poorly at high dimensions

<img src="../images/(null)-20220724222726246.(null)" alt="img" style="width:25%;" />

è®¡ç®—è¿™ä¸ªç‚¹è·Ÿæ‰€æœ‰ç‚¹çš„è·ç¦»

- K = 1çš„æ—¶å€™ ï¼Œç”¨ç¦»ä»–æœ€è¿‘çš„ä¸€ä¸ªçš„labelæ¥é¢„æµ‹



## çº¿æ€§å›å½’ç±»

### Simple Linear Regression

Assumptions

- Linearity: Y are linearly dependent on each X variable. ( a linear (technically affine) function of x)
- Independence: Observations are independent to each other. the x's are independently drawn, and not dependent on each other.
  - åä¾‹ï¼šç”¨2å¤©å‰è‚¡ä»·ã€3å¤©å‰è‚¡ä»·é¢„æµ‹ä»Šå¤©çš„
- Homo**scedas**ticityï¼šthe Ïµ's, and thus the y's, have constant variance.â€”â€”æ®‹å·® distributed arround 0
- Normalityï¼šæ®‹å·®æ­£æ€the Ïµ's are drawn from a Normal distribution (i.e. Normally-distributed errors)

å…¬å¼

<img src="../images/(null)-20220724222758272.(null)" alt="img" style="width:33%;" />

é—®é¢˜ï¼šæœ‰highly-correlated variablesçš„æ—¶å€™ï¼Œcoefficientå¯èƒ½ä¼šflip

æ”¹è¿›

- outlierå¾ˆå¤šçš„æ—¶å€™å¯ä»¥è€ƒè™‘log transformation on Y



### Ridge Regression

<img src="../images/(null)-20220724222758758.(null)" alt="img" style="width:33%;" />

- æ³¨æ„æ˜¯L2æ­£åˆ™åŒ–$$\operatorname{Min}_{w} \sum_{i=1}^{m}\left(\hat{y}_{i}-y_{i}\right)^{2}+\alpha\|w\|_{2}^{2} $$
- è§£å‡ºæ¥æ˜¯åœ¨é‡Œé¢å¤šäº†ä¸€ä¸ª$$\alpha I$$ï¼š$$\boldsymbol{w}=(X^{T} X+\alpha I )^{-1} X^{T} y$$
  - Î±è¶Šå¤§ï¼Œä¼šè¶Špush $$\boldsymbol{w}$$ to 0

### Lasso Regression

- å…¬å¼ï¼š

<img src="../images/(null)-20220724222758325.(null)" alt="img" style="width:33%;" />

- è·ŸRidgeçš„åŒºåˆ«ï¼š

  <img src="../images/(null)-20220724222758875.(null)" alt="img" style="width:33%;" />

  - Firstly hit one of the corner 4 points and 1 coefficient become 0 in lasso!
    - nç»´åº¦çš„æ—¶å€™å°±ä¼šhit one of the colomn!

  - Ridgeçš„æ—¶å€™æœ‰å¯èƒ½æ˜¯0ä½†å¤§éƒ¨åˆ†ä¸æ˜¯ï¼šhttps://stats.stackexchange.com/questions/176599/why-will-ridge-regression-not-shrink-some-coefficients-to-zero-like-lasso



### Elastic-Net regression

<img src="../images/(null)-20220724222759130.(null)" alt="img" style="width:33%;" />

- Î±ï¼šon whole regularization constrain
- Î»ï¼šcombination weight

<img src="../images/(null)-20220724222758490.(null)" alt="img" style="width:33%;" />

### L1å’ŒL2çš„åŒºåˆ«

L1ï¼ˆLassoï¼‰æ¯”L2ï¼ˆRidgeï¼‰æ›´å®¹æ˜“è·å¾—ç¨€ç–è§£ï¼ŒL2æ¯”L1æ›´å®¹æ˜“è·å¾—smoothè§£

![img](../images/(null)-20220724222758563.(null))

![img](../images/(null)-20220724222758573.(null))

![img](../images/(null)-20220724222758809.(null))



Logistic Regression

 

## Logistic Regression

#### æ¨¡å‹Setup

**Loss Function**

<img src="../images/(null)-20220724223457758.(null)" alt="img" style="width:33%;" />

- **Hinge losså’Œlog lossç›¸æ¯”0-1 losså¯ä»¥æ±‚å¯¼ï¼Œè¿™ä¸¤ä¸ªéƒ½æ˜¯upper bond of the loss functionï¼ˆé»‘è‰²é‚£ä¸ªï¼‰**

- å‡è®¾ y = 1

  - `0-1 loss`ï¼šéœ€è¦ w.T @ x + b >0 æ‰ä¼šæ˜¯0**
  - `Hinge loss`ï¼šå¦‚æœä½ é¢„æµ‹-4 æ­£ç¡®ä¸º1ï¼Œé‚£å°±æ˜¯1-(1 * (-4)) = 5ï½œå¦‚æœé¢„æµ‹4æ­£ç¡®ä¸º1ï¼Œlossæ˜¯1-1Ã—4 = -3 ç„¶åå†max(0, -3) = 0
    - **æ€»ä¹‹æ­£ç¡®çš„æ—¶å€™é¢„æµ‹æ¦‚ç‡è¶Šæ¥è¿‘1ï¼Œlossè¶Šæ¥è¿‘0ï¼Œç„¶åè¶Šç¦»è°±çš„è¯ç»™çš„Hingeå°±ä¼šç»™è¶Šé«˜çš„lossï¼Œè€Œä¸æ˜¯0-1é‚£æ ·fixedä½**
  - `Logistic Loss`: è§ä¸‹æ–¹

å…·ä½“å…¬å¼ï¼š

<img src="../images/(null)-20220724223457931.(null)" alt="img" style="width:33%;" />

æ¨å¯¼è¿‡ç¨‹ï¼š

- é¦–å…ˆè¿™æ˜¯ä¸€ç§å¹¿ä¹‰çº¿æ€§æ¨¡å‹ï¼šæŠŠy=1çš„æ¦‚ç‡pç”¨$$log(\frac{p}{1-p})$$çš„è”ç³»å‡½æ•°è·Ÿç³»ç»Ÿéƒ¨åˆ†$$w^Tx + b$$ç»™è”ç³»åœ¨äº†ä¸€èµ·ï¼Œè¿™ä¸ªfunctionä¼šæ¨å¯¼å‡ºsigmoidï¼š

<img src="../images/(null)-20220724223457515.(null)" alt="img" style="width:33%;" />

- æ¥ç€æˆ‘ä»¬ç”¨æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ¥optimizeæƒ³è¦æ±‚ï¼š$$p(y=1 \mid x)=\frac{1}{1+\exp \left(-\left(w^{T} x+b\right)\right)}$$
- **ç„¶åå°±å¾—åˆ°äº†Log Likelihood**

<img src="../images/(null)-20220724223457224.(null)" alt="img" style="width:33%;" />

- **äºæ˜¯æˆ‘ä»¬æŠŠæŸå¤±å‡½æ•°è®¾å®šæˆMin(-LL)**

<img src="../images/(null)-20220724223457293.(null)" alt="img" style="width:33%;" />

- å†åŠ å…¥æ­£åˆ™é¡¹å°±å¾—åˆ°äº†ä¸€ç³»åˆ—ï¼š**Loss function for regularized logistics regressionï¼š**

  <img src="../images/(null)-20220724223457754.(null)" alt="img" style="width:33%;" />

  - SK learné‡Œé¢æ˜¯Cï¼Œé«˜çš„$$C = \frac{1}{\alpha}$$ç›¸å½“äºæ²¡æœ‰å¯¹å‚æ•°é™åˆ¶è§æ•ˆ

#### **å¤šåˆ†ç±»é—®é¢˜å¸¸è§„è§£å†³æ–¹æ¡ˆ**

**å åŠ binary**

- **OVR (One vs Rest)**

<img src="../images/(null)-20220724223459288.(null)" alt="img" style="width:33%;" />

- **OVO (One vs One)**

<img src="../images/(null)-20220724223458436.(null)" alt="img" style="width:33%;" />

- å»ºå¥½å‡ ä¸ªbinary classificationï¼Œå¦‚æœå¤§å¤šæ•°modelè¯´ä½ æ˜¯class Xå°±æ˜¯é‚£ä¸ª

- å¯¹æ¯”

  <img src="../images/(null)-20220724223458032.(null)" alt="img" style="width:50%;" />

  - æ¯”å¦‚ä¸‰æ¡çº¿ä¸­é—´çš„åœ°æ–¹ï¼Œä¸€äººè¯´ä½ æ˜¯ä¸€ä¸ªclassï¼Œæ˜¯uncertainty


#### **Logisticsè§£å†³å¤šåˆ†ç±»é—®é¢˜**

ç›´æ¥extend

<img src="../images/(null)-20220724223459022.(null)" alt="img" style="width:50%;" />

- å› æ­¤å¯èƒ½ä¼šæ¯”ç”¨ovoå’Œovrçš„SVMè¦å¥½ï¼å› ä¸ºä»–ç›´æ¥globally optimize all the log odds of ratio. It actually solves it as a multiclass classification problem!

## SVM 

- Hard/soft maginå«ä¹‰ï¼š

<img src="../images/(null)-20220724224339951.(null)" alt="img" style="width:50%;" />



#### Primal + Hard-margin

<img src="../images/(null)-20220724224337505.(null)" alt="img" style="width:50%;" />

- **Objective functionçš„å«ä¹‰ï¼š**optimize åˆ’åˆ†è¶…å¹³é¢çš„ maximum margin ï¼ˆæœ€å¤§é—´éš”ï¼‰=$$\frac{2}{\|w\|_{2}^{2}}$$ï¼Œä¹Ÿå°±æ˜¯minimize$$\frac{\|w\|_{2}^{2}}{2}$$

- é™åˆ¶æ¡ä»¶çš„å«ä¹‰ï¼š

  - ç‚¹(xi, yi)åˆ°ç›´çº¿$$y=w^{T} x+b$$çš„è·ç¦»$$\frac{ |w^{T} x_{i}+b-yi|}{{\|w\|_{2}^{2}}} \geq 1$$

    $$\Rightarrow { |w^{T} x_{i}+b-yi|} \geq {{\|w\|_{2}^{2}}}$$

    $$\Rightarrow w^{T} x_{i}+b \geq y_i \text{ or } w^{T} x_{i}+b \leq -y_i$$

    $$\Rightarrow \begin{cases}\omega^{T} x_{i}+b \geq+1, & y_{i}=+1 \\ \omega^{T} x_{i}+b \leq-1, & y_{i}=-1\end{cases}$$

    $$\Rightarrow y_{i}\left(w^{T} x_{i}+b\right) \geq 1$$

    - ä»–ä¹‹ä¸Šé‚£ä¹ˆè¦>1ï¼Œå¦‚æœåœ¨å®ƒä¹‹ä¸‹è¦>-1ï¼Œä¹Ÿå°±æ˜¯*y1 > 1

#### Primal + Dual

<img src="../images/(null)-20220724224339721.(null)" alt="img" style="width:50%;" />

- **Linkï¼š**é€šè¿‡å¯¹wå’Œbæ±‚å¯¼ æŠŠç»“æœä»£å›loss functionå°±å¾—åˆ°äº†å¯¹å¶é—®é¢˜

- ç‰¹ç‚¹ï¼šä¹‹å‰éœ€è¦è§£wå’Œbã€ç°åœ¨åªéœ€è¦è§£Î±

  - Î± ä¸€ä¸ªmç»´å‘é‡ï¼ˆmæ˜¯æ ·æœ¬çš„sizeï¼Œnæ˜¯å˜é‡ä¸ªæ•°ï¼‰
  - ä¹‹å‰è§£çš„æ˜¯**ä¸€ä¸ªn_featureç»´é—®é¢˜ï¼ç°åœ¨å˜æˆn_sampleç»´é—®é¢˜**
    - å¦‚æœå°‘é‡featureçš„æ—¶å€™ Primalé—®é¢˜è§£çš„æ›´å¿«ï¼ˆå¤§å¤šæ•°æƒ…å†µï¼‰
    - å¦‚æœæœ‰å¤§é‡featureçš„è¯ï¼ŒDualè§£çš„æ›´å¿«

- ç›®çš„

  - dualå¯ä»¥æ›´å®¹æ˜“ç§»åŠ¨åˆ°non- linearçš„åœºæ™¯é‡Œ

- æ•ˆæœï¼šè¿™ä¸ªç­‰å¼ä¼šä¸€ç›´æ˜¯0 $\alpha_{i}\left(1-y_{i}\left(w^{T} x_{i}+b\right)\right)=0$

  - å¯¹äºéæ”¯æŒå‘é‡ï¼šÎ±i=0

  - åªæœ‰è§£å‡ºæ¥çš„æ”¯æŒå‘é‡æ»¡è¶³ï¼šÎ±i != 0,$$y_{i}\left(w^{T} x_{i}+b\right) = 1$$

#### Primal + Soft- margin

- é¦–å…ˆÎ¾ introduce lossï¼Œç„¶åå†é€šè¿‡æ•°å­¦å˜æ¢æŠŠÎ¾è§£å‡ºæ¥ï¼Œç„¶åå¾—åˆ°çš„å°±æ˜¯Hinge loss

<img src="../images/(null)-20220724224337896.(null)" alt="img" style="width:50%;" /><img src="../images/(null)-20220724224336553.(null)" alt="img" style="width:50%;" />

- Cçš„ä½œç”¨ï¼šæ§åˆ¶errorçš„é‡è¦æ€§â€”â€”Cè¶Šå¤§ï¼Œmarginè¶Šä¸é‡è¦å°±è¶Šå°ï½œ**Cè¶Šå°ï¼Œmarginè¶Šé‡è¦å°±è¶Šå¤§**

<img src="../images/(null)-20220724224337202.(null)" alt="img" style="width:50%;" />

#### Dual + Soft-margin

<img src="../images/(null)-20220724224337040.(null)" alt="img" style="width:50%;" />

- ä¹Ÿå¯ä»¥åŠ å…¥æ­£åˆ™åŒ–ï¼š

<img src="../images/(null)-20220724224337444.(null)" alt="img" style="width:50%;" />

- é¢„æµ‹çš„æ–¹æ³•ï¼š$$\operatorname{sign}\left(\sum_{i} \alpha_{i} y_{i}\left(x \cdot x_{i}\right)+b\right)$$
  - $$\alpha_i$$ï¼šç¬¬iä¸ªsupport vectorçš„dual coefficient
  - $$x_i, y_i$$ï¼šç¬¬iä¸ªsupport vectorçš„åæ ‡

#### Kernel Function

**Kernel function ğ•‚(ğ’™;ğ’™)** å¯ä»¥ **estimates inner product between two points in the projected space.**

- å‡è®¾æœ‰ä¸€ä¸ªmagic function ğ“(ğ’™)ï¼Œå¯ä»¥ projects data to **high-dimensional space**

  è¿˜æ˜¯ç»™ä¸¤ä¸ªç‚¹è¿”å›è·ç¦»ï¼ä½†æ˜¯è¿™ä¸ªè·ç¦»æ˜¯åœ¨é«˜ç»´ç©ºé—´ä¸Šçš„ æ‰€ä»¥å¯èƒ½åœ¨è¿™ä¸ªç»´ä¸Šçœ‹åˆ°çš„ä¸ä¸€æ ·

<img src="../images/(null)-20220724224338320.(null)" alt="img" style="width:50%;" />

- ä½†æˆ‘ä»¬å…³å¿ƒçš„æ˜¯ä¸¤ä¸ªğ“(ğ’™i) Ã— ğ“(ğ’™j) çš„ç»“æœï¼Œè€Œä¸æ˜¯ğ“æœ¬èº«ï¼Œæ‰€ä»¥å¯ä»¥ä½¿ç”¨ä¸€ä¸ªKernel trickï¼Œå‡è®¾**ğ•‚**æ˜¯ç›´æ¥ä½œç”¨åœ¨dot productä¸Šé¢çš„ï¼Œä»è€Œåªéœ€è¦assume ğ“çš„å­˜åœ¨å°±å¯ä»¥äº†

<img src="../images/(null)-20220724224337915.(null)" alt="img" style="width:50%;" />

- å› æ­¤è¿™ä¸ªé—®é¢˜å°±å˜æˆäº†ï¼š

<img src="../images/(null)-20220724224338928.(null)" alt="img" style="width:50%;" />

- ç±»å‹ï¼š

<img src="../images/(null)-20220724224338479.(null)" alt="img" style="width:50%;" />

- Linear Kernelï¼šæ²¡åšå•¥ï¼
- Polynomialï¼š
  - æ¯”å¦‚ä¸€ä¸ªä¸€ç»´æ˜ å°„åˆ°ä¸‰ç»´çš„å‡½æ•°$$\phi\left(x\right)=\left(x, \sqrt{2} x, 1\right)$$ï¼Œä»–çš„æ•ˆæœæ˜¯$$\phi\left(x_{i}\right) \cdot \phi\left(x_{j}\right)=\left(x_{i}, \sqrt{2} x_{i}, 1\right) \cdot \left(x_{j}, \sqrt{2} x_{j}, 1\right) = \left(x_{i} x_{j}+1\right)^{2}$$
  - ä»–å…¶å®ç­‰ä»·äºä¸€ä¸ªPolynomialçš„Kernel Function$$\mathbb{K}\left(x_{i}, x_{j}\right)=\left(x_{i} x_{j}+1\right)^{2}$$å°±å¯ä»¥ç»™å‡ºé«˜ç»´ç©ºé—´Ï†ä¸‹çš„ç‚¹ä¹˜ç»“æœäº†
  - è®¡ç®—å¤æ‚åº¦ä¹Ÿä½å¤šäº†ï¼

<img src="../images/(null)-20220724224338277.(null)" alt="img" style="width:50%;" />

- RBF(Radial Basis Function) Kernelï¼šå¦‚æœå……åˆ†tune Î³ enoughï¼ŒèƒŒåå®é™…ä¸Šæ˜¯ä¸€ä¸ªinfinite diminutionalçš„spaceï¼Œä»è€Œalwayså¯ä»¥separate dataï¼å¯ä»¥perfectly separate data.

  RBDä¸»è¦ç”¨äºçº¿æ€§ä¸å¯åˆ†çš„æƒ…å½¢

  - ä¸¤ä¸ªç‚¹åœ¨é«˜ç»´ç©ºé—´è¶Šæ¥è¿‘ï¼Œå°±ä¼šæœ‰è¶Šé«˜çš„valuesï½œSpread out çš„ç‚¹ä¼šsmaller
    - `Î³`ä¼šæ§åˆ¶ how the function value decays with distanceï¼ˆÎ³è¶Šå¤§çš„è¯ï¼Œè·ç¦»è¶Šè¿œçš„sample å€¼ä¸‹é™å¾—è¶Šå¿«ï¼‰ 
    - e.g

<img src="../images/(null)-20220724224338727.(null)" alt="img" style="width:50%;" />

**Kernelé€‰æ‹©çš„æ€è·¯ï¼š**

ï¼ˆ1ï¼‰å¦‚æœç‰¹å¾ç»´æ•°å¾ˆé«˜ï¼Œå¾€å¾€çº¿æ€§å¯åˆ†ï¼ˆSVMè§£å†³éçº¿æ€§åˆ†ç±»é—®é¢˜çš„æ€è·¯å°±æ˜¯å°†æ ·æœ¬æ˜ å°„åˆ°æ›´é«˜ç»´çš„ç‰¹å¾ç©ºé—´ä¸­ï¼‰ï¼Œå¯ä»¥é‡‡ç”¨LRæˆ–è€…çº¿æ€§æ ¸çš„SVMï¼›

ï¼ˆ2ï¼‰å¦‚æœæ ·æœ¬æ•°é‡å¾ˆå¤šï¼Œç”±äºæ±‚è§£æœ€ä¼˜åŒ–é—®é¢˜çš„æ—¶å€™ï¼Œç›®æ ‡å‡½æ•°æ¶‰åŠä¸¤ä¸¤æ ·æœ¬è®¡ç®—å†…ç§¯ï¼Œä½¿ç”¨é«˜æ–¯æ ¸æ˜æ˜¾è®¡ç®—é‡ä¼šå¤§äºçº¿æ€§æ ¸ï¼Œæ‰€ä»¥æ‰‹åŠ¨æ·»åŠ ä¸€äº›ç‰¹å¾ï¼Œä½¿å¾—çº¿æ€§å¯åˆ†ï¼Œç„¶åå¯ä»¥ç”¨LRæˆ–è€…çº¿æ€§æ ¸çš„SVMï¼›

ï¼ˆ3ï¼‰å¦‚æœä¸æ»¡è¶³ä¸Šè¿°ä¸¤ç‚¹ï¼Œå³**ç‰¹å¾ç»´æ•°å°‘ï¼Œæ ·æœ¬æ•°é‡æ­£å¸¸**ï¼Œå¯ä»¥ä½¿ç”¨é«˜æ–¯æ ¸çš„SVMã€‚



## Ensemble Methods

Baggingå’ŒBoostingéƒ½æ˜¯ensembleï¼Œå°±æ˜¯æŠŠå¼±åˆ†ç±»å™¨ç»„è£…æˆå¼ºåˆ†ç±»å™¨çš„æ–¹æ³•

**Motivation**

- The decision trees are highly **unstable** and can structurally change with slight variation in input data
- Decision trees perform poorly on continuous outcomes (regression) due to limited model capacity.

**å®šä¹‰**

- Several weak/simple learners are **combined** to make the ï¬nal prediction
- ç›®çš„ï¼šGenerally ensemble methods aim to **reduce model variance**
  - Like have multiple such outputs and then you take an average of that.
- æ•ˆæœï¼šEnsemble methods improve performance especially if the individual learners are not correlated.
  - **took a different or a different perspective** of the data itself.
  - é‡‡æ ·ä¼šâ‡’æˆåŠŸ
    - if you had one or two highly **dominant features** that probably is saying is highly correlated to your outcome. 
      - Suppose every tree that a building has access to that feature. Probably every tree is going to look very similar right now.
      - Assume that you actually had some trees **not have access to that feature. Then they'll start looking at the data from a different perspective and they'll probably build trees that are giving you another notion of your data center. And it's not dominated by these one or two features that are are highly correlated to the outcome.**
- ç±»å‹ï¼šDepending on training sample construction and output aggregation, there are two categories:
  - Bagging (Bootstrap aggregation)
  - Boosting

#### Bagging

Baggingçš„ä¸»è¦ç›®çš„æ˜¯å‡å°‘æ–¹å·®

- å¤šæ¬¡é‡‡æ ·ï¼Œè®­ç»ƒå¤šä¸ªåˆ†ç±»å™¨Several training samples (of same size) are created by sampling the dataset **with replacement**

  - ä»åŸå§‹æ ·æœ¬é›†ä¸­æŠ½å–è®­ç»ƒé›†ã€‚æ¯è½®ä»åŸå§‹æ ·æœ¬é›†ä¸­ä½¿ç”¨Bootstrapingçš„æ–¹æ³•æŠ½å–nä¸ªè®­ç»ƒæ ·æœ¬ï¼ˆåœ¨è®­ç»ƒé›†ä¸­ï¼Œæœ‰äº›æ ·æœ¬å¯èƒ½è¢«å¤šæ¬¡æŠ½å–åˆ°ï¼Œè€Œæœ‰äº›æ ·æœ¬å¯èƒ½ä¸€æ¬¡éƒ½æ²¡æœ‰è¢«æŠ½ä¸­ï¼‰ã€‚å…±è¿›è¡Œkè½®æŠ½å–ï¼Œå¾—åˆ°kä¸ªè®­ç»ƒé›†ã€‚ï¼ˆkä¸ªè®­ç»ƒé›†ä¹‹é—´æ˜¯ç›¸äº’ç‹¬ç«‹çš„ï¼‰

    there could be samples that are **repeated** and there are samples that **do not get picked at all.**

    <img src="../images/(null)-20220725100730752.(null)" alt="img" style="width: 50%;" />

  - æ¯æ¬¡ä½¿ç”¨ä¸€ä¸ªè®­ç»ƒé›†å¾—åˆ°ä¸€ä¸ªæ¨¡å‹ï¼Œkä¸ªè®­ç»ƒé›†å…±å¾—åˆ°kä¸ªæ¨¡å‹ã€‚ï¼ˆæ³¨ï¼šè¿™é‡Œå¹¶æ²¡æœ‰å…·ä½“çš„åˆ†ç±»ç®—æ³•æˆ–å›å½’æ–¹æ³•ï¼Œæˆ‘ä»¬å¯ä»¥æ ¹æ®å…·ä½“é—®é¢˜é‡‡ç”¨ä¸åŒçš„åˆ†ç±»æˆ–å›å½’æ–¹æ³•ï¼Œå¦‚å†³ç­–æ ‘ã€æ„ŸçŸ¥å™¨ç­‰ï¼‰

- åˆ†ç±»é—®é¢˜ï¼šå¯¹åˆ†ç±»é—®é¢˜ï¼šå°†ä¸Šæ­¥å¾—åˆ°çš„kä¸ªæ¨¡å‹é‡‡ç”¨æŠ•ç¥¨çš„æ–¹å¼å¾—åˆ°åˆ†ç±»ç»“æœ

  å¯¹å›å½’é—®é¢˜ï¼Œè®¡ç®—ä¸Šè¿°æ¨¡å‹çš„å‡å€¼ä½œä¸ºæœ€åçš„ç»“æœã€‚ï¼ˆæ‰€æœ‰æ¨¡å‹çš„é‡è¦æ€§ç›¸åŒï¼‰ 

#### Boosting

Boostingçš„ç›®çš„ä¸»è¦æ˜¯å‡å°‘åå·®

- Includes a family of ML algorithms that convert weak learners to strong ones.

  - The weak learners are learned sequentially with early learners ï¬tting simple models to the data and then analysing data for errors.

  - When an input is misclassiï¬ed by one tree, its output is adjusted so that next tree is more likely to learn it correctly.

     æ¯ä¸ªtreeçš„ç›®æ ‡æ˜¯do better on the misclassify samples of the previous.

    - ä¹‹å‰åˆ†å¯¹çš„down weight
    - ä¹‹å‰åˆ†é”™çš„out weight

<img src="../images/(null)-20220725100731093.(null)" alt="img" style="width:50%;" />

- æœ€å combined by a weighted average of each of those trees. ä½†è¿™ä¸ªæƒé‡è·Ÿæ¨¡å‹æœ‰å…³ï¼

> Random Forestæ˜¯ç›´æ¥averageï¼



- ç®—æ³•æµç¨‹ï¼š

  - ç»™å®šåˆå§‹è®­ç»ƒæ•°æ®ï¼Œç”±æ­¤è®­ç»ƒå‡ºç¬¬ä¸€ä¸ªåŸºå­¦ä¹ å™¨ï¼›

  - æ ¹æ®åŸºå­¦ä¹ å™¨çš„è¡¨ç°å¯¹æ ·æœ¬è¿›è¡Œè°ƒæ•´ï¼Œåœ¨ä¹‹å‰å­¦ä¹ å™¨åšé”™çš„æ ·æœ¬ä¸ŠæŠ•å…¥æ›´å¤šå…³æ³¨ï¼›

  - ç”¨è°ƒæ•´åçš„æ ·æœ¬ï¼Œè®­ç»ƒä¸‹ä¸€ä¸ªåŸºå­¦ä¹ å™¨ï¼›

  - é‡å¤ä¸Šè¿°è¿‡ç¨‹Tæ¬¡ï¼Œå°†Tä¸ªå­¦ä¹ å™¨åŠ æƒç»“åˆã€‚

- é€šè¿‡æé«˜é‚£äº›åœ¨å‰ä¸€è½®è¢«å¼±åˆ†ç±»å™¨åˆ†é”™æ ·ä¾‹çš„æƒå€¼ï¼Œå‡å°å‰ä¸€è½®åˆ†å¯¹æ ·ä¾‹çš„æƒå€¼ï¼Œæ¥ä½¿å¾—åˆ†ç±»å™¨å¯¹è¯¯åˆ†çš„æ•°æ®æœ‰è¾ƒå¥½çš„æ•ˆæœã€‚

- é€šè¿‡ä»€ä¹ˆæ–¹å¼æ¥ç»„åˆå¼±åˆ†ç±»å™¨ï¼Ÿ

  - é€šè¿‡åŠ æ³•æ¨¡å‹å°†å¼±åˆ†ç±»å™¨è¿›è¡Œçº¿æ€§ç»„åˆï¼Œæ¯”å¦‚AdaBoosté€šè¿‡åŠ æƒå¤šæ•°è¡¨å†³çš„æ–¹å¼ï¼Œå³å¢å¤§é”™è¯¯ç‡å°çš„åˆ†ç±»å™¨çš„æƒå€¼ï¼ŒåŒæ—¶å‡å°é”™è¯¯ç‡è¾ƒå¤§çš„åˆ†ç±»å™¨çš„æƒå€¼ã€‚

  - è€Œæå‡æ ‘é€šè¿‡æ‹Ÿåˆæ®‹å·®çš„æ–¹å¼é€æ­¥å‡å°æ®‹å·®ï¼Œå°†æ¯ä¸€æ­¥ç”Ÿæˆçš„æ¨¡å‹å åŠ å¾—åˆ°æœ€ç»ˆæ¨¡å‹ã€‚



#### Baggingå’ŒBoostingçš„åŒºåˆ«

- 1ï¼‰æ ·æœ¬é€‰æ‹©ä¸Šï¼š

  - Baggingï¼šè®­ç»ƒé›†æ˜¯åœ¨åŸå§‹é›†ä¸­æœ‰æ”¾å›é€‰å–çš„ï¼Œä»åŸå§‹é›†ä¸­é€‰å‡ºçš„å„è½®è®­ç»ƒé›†ä¹‹é—´æ˜¯ç‹¬ç«‹çš„ã€‚

  - Boostingï¼šæ¯ä¸€è½®çš„è®­ç»ƒé›†ä¸å˜ï¼Œåªæ˜¯è®­ç»ƒé›†ä¸­æ¯ä¸ªæ ·ä¾‹åœ¨åˆ†ç±»å™¨ä¸­çš„æƒé‡å‘ç”Ÿå˜åŒ–ã€‚è€Œæƒå€¼æ˜¯æ ¹æ®ä¸Šä¸€è½®çš„åˆ†ç±»ç»“æœè¿›è¡Œè°ƒæ•´ã€‚

- 2ï¼‰æ ·ä¾‹æƒé‡ï¼š

  - Baggingï¼šä½¿ç”¨å‡åŒ€å–æ ·ï¼Œæ¯ä¸ªæ ·ä¾‹çš„æƒé‡ç›¸ç­‰

  - Boostingï¼šæ ¹æ®é”™è¯¯ç‡ä¸æ–­è°ƒæ•´æ ·ä¾‹çš„æƒå€¼ï¼Œé”™è¯¯ç‡è¶Šå¤§åˆ™æƒé‡è¶Šå¤§ã€‚

- 3ï¼‰é¢„æµ‹å‡½æ•°ï¼š

  - Baggingï¼šæ‰€æœ‰é¢„æµ‹å‡½æ•°çš„æƒé‡ç›¸ç­‰ã€‚

  - Boostingï¼šæ¯ä¸ªå¼±åˆ†ç±»å™¨éƒ½æœ‰ç›¸åº”çš„æƒé‡ï¼Œå¯¹äºåˆ†ç±»è¯¯å·®å°çš„åˆ†ç±»å™¨ä¼šæœ‰æ›´å¤§çš„æƒé‡ã€‚

- 4ï¼‰å¹¶è¡Œè®¡ç®—ï¼š

  - Baggingï¼šå„ä¸ªé¢„æµ‹å‡½æ•°å¯ä»¥å¹¶è¡Œç”Ÿæˆ

  - Boostingï¼šå„ä¸ªé¢„æµ‹å‡½æ•°åªèƒ½é¡ºåºç”Ÿæˆï¼Œå› ä¸ºåä¸€ä¸ªæ¨¡å‹å‚æ•°éœ€è¦å‰ä¸€è½®æ¨¡å‹çš„ç»“æœã€‚

#### Stacking

- å¤šæ¬¡é‡‡æ ·ï¼Œè®­ç»ƒå¤šä¸ªåˆ†ç±»å™¨ï¼Œå°†è¾“å‡ºä½œä¸ºæœ€åçš„è¾“å…¥ç‰¹å¾

- å°†è®­ç»ƒå¥½çš„æ‰€æœ‰åŸºæ¨¡å‹å¯¹è®­ç»ƒé›†è¿›è¡Œé¢„æµ‹ï¼Œç¬¬ä¸ª$i$åŸºæ¨¡å‹å¯¹ç¬¬$i$ä¸ªè®­ç»ƒæ ·æœ¬çš„é¢„æµ‹å€¼å°†ä½œä¸ºæ–°çš„è®­ç»ƒé›†ä¸­ç¬¬$i$ä¸ªæ ·æœ¬çš„ç¬¬$i$ä¸ªç‰¹å¾å€¼ï¼Œæœ€ååŸºäºæ–°çš„è®­ç»ƒé›†è¿›è¡Œè®­ç»ƒã€‚åŒç†ï¼Œé¢„æµ‹çš„è¿‡ç¨‹ä¹Ÿè¦å…ˆç»è¿‡æ‰€æœ‰åŸºæ¨¡å‹çš„é¢„æµ‹å½¢æˆæ–°çš„æµ‹è¯•é›†ï¼Œæœ€åå†å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹ã€‚

- stackingå¸¸è§çš„ä½¿ç”¨æ–¹å¼ï¼š
  - ç”±k-NNã€éšæœºæ£®æ—å’Œæœ´ç´ è´å¶æ–¯åŸºç¡€åˆ†ç±»å™¨ç»„æˆï¼Œå®ƒçš„é¢„æµ‹ç»“æœç”±ä½œä¸ºå…ƒåˆ†ç±»å™¨çš„é€»å›å½’ç»„åˆã€‚



## Trees

###  Decision Trees

**ç‰¹ç‚¹**

- Greedy algorithm
- Applicable to both classiï¬cation & regression problems
  - Regressionçš„è¯åªèƒ½æ˜¯finiteçš„æ•°å€¼

<img src="../images/(null)-20220724230711129.(null)" alt="img" style="width:67%;" />

- Easy to interpret & deployï¼šå¯ä»¥è®©åˆ«äººhandwrite
- Non-linear decision boundary
  - treeç›¸å½“äºæ˜¯åœ¨é«˜ç»´ç©ºé—´çš„å¥½å‡ ä¸ªç»´åº¦ä¸Šå»splitting up the space
- Minimal preprocessing
  - missing å’Œ categoricalå¯ä»¥handleï¼
- Invariant to scale of data
  - Invariant to the scale of the data because it is it is *not really looking at at absolute values of the of the features*. More on the ranges of the features.

**Framework**

<img src="../images/(null)-20220724230711222.(null)" alt="img" style="width:67%;" />

**Loss**



åˆ†ç±»ä»»åŠ¡ï¼š

- Impurityè¡¡é‡ï¼š$Entropy(node)=-\sum_{i=1}^{K} p_{i} \log _{2} p_{i} $ ï½œ $Gini Index(node)=1-\sum_{i=1}^{K} p_{i}^{2}$$

  - $$p_{i}=\text{probability of beloing to a class} =\frac{\text{number of samples of the class}}{\text{total number of samples in that node}}$$ 

  - ä¾‹å­

    <img src="../images/(null)-20220724230710858.(null)" alt="img" style="width:33%;" /><img src="../images/(null)-20220724230711014.(null)" alt="img" style="width:33%;" /> 

  - ä¸¤è€…åŒºåˆ«ä¸å¤§â€”â€”giniçš„æœ€å¤§å€¼æ˜¯1ï¼Œentropyçš„æœ€å¤§å€¼æ˜¯0.5

- Information Gainï¼ˆè¯„ä¼°ä¿¡æ¯å¢ç›ŠInformation Gainå¯¹ç¡®å®šçš„featureå’Œç¡®å®šçš„split thresholdï¼‰

  [Expected](https://en.wikipedia.org/wiki/Expectation_value) information gain $$IG(T, a)=H(T)-H(T \mid a)$$ is the reduction in [information entropy](https://en.wikipedia.org/wiki/Information_entropy) *Î—* from a prior state to a state that takes some information as given. ä¹Ÿå°±æ˜¯ä¸çº¯åº¦çš„ä¸‹é™

  - $$H(T)$$ï¼ša priori Shannon entropy 
  - $$H(T\mid a) = \sum_{v \in vals(a)} \frac{\left|S_{a}(v)\right|}{|T|}  H\left(S_{a}(v)\right)$$ï¼šæ ·æœ¬å æ¯”ä¸ºæƒçš„åŠ æƒå¹³å‡entropy
    - $$\frac{\left|S_{a}(v)\right|}{|T|} $$: å…¶å®åˆ†åˆ°açš„å æ¯”ï¼ˆæƒé‡ï¼‰ï¼Œæ‹¿è¿™ä¸ªå»åŠ æƒå¹³å‡
      - $$S_{a}(v)=\left\{x \in T \mid x_{a}=v\right\}$$è¡¨ç¤ºTä¸­åˆ†è£‚åˆ°aé‡Œé¢çš„nodeç»„æˆçš„é›†åˆ
    - $$H\left(S_{a}(v)\right)$$ï¼š$$S_{a}(v)$$çš„Entropy
  - e.g

  <img src="../images/(null)-20220724231640012.(null)" alt="img" style="width:33%;" />

  - Numericalçš„ï¼š An exhaustive **search across all features and values** to ï¬nd the (feature, threshold) combination with the highest information gain (IG).

  <img src="../images/(null)-20220724231943423.(null)" alt="img" style="width:50%;" />

  - Categoricalçš„ï¼š

    - An exhaustive **search across all categorical features** and their categories to ï¬nd the(feature, subsets) combination with the highest information gain (IG).

      <img src="../images/(null)-20220724231943637.(null)" alt="img" style="width:50%;" />

    - Use **target encoding** to reduce time complexity by only evaluating O(L)  splits of the ordered categories.

      - é¦–å…ˆï¼šThe categories are ordered by mean response rate 

      - æ¥ç€æŒ‰é¡ºåºä¸€ä¸ªä¸€ä¸ªinclude feature

      - è¿™æ ·æ‰¾å‡ºæ¥çš„ä¼šæ˜¯optimalçš„ï¼

        <img src="../images/(null)-20220724231943028.(null)" alt="img" style="width:50%;" />



#### å†³ç­–æ ‘Overfittingçš„è§£å†³

> ä¸è®­ç»ƒåˆ°å®Œæ•´çš„tree

- `Pruning`ï¼šstart from the bottom and start chopping off parts of the tree that don't make sense.
  - Reduced error
    - Starting at the leaves, each node is replaced with its most popular class (chopping)
    - If the validation metric is not negatively aï¬€ected, then the change is kept, else it is reverted.
    - Reduced error pruning has the advantage of speed and simplicity.
    
  - Cost complexity
    - The node with the least $$\alpha_{eff}=\frac{R(t)-R_{\alpha}\left(T_{t}\right)}{T-1}$$is pruned
    
    - Î±ç›¸å½“äºä¸€ä¸ªæ­£åˆ™é¡¹çš„ç³»æ•°ï¼Œä»è€Œ$$R_{\alpha}(T)=R(T)+\alpha|T|$$
    
      - $$R\left(T_{t}\right)=\sum_{i \in \text { leaf nodes }} R(i)$$, sum of impurities for all leaf nodes t of a tree rooted at node t.
    - Î±è¶Šé«˜ï¼Œæƒ©ç½šè¶Šå¤§ï¼Œä¼šä»overfitting -> sweetspot -> underfitting. éšç€alphaæé«˜ï¼Œæ€»impurityä¼šæœ‰steep change

<img src="../images/(null)-20220724230711183.(null)" alt="img" style="width:40%;" /><img src="../images/(null)-20220724230711425.(null)" alt="img" style="width:40%;" />

- `Early stopping`ï¼šbuild up to a point and then you stop.

  - Maximum depthï¼šonly build it to a certain depth which prevents you from going very deep.
    - æ¯”å¦‚ç”¨DFSï¼Œé‚£å°±æ˜¯å¾€ä¸‹splitç›´åˆ°å˜æˆpure node or reach max_depth
  - Maximum leaf nodesï¼šonly have a certain number of leaf nodes
  - Minimum samples splitï¼šthere are a minimum number of samples before I consider it a split
  - Minimum impurity decrease
  
  

#### Feature Importance

probability of sample reaching that nodeï¼š the (normalized) total reduction of the criterion brought by that feature





### Random Forests

éšæœºä¸€ç§åŸºäºæ ‘æ¨¡å‹çš„Baggingçš„ä¼˜åŒ–ç‰ˆæœ¬ï¼Œä¸€æ£µæ ‘çš„ç”Ÿæˆè‚¯å®šè¿˜æ˜¯ä¸å¦‚å¤šæ£µæ ‘ï¼Œå› æ­¤å°±æœ‰äº†éšæœºæ£®æ—ï¼Œè§£å†³å†³ç­–æ ‘æ³›åŒ–èƒ½åŠ›å¼±çš„ç‰¹ç‚¹ã€‚

- å¤šæ¬¡éšæœºå–æ ·ï¼Œå¤šæ¬¡éšæœºå–å±æ€§ï¼Œé€‰å–æœ€ä¼˜åˆ†å‰²ç‚¹ï¼Œæ„å»ºå¤šä¸ª(CART)åˆ†ç±»å™¨ï¼ŒæŠ•ç¥¨è¡¨å†³

- ç®—æ³•æµç¨‹ï¼š

  - è¾“å…¥ä¸ºæ ·æœ¬é›†$D={(xï¼Œy_1)ï¼Œ(x_2ï¼Œy_2) \dots (x_mï¼Œy_m)}$ï¼Œå¼±åˆ†ç±»å™¨è¿­ä»£æ¬¡æ•°$T$ã€‚

  - è¾“å‡ºä¸ºæœ€ç»ˆçš„å¼ºåˆ†ç±»å™¨$f(x)$

  - å¯¹äº$t=1ï¼Œ2 \dots T$

    - å¯¹è®­ç»ƒé›†è¿›è¡Œç¬¬$t$æ¬¡éšæœºé‡‡æ ·ï¼Œå…±é‡‡é›†$m$æ¬¡ï¼Œå¾—åˆ°åŒ…å«$m$ä¸ªæ ·æœ¬çš„é‡‡æ ·é›†Dt

    - ç”¨é‡‡æ ·é›†$D_t$è®­ç»ƒç¬¬$t$ä¸ªå†³ç­–æ ‘æ¨¡å‹$G_t(x)$ï¼Œåœ¨è®­ç»ƒå†³ç­–æ ‘æ¨¡å‹çš„èŠ‚ç‚¹çš„æ—¶å€™ï¼Œåœ¨èŠ‚ç‚¹ä¸Šæ‰€æœ‰çš„æ ·æœ¬ç‰¹å¾ä¸­é€‰æ‹©ä¸€éƒ¨åˆ†æ ·æœ¬ç‰¹å¾ï¼Œåœ¨è¿™äº›éšæœºé€‰æ‹©çš„éƒ¨åˆ†æ ·æœ¬ç‰¹å¾ä¸­é€‰æ‹©ä¸€ä¸ªæœ€ä¼˜çš„ç‰¹å¾æ¥åšå†³ç­–æ ‘çš„å·¦å³å­æ ‘åˆ’åˆ†

  - å¦‚æœæ˜¯åˆ†ç±»ç®—æ³•é¢„æµ‹ï¼Œåˆ™$T$ä¸ªå¼±å­¦ä¹ å™¨æŠ•å‡ºæœ€å¤šç¥¨æ•°çš„ç±»åˆ«æˆ–è€…ç±»åˆ«ä¹‹ä¸€ä¸ºæœ€ç»ˆç±»åˆ«

    å¦‚æœæ˜¯å›å½’ç®—æ³•ï¼Œ$T$ä¸ªå¼±å­¦ä¹ å™¨å¾—åˆ°çš„å›å½’ç»“æœè¿›è¡Œç®—æœ¯å¹³å‡å¾—åˆ°çš„å€¼ä¸ºæœ€ç»ˆçš„æ¨¡å‹è¾“å‡ºã€‚

- éšæœºæ£®æ—ä¸ºä»€ä¹ˆä¸å®¹æ˜“è¿‡æ‹Ÿåˆï¼Ÿ

  - éšæœºæ£®æ—ä¸­çš„æ¯ä¸€é¢—æ ‘éƒ½æ˜¯è¿‡æ‹Ÿåˆçš„ï¼Œæ‹Ÿåˆåˆ°éå¸¸å°çš„ç»†èŠ‚ä¸Š
    - éšæœºæ£®æ—é€šè¿‡å¼•å…¥éšæœºæ€§ï¼Œä½¿æ¯ä¸€é¢—æ ‘æ‹Ÿåˆçš„ç»†èŠ‚ä¸åŒ

  - æ‰€æœ‰æ ‘ç»„åˆåœ¨ä¸€èµ·ï¼Œè¿‡æ‹Ÿåˆçš„éƒ¨åˆ†å°±ä¼šè‡ªåŠ¨è¢«æ¶ˆé™¤æ‰ã€‚

**ç®—æ³•**ï¼š

<img src="../images/(null)-20220725101113336.(null)" alt="img" style="width:50%;" />

- Applicable to both classiï¬cation and regression problems
- Smarter bagging for trees
- Motivated by theory that generalization **improves with uncorrelated trees**
- Bootstrapped samples and random subset of features are used to train each tree
  - sample rows and columns, æ¯ä¸ªå»train decision tree
  - highly dominate çš„å˜é‡ RF ä¼šå»debiased features
- The outputs from each of the models are averaged to make the ï¬nal prediction.

è¶…**å‚æ•°**ï¼š

- Random Forest hyperparameters:
  - \# of trees
  - \# of features
    - Classiï¬cation - sqrt(# of features) æ˜¯ general guideline
    - Regression - # of featuresï¼Œä¸€èˆ¬ä¸sample feature
- Decision Tree hyperparameters (splitting criteria, maximum depth, etc. ) 

**RandomForestä¸éœ€è¦CVçš„åŸå› **

- æ¯æ¬¡è®­ç»ƒçš„æ—¶å€™ éƒ½åªçœ‹äº†bootstrap sampleï¼Œæœ‰ä¸€éƒ¨åˆ†çš„æ•°æ®æ˜¯æ²¡æœ‰touchçš„
- Uses `out-of-bag (OOB)` error for model selection
  - OOB error is the **average error of** **a data point** calculated using predictions from the trees that do not contain it in their respective bootstrap sample
  - æ¯ä¸ªdata point çš„ error = æ ·æœ¬å¤–é¢„æµ‹çš„errorçš„å¹³å‡
    - å¦‚æœæœ‰ä¸€ä¸ªsampleå»äº†æ‰€æœ‰çš„treeï¼Œé‚£ä¹ˆå®ƒå°±ä¸ä¼šåŠ å…¥out-of-bagçš„è®¡ç®—
    - å¦‚æœæœ‰ä¸€ä¸ªsampleåªå»äº†ä¸€ä¸ªtreeï¼Œé‚£ä¹ˆè¿™ä¸ªtreeçš„errorå°±æ˜¯è¿™ä¸ªdata pointçš„oob error
      - If I built 100 trees and 99 trees used 1 sample and one tree did not use that sample, then that one tree will make a prediction on this, and you can calculate the error from that.

 **Feature Importances**

- RFæœ‰ä¸¤ç§æ–¹æ³•ï¼š

  - é€šè¿‡è®¡ç®—Giniç³»æ•°çš„å‡å°‘é‡VIm=GIâˆ’(GIL+GIR)åˆ¤æ–­ç‰¹å¾é‡è¦æ€§ï¼Œè¶Šå¤§è¶Šé‡è¦ã€‚

  - å¯¹äºä¸€é¢—æ ‘ï¼Œå…ˆä½¿ç”¨è¢‹å¤–é”™è¯¯ç‡(OOB)æ ·æœ¬è®¡ç®—æµ‹è¯•è¯¯å·®aï¼Œå†éšæœºæ‰“ä¹±OOBæ ·æœ¬ä¸­ç¬¬iä¸ªç‰¹å¾ï¼ˆä¸Šä¸‹æ‰“ä¹±ç‰¹å¾çŸ©é˜µç¬¬iåˆ—çš„é¡ºåºï¼‰åè®¡ç®—æµ‹è¯•è¯¯å·®bï¼Œaä¸bå·®è·è¶Šå¤§ç‰¹å¾iè¶Šé‡è¦ã€‚

    - è¢‹å¤–æ•°æ®(OOB)ï¼š å¤§çº¦æœ‰1/3çš„è®­ç»ƒå®ä¾‹æ²¡æœ‰å‚ä¸ç¬¬kæ£µæ ‘çš„ç”Ÿæˆï¼Œå®ƒä»¬ç§°ä¸ºç¬¬$k$æ£µæ ‘çš„è¢‹å¤–æ•°æ®æ ·æœ¬ã€‚

    - åœ¨éšæœºæ£®æ—ä¸­æŸä¸ªç‰¹å¾$X$çš„é‡è¦æ€§çš„è®¡ç®—æ–¹æ³•å¦‚ä¸‹ï¼š

    - å¯¹äºéšæœºæ£®æ—ä¸­çš„æ¯ä¸€é¢—å†³ç­–æ ‘ï¼Œä½¿ç”¨ç›¸åº”çš„OOB(è¢‹å¤–æ•°æ®)æ¥è®¡ç®—å®ƒçš„è¢‹å¤–æ•°æ®è¯¯å·®ï¼Œè®°ä¸º$err_{OOB1}$ã€‚

    - éšæœºåœ°å¯¹è¢‹å¤–æ•°æ®OOBæ‰€æœ‰æ ·æœ¬çš„ç‰¹å¾$X$åŠ å…¥å™ªå£°å¹²æ‰°(å°±å¯ä»¥éšæœºçš„æ”¹å˜æ ·æœ¬åœ¨ç‰¹å¾Xå¤„çš„å€¼)ï¼Œå†æ¬¡è®¡ç®—å®ƒçš„è¢‹å¤–æ•°æ®è¯¯å·®ï¼Œè®°ä¸º$err_{OOB2}$ã€‚

    - å‡è®¾éšæœºæ£®æ—ä¸­æœ‰$N$æ£µæ ‘ï¼Œé‚£ä¹ˆå¯¹äºç‰¹å¾$X$çš„é‡è¦æ€§ä¸º$(err_{OOB2}-err_{OOB1}/N)$ï¼Œä¹‹æ‰€ä»¥å¯ä»¥ç”¨è¿™ä¸ªè¡¨è¾¾å¼æ¥ä½œä¸ºç›¸åº”ç‰¹å¾çš„é‡è¦æ€§çš„åº¦é‡å€¼æ˜¯å› ä¸ºï¼šè‹¥ç»™æŸä¸ªç‰¹å¾éšæœºåŠ å…¥å™ªå£°ä¹‹åï¼Œè¢‹å¤–çš„å‡†ç¡®ç‡å¤§å¹…åº¦é™ä½ï¼Œåˆ™è¯´æ˜è¿™ä¸ªç‰¹å¾å¯¹äºæ ·æœ¬çš„åˆ†ç±»ç»“æœå½±å“å¾ˆå¤§ï¼Œä¹Ÿå°±æ˜¯è¯´å®ƒçš„é‡è¦ç¨‹åº¦æ¯”è¾ƒé«˜ã€‚

- Feature importance is calculated as the **decrease in node impurity** weighted by the *probability of samples in node*s that are reaching that node.
  - The node probability can be calculated by the **number of samples that reach the node**, divided by the total number of samples.
  - æœ‰æ—¶å€™ä¼šé€‰å°‘çš„number of trees ç‰¹åˆ«æ˜¯æå‡ä¸æ˜¾è‘—çš„æ—¶å€™ï¼Œæ¯”å¦‚100ä¸ªtreeå¯èƒ½åªç”¨äº†5ä¸ªfeaturesï¼Œé‚£ä¹ˆåé¢æˆ‘å°±å¯ä»¥åªmaintainè¿™5ä¸ªfeatures
- The higher the value the more important the feature.

<img src="../images/(null)-20220725101112347.(null)" alt="img" style="width:50%;" />

- SKLearnä¸­çš„ `warm_start`
  - When fitting an estimator repeatedly on the same dataset, but for multiple parameter values (such as to find the value maximizing performance as in [grid search](https://scikit-learn.org/stable/modules/grid_search.html#grid-search)), it may be possible to reuse aspects of the model learned from the previous parameter value, saving time. When `warm_start` is true, the existing [fitted](https://scikit-learn.org/stable/glossary.html#term-fitted) model [attributes](https://scikit-learn.org/stable/glossary.html#term-attributes) are used to initialize the new model in a subsequent call to [fit](https://scikit-learn.org/stable/glossary.html#term-fit).
  - Note that this is only applicable for some models and some parameters, and even some orders of parameter values. For example, `warm_start` may be used when building random forests to add more trees to the forest (increasing `n_estimators`) but not to reduce their number.



### Adaptive Boosting

Adaboostç®—æ³•åˆ©ç”¨åŒä¸€ç§åŸºåˆ†ç±»å™¨ï¼ˆå¼±åˆ†ç±»å™¨ï¼‰ï¼ŒåŸºäºåˆ†ç±»å™¨çš„é”™è¯¯ç‡åˆ†é…ä¸åŒçš„æƒé‡å‚æ•°ï¼Œæœ€åç´¯åŠ åŠ æƒçš„é¢„æµ‹ç»“æœä½œä¸ºè¾“å‡ºã€‚

**æµç¨‹**ï¼š

- æ ·æœ¬èµ‹äºˆæƒé‡ï¼Œå¾—åˆ°ç¬¬ä¸€ä¸ªåˆ†ç±»å™¨ã€‚

  Initially, a decision stump classiï¬er (just splits the data into two regions) is ï¬t to the data

- è®¡ç®—è¯¥åˆ†ç±»å™¨çš„é”™è¯¯ç‡ï¼Œæ ¹æ®**é”™è¯¯ç‡èµ‹äºˆåˆ†ç±»å™¨æƒé‡ï¼ˆ**æ³¨æ„è¿™é‡Œæ˜¯åˆ†ç±»å™¨çš„æƒé‡ï¼‰

- <u>å¢åŠ åˆ†é”™æ ·æœ¬çš„æƒé‡ï¼Œå‡å°åˆ†å¯¹æ ·æœ¬çš„æƒé‡</u>ï¼ˆæ³¨æ„è¿™é‡Œæ˜¯æ ·æœ¬çš„æƒé‡ï¼‰

  The data points correctly classiï¬ed are given less weightage while misclassiï¬ed data points are given higher weightage in the next iteration

- ç„¶åå†ç”¨æ–°çš„æ ·æœ¬æƒé‡è®­ç»ƒæ•°æ®ï¼Œå¾—åˆ°æ–°çš„åˆ†ç±»å™¨

  A decision stump classiï¬er is now ï¬t to the data with weights determined in previous iteration

- å¤šæ¬¡è¿­ä»£ï¼Œç›´åˆ°**åˆ†ç±»å™¨é”™è¯¯ç‡ä¸º0æˆ–è€…æ•´ä½“å¼±åˆ†ç±»å™¨é”™è¯¯ä¸º0ï¼Œæˆ–è€…åˆ°è¾¾è¿­ä»£æ¬¡æ•°ã€‚**

- å°†**æ‰€æœ‰å¼±åˆ†ç±»å™¨çš„ç»“æœåŠ æƒæ±‚å’Œ**ï¼Œå¾—åˆ°ä¸€ä¸ªè¾ƒä¸ºå‡†ç¡®çš„åˆ†ç±»ç»“æœã€‚é”™è¯¯ç‡ä½çš„åˆ†ç±»å™¨è·å¾—æ›´é«˜çš„å†³å®šç³»æ•°ï¼Œä»è€Œåœ¨å¯¹æ•°æ®è¿›è¡Œé¢„æµ‹æ—¶èµ·å…³é”®ä½œç”¨ã€‚

  Weights (ğ†) for each classiï¬er (estimated during the training process) are used to combine the outputs and make the ï¬nal prediction.

<img src="../images/(null)-20220725101132539.(null)" alt="img" style="width: 50%;" />

**ç®—æ³•ï¼š**

1. Initialize the observation weights $w_{i}=1 / N, i=1,2, \ldots, N$.

   æœ€å¼€å§‹æ‰€æœ‰è§‚æµ‹éƒ½æ˜¯equal weights

2. For $m=1$ to $M$ è®­ç»ƒMä¸ªclassifier:

   1. Fit a classifier $G_{m}(x)$ to the training data using weights $w_{i}$.

   2. è®¡ç®—å®ƒçš„weighted error = $\frac{é”™è¯¯æ ·æœ¬çš„æ€»æƒé‡}{æ€»æƒé‡}$ï¼š

      $$\operatorname{err}_{m}=\frac{\sum_{i=1}^{N} w_{i} I\left(y_{i} \neq G_{m}\left(x_{i}\right)\right)}{\sum_{i=1}^{N} w_{i}} $$

     3. Compute $\alpha_{m}=\log \left(\left(1-\operatorname{err}_{m}\right) / \operatorname{err}_{m}\right)$ å¾—åˆ°classifierçš„æƒé‡


     4. Set $w_{i} \leftarrow w_{i} \cdot \exp \left[\alpha_{m} \cdot I\left(y_{i} \neq G_{m}\left(x_{i}\right)\right)\right], i=1,2, \ldots, N$


3. Output $G(x)=\operatorname{sign}\left[\sum_{m=1}^{M} \alpha_{m} G_{m}(x)\right]$: æ‰€æœ‰çš„Classifierçš„ç»“æœæ ¹æ®$$\alpha_m$$ä¸ºæƒåŠ æƒå¹³å‡ï¼

**æ•°å­¦ç†è§£**

- ç®—æ³•çš„Assumed Formula $$G(x)=\sum_{m} \alpha_{m} G_{m}(x)$$

  - Assume the Loss function is a exponentially loss function:$$L_{e x p}(x, y)=\exp (-y G(x))$$

  - æ‰€ä»¥ç›®æ ‡å˜æˆäº†$E=\operatorname{Min}_{\alpha_{m}, G_{m}}\left(\sum_{i} \exp \left(-y_{i} \sum_{m} \alpha_{m} G_{m}\left(x_{i}\right)\right)\right)$

    - æ±‚$$\frac{\partial E}{\partial \alpha_{m}}=0$$:

      $$\alpha_{m}=\ln \left(\frac{1-e r r_{m}}{e r r_{m}}\right)$$

      $$\operatorname{err}_{m}=\frac{\sum_{i=1}^{N} w_{i} I\left(y_{i} \neq G_{m}\left(x_{i}\right)\right)}{\sum_{i=1}^{N} w_{i}}$$

**è¶…å‚æ•°**

- Classiï¬cation:
  - \# estimators
  - learning rateï¼šæ¯æ¬¡åŠ å…¥a fraction of the value
  - base estimatorï¼šå¯ä»¥æ¢æˆåˆ«çš„æ¨¡å‹ï¼Œè€Œä¸æ˜¯`decision stump classiï¬er`
- Regressionï¼š
  - loss function
  - learning rate
  - \# of estimators
  - base estimator



### Gradient Boosting

- åˆ†ç±»å›å½’éƒ½å¯ä»¥ï¼ˆåˆ†ç±»çš„è¯ ä¹Ÿæ˜¯æ‹¿probabilityå»regressionï¼‰
- Trains regression trees in a sequential manner on **modiï¬ed versions of the datasets.** 
- Every tree is trained on the residuals of the data points obtained by subtracting the predictions from the previous tree
  - **weights** for each classiï¬er (estimated during the training process) are used to combine the outputs and make the ï¬nal prediction.

<img src="../images/(null)-20220725101152441.(null)" alt="img" style="width:50%;" />

**ç®—æ³•**

Input: training set $\left\{\left(x_{i}, y_{i}\right)\right\}_{i=1}^{n}$, a differentiable loss function $L(y, F(x))$, number of iterations $M$.
Algorithm:
1. Initialize model with a constant value ç¬¬ä¸€æ­¥å…ˆç”¨ä¸€ä¸ªlossæœ€å°çš„å¸¸æ•°æ¥é¢„æµ‹:
$$
F_{0}(x)=\underset{\gamma}{\arg \min } \sum_{i=1}^{n} L\left(y_{i}, \gamma\right) .
$$
2. For $m=1$ to $M$ :
   1. Compute so-called pseudo-residuals è®¡ç®—ä¸€ä¸ªæ®‹å·® å…¶å®ä¹Ÿå°±æ˜¯æ¢¯åº¦ï¼:
$$
r_{i m}=-\left[\frac{\partial L\left(y_{i}, F\left(x_{i}\right)\right)}{\partial F\left(x_{i}\right)}\right]_{F(x)=F_{m-1}(x)} \text { for } i=1, \ldots, n \text {. }
$$
2. Fit a base learner (or weak learner, e.g. tree) $h_{m}(x)$ to pseudo-residuals, i.e. train it using the training set $\left\{\left(x_{i}, r_{i m}\right)\right\}_{i=1}^{n}$
3. Compute multiplier $\gamma_{m}$ by solving the following one-dimensional optimization problem:
$$
\gamma_{m}=\underset{\gamma}{\arg \min } \sum_{i=1}^{n} L\left(y_{i}, F_{m-1}\left(x_{i}\right)+\gamma h_{m}\left(x_{i}\right)\right) .
$$
4. Update the model:
$$
F_{m}(x)=F_{m-1}(x)+\gamma_{m} h_{m}(x) .
$$
3. Output $F_{M}(x)$.

**ä¸ºä»€ä¹ˆå«gradient boostingï¼Ÿ**

- Gradient Descent

<img src="../images/(null)-20220725101152792.(null)" alt="img" style="width:50%;" />

- Gradient Boostingï¼šThe gradient in Gradient boosting is nothing but the residual. As every tree we are boosting the residual (fit a model that does well on that that residual), we are actually boosting the gradient

- ç›®æ ‡å‡½æ•°$E=\operatorname{Min}_{\gamma_{m}, F_{m}}\left(\frac{1}{2} \sum_{i}\left(y_{i}-F\left(x_{i}\right)\right)^{2}\right)$
  - å‡è®¾Functionçš„å½¢å¼æ˜¯$$F(x)=\sum_{m} \gamma_{m} F_{m}(x)$$ï¼Œ ç”¨squared error$$L(x, y)=\frac{1}{2}(y-F(x))^{2}$$
  - å¯ä»¥è®¡ç®—åœ¨mè¿™ä¸ªæ¨¡å‹å‡ºæ¥çš„æ—¶å€™è®¡ç®—çš„Gradient = $$ \frac{\partial E}{\partial F_{m-1}(x)} = - (y - F_{m-1}(x))=$$ç¬¬m-1è½®çš„residualç›¸åæ•°
  - ç„¶åå°±å¾—åˆ°äº†æ›´æ–°è§„åˆ™ï¼š$$F_{m}(x)=F_{m-1}(x)-\gamma \frac{\partial E}{\partial F_{m-1}(x)}$$
    - Gradient = $$ \frac{\partial E}{\partial F_{m-1}(x)} = y - F_{m-1}(x) =$$ç¬¬mè½®è®­ç»ƒæ—¶é¢å¯¹çš„ä¸Šä¸€è½®çš„residual
- åœ¨MSEçš„æƒ…æ³ä¸‹ï¼Œè´ŸGradientåˆšå¥½æ˜¯residualï¼Œè€Œåœ¨å…¶å®ƒæƒ…å†µä¸‹ï¼Œgradient descentçš„æ—¶å€™ä¾ç„¶æ˜¯åœ¨æ²¿ç€gradientçš„æ–¹å‘å­¦ä¹ ä¼˜åŒ–

**è¶…å‚æ•°**

- \# of estimators
- Learning rate
- Decision tree parameters (max depth, min number of samples etc.)
- Regularization parameters
- Row sampling / Column sampling: Pass tree from one to anotherçš„æ—¶å€™å¯ä»¥åªpass a sample of samples.

---

å„ç§Implementation

---

#### **GBDT**

- é¦–å…ˆä»‹ç»Adaboost Treeï¼Œæ˜¯ä¸€ç§boostingçš„æ ‘é›†æˆæ–¹æ³•ã€‚åŸºæœ¬æ€è·¯æ˜¯ä¾æ¬¡è®­ç»ƒå¤šæ£µæ ‘ï¼Œæ¯æ£µæ ‘è®­ç»ƒæ—¶å¯¹åˆ†é”™çš„æ ·æœ¬è¿›è¡ŒåŠ æƒã€‚æ ‘æ¨¡å‹ä¸­å¯¹æ ·æœ¬çš„åŠ æƒå®é™…æ˜¯å¯¹æ ·æœ¬é‡‡æ ·å‡ ç‡çš„åŠ æƒï¼Œåœ¨è¿›è¡Œæœ‰æ”¾å›æŠ½æ ·æ—¶ï¼Œåˆ†é”™çš„æ ·æœ¬æ›´æœ‰å¯èƒ½è¢«æŠ½åˆ°
- GBDTæ˜¯Adaboost Treeçš„æ”¹è¿›ï¼Œæ¯æ£µæ ‘éƒ½æ˜¯CARTï¼ˆåˆ†ç±»å›å½’æ ‘ï¼‰ï¼Œæ ‘åœ¨å¶èŠ‚ç‚¹è¾“å‡ºçš„æ˜¯ä¸€ä¸ªæ•°å€¼ï¼Œåˆ†ç±»è¯¯å·®å°±æ˜¯çœŸå®å€¼å‡å»å¶èŠ‚ç‚¹çš„è¾“å‡ºå€¼ï¼Œå¾—åˆ°æ®‹å·®ã€‚GBDTè¦åšçš„å°±æ˜¯ä½¿ç”¨æ¢¯åº¦ä¸‹é™çš„æ–¹æ³•å‡å°‘åˆ†ç±»è¯¯å·®å€¼ã€‚
- åœ¨GBDTçš„è¿­ä»£ä¸­ï¼Œå‡è®¾æˆ‘ä»¬å‰ä¸€è½®è¿­ä»£å¾—åˆ°çš„å¼ºå­¦ä¹ å™¨æ˜¯ftâˆ’1(x), æŸå¤±å‡½æ•°æ˜¯L(y,ftâˆ’1(x)), æˆ‘ä»¬æœ¬è½®è¿­ä»£çš„ç›®æ ‡æ˜¯æ‰¾åˆ°ä¸€ä¸ªCARTå›å½’æ ‘æ¨¡å‹çš„å¼±å­¦ä¹ å™¨ht(x)ï¼Œè®©æœ¬è½®çš„æŸå¤±æŸå¤±L(y,ft(x)=L(y,ftâˆ’1(x)+ht(x))æœ€å°ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæœ¬è½®è¿­ä»£æ‰¾åˆ°å†³ç­–æ ‘ï¼Œè¦è®©æ ·æœ¬çš„æŸå¤±å°½é‡å˜å¾—æ›´å°ã€‚
- å¾—åˆ°å¤šæ£µæ ‘åï¼Œæ ¹æ®æ¯é¢—æ ‘çš„åˆ†ç±»è¯¯å·®è¿›è¡ŒåŠ æƒæŠ•ç¥¨
- GBDTçš„æ€æƒ³å¯ä»¥ç”¨ä¸€ä¸ªé€šä¿—çš„ä¾‹å­è§£é‡Šï¼Œå‡å¦‚æœ‰ä¸ªäºº30å²ï¼Œæˆ‘ä»¬é¦–å…ˆç”¨20å²å»æ‹Ÿåˆï¼Œå‘ç°æŸå¤±æœ‰10å²ï¼Œè¿™æ—¶æˆ‘ä»¬ç”¨6å²å»æ‹Ÿåˆå‰©ä¸‹çš„æŸå¤±ï¼Œå‘ç°å·®è·è¿˜æœ‰4å²ï¼Œç¬¬ä¸‰è½®æˆ‘ä»¬ç”¨3å²æ‹Ÿåˆå‰©ä¸‹çš„å·®è·ï¼Œå·®è·å°±åªæœ‰ä¸€å²äº†ã€‚å¦‚æœæˆ‘ä»¬çš„è¿­ä»£è½®æ•°è¿˜æ²¡æœ‰å®Œï¼Œå¯ä»¥ç»§ç»­è¿­ä»£ä¸‹é¢ï¼Œæ¯ä¸€è½®è¿­ä»£ï¼Œæ‹Ÿåˆçš„å²æ•°è¯¯å·®éƒ½ä¼šå‡å°ã€‚

#### GradientBoostingClassiï¬er

Early implementation of Gradient Boosting in sklearn

- Most important parametersï¼š
  - of estimators
  - learning rate
- å¥½ç”¨æ€§ï¼š
  - Supports both binary & multi-class classiï¬cation
  - Supports sparse data
- ç¼ºç‚¹ï¼š
  - Typical slow on large datasets
- ç‰¹å¾é‡è¦æ€§ï¼š
  - æ‰€æœ‰å›å½’æ ‘ä¸­é€šè¿‡ç‰¹å¾iåˆ†è£‚å**å¹³æ–¹æŸå¤±çš„å‡å°‘å€¼**çš„å’Œ/å›å½’æ ‘æ•°é‡ å¾—åˆ°ç‰¹å¾é‡è¦æ€§ã€‚
  - åœ¨sklearnä¸­ï¼ŒGBDTå’ŒRFçš„ç‰¹å¾é‡è¦æ€§è®¡ç®—æ–¹æ³•æ˜¯ç›¸åŒçš„ï¼Œéƒ½æ˜¯åŸºäºå•æ£µæ ‘è®¡ç®—æ¯ä¸ªç‰¹å¾çš„é‡è¦æ€§ï¼Œæ¢ç©¶æ¯ä¸ªç‰¹å¾åœ¨æ¯æ£µæ ‘ä¸Šåšäº†å¤šå°‘çš„è´¡çŒ®ï¼Œå†å–ä¸ªå¹³å‡å€¼ã€‚

#### HistGradientBoostingClassiï¬er

- Orders of magnitude **faster** than GradientBoostingClassiï¬er on large datasets 
- Inspired by LightGBM implementation
- Histogram-based split ï¬nding in tree learning
- ç¼ºç‚¹ï¼š
  - Does not support sparse data
  - Does not support monotonicity constraintsï¼šæ¯”å¦‚enforceä¸€ä¸ªå˜é‡çš„ç³»æ•°ä¸ºæ­£/è´Ÿçš„ï¼
    -  the true relationship has some quality, constraints can be used to improve the predictive performance of the model.
- ä¼˜ç‚¹ï¼š
  - Supports both binary & multi-class classiï¬cation
  - Natively supports categorical featuresï¼ˆä¸éœ€è¦Preprocessï¼‰
  - Bin the value into less bins (1000 uniqueæ•°å€¼ -> 10)

<img src="../images/(null)-20220725101152501.(null)" alt="img" style="width:50%;" />



### XGBoost

XGBoosté‡‡ç”¨çš„æ˜¯level-wiseï¼ˆBFSï¼‰ç”Ÿé•¿ç­–ç•¥ï¼Œèƒ½å¤ŸåŒæ—¶åˆ†è£‚åŒä¸€å±‚çš„å¶å­ï¼Œä»è€Œè¿›è¡Œå¤šçº¿ç¨‹ä¼˜åŒ–ã€‚

- åœ¨å†³ç­–æ ‘çš„ç”Ÿé•¿è¿‡ç¨‹ä¸­ï¼Œä¸€ä¸ªéå¸¸å…³é”®çš„é—®é¢˜æ˜¯å¦‚ä½•æ‰¾åˆ°å¶å­çš„èŠ‚ç‚¹çš„æœ€ä¼˜åˆ‡åˆ†ç‚¹"Xgboost æ”¯æŒä¸¤ç§åˆ†è£‚èŠ‚ç‚¹çš„æ–¹æ³•â€”â€”è´ªå¿ƒç®—æ³•å’Œè¿‘ä¼¼ç®—æ³•

  - è´ªå¿ƒç®—æ³•ï¼šé’ˆå¯¹æ¯ä¸ªç‰¹å¾ï¼ŒæŠŠå±äºè¯¥èŠ‚ç‚¹çš„è®­ç»ƒæ ·æœ¬æ ¹æ®è¯¥ç‰¹å¾å€¼è¿›è¡Œå‡åºæ’åˆ—ï¼Œé€šè¿‡çº¿æ€§æ‰«æçš„æ–¹å¼æ¥å†³å®šè¯¥ç‰¹å¾çš„æœ€ä½³åˆ†è£‚ç‚¹ï¼Œå¹¶è®°å½•è¯¥ç‰¹å¾çš„åˆ†è£‚æ”¶ç›Š
  - è¿‘ä¼¼ç®—æ³•ï¼šå¯¹äºæ¯ä¸ªç‰¹å¾ï¼Œé¦–å…ˆæ ¹æ®ç‰¹å¾åˆ†å¸ƒçš„åˆ†ä½æ•°æå‡ºå€™é€‰åˆ’åˆ†ç‚¹ï¼Œç„¶åå°†è¿ç»­å‹ç‰¹å¾æ˜ å°„åˆ°ç”±è¿™äº›å€™é€‰ç‚¹åˆ’åˆ†çš„æ¡¶ä¸­ï¼Œç„¶åèšåˆç»Ÿè®¡ä¿¡æ¯æ‰¾åˆ°æ‰€æœ‰åŒºé—´çš„æœ€ä½³åˆ†è£‚ç‚¹

- ä¼˜ç‚¹

  - æŸå¤±å‡½æ•°è¿›è¡Œäº†äºŒé˜¶æ³°å‹’å±•å¼€ï¼š

    - æ³°å‹’äºŒé˜¶è¿‘ä¼¼æ¯”GBDTä¸€é˜¶è¿‘ä¼¼æ›´æ¥è¿‘çœŸå®çš„Loss Fnctionï¼Œè‡ªç„¶ä¼˜åŒ–çš„æ›´å½»åº•äºŒé˜¶ä¿¡æ¯èƒ½å¤Ÿè®©æ¢¯åº¦æ”¶æ•›çš„æ›´å¿«ï¼Œç±»ä¼¼ç‰›é¡¿æ³•æ¯”SGDæ”¶æ•›æ›´å¿«ã€‚

      äºŒé˜¶ä¿¡æ¯æœ¬èº«å°±èƒ½è®©æ¢¯åº¦æ”¶æ•›æ›´å¿«æ›´å‡†ç¡®ã€‚è¿™ä¸€ç‚¹åœ¨ä¼˜åŒ–ç®—æ³•é‡Œçš„**ç‰›é¡¿æ³•**ä¸­å·²ç»è¯å®ã€‚å¯ä»¥ç®€å•è®¤ä¸ºä¸€é˜¶å¯¼æŒ‡å¼•æ¢¯åº¦æ–¹å‘ï¼ŒäºŒé˜¶å¯¼æŒ‡å¼•æ¢¯åº¦æ–¹å‘å¦‚ä½•å˜åŒ–ã€‚ç®€å•æ¥è¯´ï¼Œç›¸å¯¹äºGBDTçš„ä¸€é˜¶æ³°å‹’å±•å¼€ï¼ŒXGBoosté‡‡ç”¨äºŒé˜¶æ³°å‹’å±•å¼€ï¼Œå¯ä»¥æ›´ä¸ºç²¾å‡†çš„é€¼è¿‘çœŸå®çš„æŸå¤±å‡½æ•°ã€‚

    - èƒ½å¤Ÿè‡ªå®šä¹‰æŸå¤±å‡½æ•°ï¼ŒäºŒé˜¶æ³°å‹’å±•å¼€å¯ä»¥è¿‘ä¼¼å¤§é‡æŸå¤±å‡½æ•°ï¼›

    - æ³¨æ„ï¼šGBDT+MSEçš„æ—¶å€™boostingæ‹Ÿåˆçš„æ‰æ˜¯æ®‹å·®ï¼Œ**XGBoostæ‹Ÿåˆçš„ä¸æ˜¯æ®‹å·®è€Œæ˜¯ç›´æ¥åˆ©ç”¨äº†äºŒé˜¶å¯¼æ•°ä½œä¸ºæ‹Ÿåˆå¯¹è±¡ï¼Œæ‰¾åˆ°è¯¯å·®å‡½æ•°objå‡å°çš„å¹…åº¦**

  - å¯ä»¥åœ¨ç‰¹å¾é¢—ç²’åº¦å¹¶è¡Œè®­ç»ƒ

    - ä¸æ˜¯è¯´æ¯æ£µæ ‘å¯ä»¥å¹¶è¡Œè®­ç»ƒï¼Œ$XGBoost$æœ¬è´¨ä¸Šä»ç„¶é‡‡ç”¨$Boosting$æ€æƒ³ï¼Œæ¯æ£µæ ‘è®­ç»ƒå‰éœ€è¦ç­‰å‰é¢çš„æ ‘è®­ç»ƒå®Œæˆæ‰èƒ½å¼€å§‹è®­ç»ƒã€‚
    - å†³ç­–æ ‘çš„å­¦ä¹ æœ€è€—æ—¶çš„ä¸€ä¸ªæ­¥éª¤å°±æ˜¯å¯¹ç‰¹å¾çš„å€¼è¿›è¡Œæ’åºï¼ˆå› ä¸ºè¦ç¡®å®šæœ€ä½³åˆ†å‰²ç‚¹ï¼‰ï¼ŒXGBooståœ¨è®­ç»ƒä¹‹å‰ï¼Œæ¯ä¸ªç‰¹å¾æŒ‰ç‰¹å¾å€¼å¯¹æ ·æœ¬è¿›è¡Œé¢„æ’åº**å¹¶å­˜å‚¨ä¸ºblockç»“æ„**
    - åœ¨åé¢æŸ¥æ‰¾ç‰¹å¾åˆ†å‰²ç‚¹æ—¶å¯ä»¥é‡å¤ä½¿ç”¨block
    - åªä¸è¿‡åœ¨è¿›è¡ŒèŠ‚ç‚¹çš„åˆ†è£‚æ—¶ï¼Œéœ€è¦è®¡ç®—æ¯ä¸ªç‰¹å¾çš„å¢ç›Šï¼Œæœ€ç»ˆé€‰å¢ç›Šæœ€å¤§çš„é‚£ä¸ªç‰¹å¾å»åšåˆ†è£‚ï¼Œè¿™é‡Œå„ä¸ªç‰¹å¾çš„å¢ç›Šè®¡ç®—ä¹Ÿå¯ä»¥å¤šçº¿ç¨‹è¿›è¡Œ

  - ç›´æ–¹å›¾ï¼šFast approximate split ï¬nding based on histograms

    - **xgbooståœ¨æ¯ä¸€å±‚éƒ½åŠ¨æ€æ„å»ºç›´æ–¹å›¾**ï¼Œåˆ†æ¡¶çš„ä¾æ®æ˜¯æ ·æœ¬çš„äºŒçº§æ¢¯åº¦ï¼Œæ¯ä¸€å±‚éƒ½è¦é‡æ–°æ„å»º

      lightgbmä¸­å¯¹æ¯ä¸ªç‰¹å¾éƒ½æœ‰ä¸€ä¸ªç›´æ–¹å›¾ï¼Œæ‰€ä»¥æ„å»ºä¸€æ¬¡ç›´æ–¹å›¾å°±å¤Ÿäº†

  - åŠ å…¥æ­£åˆ™é¡¹`Adds l1 and l2 penalties on leaf weights`: åŠ å…¥äº†æ­£åˆ™é¡¹ï¼Œç”¨äºæ§åˆ¶æ¨¡å‹çš„å¤æ‚åº¦ã€‚æ­£åˆ™é¡¹é‡ŒåŒ…å«äº†æ ‘çš„å¶å­èŠ‚ç‚¹ä¸ªæ•°ã€å¶å­èŠ‚ç‚¹æƒé‡çš„ L2 èŒƒå¼ã€‚æ­£åˆ™é¡¹é™ä½äº†æ¨¡å‹çš„æ–¹å·®ï¼Œä½¿å­¦ä¹ å‡ºæ¥çš„æ¨¡å‹æ›´åŠ ç®€å•ï¼Œæœ‰åŠ©äºé˜²æ­¢è¿‡æ‹Ÿåˆï¼›

  - **Shrinkageï¼ˆç¼©å‡ï¼‰ï¼š**ç›¸å½“äºå­¦ä¹ é€Ÿç‡ã€‚XGBoost åœ¨è¿›è¡Œå®Œä¸€æ¬¡è¿­ä»£åï¼Œä¼šå°†å¶å­èŠ‚ç‚¹çš„æƒé‡ä¹˜ä¸Šè¯¥ç³»æ•°ï¼Œä¸»è¦æ˜¯ä¸ºäº†å‰Šå¼±æ¯æ£µæ ‘çš„å½±å“ï¼Œè®©åé¢æœ‰æ›´å¤§çš„å­¦ä¹ ç©ºé—´ï¼›

  - å¥½ç”¨çš„ç‚¹ï¼š

    - Supports `GPU training`å—ç»“æ„å¯ä»¥å¾ˆå¥½çš„æ”¯æŒå¹¶è¡Œè®¡ç®—ï½œsparse dataï½œmissing valuesï½œWorks well with pipelines in sklearn due to a compatible interface
      - ç¼ºå¤±å€¼çš„å¤„ç†ï¼ˆLight GBMä¸€æ ·ï¼‰ï¼šå…ˆä¸å¤„ç†é‚£äº›å€¼ç¼ºå¤±çš„æ ·æœ¬ï¼Œé‡‡ç”¨é‚£äº›æœ‰å€¼çš„æ ·æœ¬æå‡ºåˆ†è£‚ç‚¹ï¼Œåœ¨éå†æ¯ä¸ªæœ‰å€¼ç‰¹å¾çš„æ—¶å€™ï¼Œå°è¯•å°†ç¼ºå¤±æ ·æœ¬åˆ’å…¥å·¦å­æ ‘å’Œå³å­æ ‘ï¼Œé€‰æ‹©ä½¿æŸå¤±æœ€ä¼˜çš„å€¼ä½œä¸ºåˆ†è£‚ç‚¹
    - Monotonicity & feature interaction constraints
      - **feature interaction constraints:** when you consider one feature, you don't want to consider another feature in that branch itself. So we can impose such feature interaction constraints as well, in addition to monotonic relationships.

- ç¼ºç‚¹

  - è™½ç„¶åˆ©ç”¨é¢„æ’åºå’Œè¿‘ä¼¼ç®—æ³•å¯ä»¥é™ä½å¯»æ‰¾æœ€ä½³åˆ†è£‚ç‚¹çš„è®¡ç®—é‡ï¼Œä½†åœ¨èŠ‚ç‚¹åˆ†è£‚è¿‡ç¨‹ä¸­ä»éœ€è¦éå†æ•°æ®é›†ï¼›
  - é¢„æ’åºè¿‡ç¨‹çš„ç©ºé—´å¤æ‚åº¦è¿‡é«˜ï¼Œä¸ä»…éœ€è¦å­˜å‚¨ç‰¹å¾å€¼ï¼Œè¿˜éœ€è¦å­˜å‚¨ç‰¹å¾å¯¹åº”æ ·æœ¬çš„æ¢¯åº¦ç»Ÿè®¡å€¼çš„ç´¢å¼•ï¼Œç›¸å½“äºæ¶ˆè€—äº†ä¸¤å€çš„å†…å­˜ã€‚
  - ä½†ä¸åŠ åŒºåˆ†çš„å¯¹å¾…åŒä¸€å±‚çš„å¶å­ï¼Œå¸¦æ¥äº†å¾ˆå¤šæ²¡å¿…è¦çš„å¼€é”€

  - Does not support categorical variables natively

**Feature importanceï¼š**

- importance_type=weightï¼ˆé»˜è®¤å€¼ï¼‰ï¼Œç‰¹å¾é‡è¦æ€§ä½¿ç”¨ç‰¹å¾åœ¨æ‰€æœ‰æ ‘ä¸­ä½œä¸ºåˆ’åˆ†å±æ€§çš„æ¬¡æ•°ã€‚

- mportance_type=gainï¼Œç‰¹å¾é‡è¦æ€§ä½¿ç”¨ç‰¹å¾åœ¨ä½œä¸ºåˆ’åˆ†å±æ€§æ—¶losså¹³å‡çš„é™ä½é‡ã€‚

- importance_type=coverï¼Œç‰¹å¾é‡è¦æ€§ä½¿ç”¨ç‰¹å¾åœ¨ä½œä¸ºåˆ’åˆ†å±æ€§æ—¶å¯¹æ ·æœ¬çš„è¦†ç›–åº¦

- `shap value`: Shapley Additive explanationsçš„ç¼©å†™
  - è¾“å‡ºçš„å½¢å¼ï¼š

    - æ¯ä¸ªæ ·æœ¬: å¯ä»¥çœ‹åˆ°æ¯ä¸ªç‰¹å¾çš„shap_valueè´¡çŒ®ï¼ˆæœ‰æ­£è´Ÿï¼‰
    - æ¯ä¸ªç‰¹å¾ï¼šå¯ä»¥çœ‹åˆ°æ•´ä½“æ ·æœ¬ä¸Šçš„Shapç»å¯¹å€¼å–å¹³å‡å€¼æ¥ä»£è¡¨è¯¥ç‰¹å¾çš„é‡è¦æ€§â€”â€”shapå‡å€¼è¶Šå¤§ï¼Œåˆ™ç‰¹å¾è¶Šé‡è¦
  - è¾“å‡ºçš„å›¾æ ‡
    - dependency
    - individual




**å¤„ç†è¿‡æ‹Ÿåˆçš„æƒ…å†µ**ï¼šé¦–å…ˆBFSçš„æ²¡é‚£ä¹ˆå®¹æ˜“è¿‡æ‹Ÿåˆ

- ç›®æ ‡å‡½æ•°ä¸­å¢åŠ äº†æ­£åˆ™é¡¹ï¼šä½¿ç”¨å¶å­ç»“ç‚¹çš„æ•°ç›®å’Œå¶å­ç»“ç‚¹æƒé‡çš„$L2$æ¨¡çš„å¹³æ–¹ï¼Œæ§åˆ¶æ ‘çš„å¤æ‚åº¦ã€‚

- è®¾ç½®ç›®æ ‡å‡½æ•°çš„**å¢ç›Šé˜ˆå€¼**ï¼šå¦‚æœåˆ†è£‚åç›®æ ‡å‡½æ•°çš„å¢ç›Šå°äºè¯¥é˜ˆå€¼ï¼Œåˆ™ä¸åˆ†è£‚ã€‚

- è®¾ç½®**æœ€å°æ ·æœ¬æƒé‡å’Œ**çš„é˜ˆå€¼ï¼šå½“å¼•å…¥ä¸€æ¬¡åˆ†è£‚åï¼Œé‡æ–°è®¡ç®—æ–°ç”Ÿæˆçš„å·¦ã€å³ä¸¤ä¸ªå¶å­ç»“ç‚¹çš„æ ·æœ¬æƒé‡å’Œã€‚å¦‚æœä»»ä¸€ä¸ªå¶å­ç»“ç‚¹çš„æ ·æœ¬æƒé‡ä½äºæŸä¸€ä¸ªé˜ˆå€¼ï¼ˆæœ€å°æ ·æœ¬æƒé‡å’Œï¼‰ï¼Œä¹Ÿä¼šæ”¾å¼ƒæ­¤æ¬¡åˆ†è£‚ã€‚

- è®¾ç½®æ ‘çš„æœ€å¤§æ·±åº¦ï¼š$XGBoost$ å…ˆä»é¡¶åˆ°åº•å»ºç«‹æ ‘ç›´åˆ°æœ€å¤§æ·±åº¦ï¼Œå†ä»åº•åˆ°é¡¶åå‘æ£€æŸ¥æ˜¯å¦æœ‰**ä¸æ»¡è¶³åˆ†è£‚æ¡ä»¶çš„ç»“ç‚¹ï¼Œè¿›è¡Œå‰ªæã€‚**

- **shrinkage**: å­¦ä¹ ç‡æˆ–æ­¥é•¿é€æ¸ç¼©å°ï¼Œç»™åé¢çš„è®­ç»ƒç•™å‡ºæ›´å¤šçš„å­¦ä¹ ç©ºé—´

- å­é‡‡æ ·ï¼š<u>*æ¯è½®è®¡ç®—å¯ä»¥ä¸ä½¿ç”¨å…¨éƒ¨æ ·æœ¬ï¼Œä½¿ç®—æ³•æ›´åŠ ä¿å®ˆ*</u>

- **åˆ—æŠ½æ ·**ï¼šè®­ç»ƒçš„æ—¶å€™åªç”¨ä¸€éƒ¨åˆ†ç‰¹å¾ï¼ˆä¸è€ƒè™‘å‰©ä½™çš„blockå—å³å¯ï¼‰

**å‚æ•°ï¼š**

- ç¬¬ä¸€ç±»å‚æ•°ï¼šç”¨äºç›´æ¥æ§åˆ¶å½“ä¸ªæ ‘æ¨¡å‹çš„å¤æ‚åº¦ã€‚åŒ…æ‹¬max_depthï¼Œmin_child_weightï¼Œgamma ç­‰å‚æ•°
  - gammaï¼šåœ¨èŠ‚ç‚¹åˆ†è£‚æ—¶ï¼Œåªæœ‰åˆ†è£‚åæŸå¤±å‡½æ•°çš„å€¼ä¸‹é™äº†ï¼Œæ‰ä¼šåˆ†è£‚è¿™ä¸ªèŠ‚ç‚¹ã€‚GammaæŒ‡å®šäº†èŠ‚ç‚¹åˆ†è£‚æ‰€éœ€çš„æœ€å°æŸå¤±å‡½æ•°ä¸‹é™å€¼ã€‚ è¿™ä¸ªå‚æ•°çš„å€¼è¶Šå¤§ï¼Œç®—æ³•è¶Šä¿å®ˆ

- ç¬¬äºŒç±»å‚æ•°ï¼šç”¨äºå¢åŠ éšæœºæ€§ï¼Œä»è€Œä½¿å¾—æ¨¡å‹åœ¨è®­ç»ƒæ—¶å¯¹äºå™ªéŸ³ä¸æ•æ„Ÿã€‚åŒ…æ‹¬ï¼š
  - subsample - æ¯æ£µæ ‘ï¼Œéšæœºé‡‡æ ·çš„æ¯”ä¾‹
  - colsample_bytree - æ§åˆ¶æ¯æ£µéšæœºé‡‡æ ·çš„åˆ—æ•°çš„å æ¯”

- è¿˜æœ‰å°±æ˜¯ç›´æ¥å‡å°learning rateï¼Œä½†éœ€è¦åŒæ—¶å¢åŠ estimator å‚æ•°ã€‚



### LightGBM

ä» LightGBM åå­—æˆ‘ä»¬å¯ä»¥çœ‹å‡ºå…¶æ˜¯è½»é‡çº§ï¼ˆLightï¼‰çš„æ¢¯åº¦æå‡æœºï¼ˆGBMï¼‰ï¼Œå…¶ç›¸å¯¹ XGBoost å…·æœ‰è®­ç»ƒé€Ÿåº¦å¿«ã€å†…å­˜å ç”¨ä½çš„ç‰¹ç‚¹ã€‚

LightGBMé‡‡ç”¨leaf-wiseç”Ÿé•¿ç­–ç•¥ï¼ˆDFSï¼‰ï¼šæ¯æ¬¡ä»å½“å‰æ‰€æœ‰å¶å­ä¸­æ‰¾åˆ°åˆ†è£‚å¢ç›Šæœ€å¤§ï¼ˆä¸€èˆ¬ä¹Ÿæ˜¯æ•°æ®é‡æœ€å¤§ï¼‰çš„ä¸€ä¸ªå¶å­ï¼Œç„¶ååˆ†è£‚ï¼Œå¦‚æ­¤å¾ªç¯ï¼›ä½†ä¼šç”Ÿé•¿å‡ºæ¯”è¾ƒæ·±çš„å†³ç­–æ ‘ï¼Œäº§ç”Ÿè¿‡æ‹Ÿåˆã€‚

- ä¼˜ç‚¹

  - **Histogram**ï¼šç›´æ–¹å›¾ç®—æ³•çš„åŸºæœ¬æ€æƒ³æ˜¯å…ˆæŠŠè¿ç»­çš„æµ®ç‚¹ç‰¹å¾å€¼ç¦»æ•£åŒ–æˆkä¸ªæ•´æ•°ï¼ˆå…¶å®åˆæ˜¯åˆ†æ¡¶çš„æ€æƒ³ï¼Œè€Œè¿™äº›æ¡¶ç§°ä¸ºbinï¼Œæ¯”å¦‚[0,0.1)â†’0, [0.1,0.3)â†’1ï¼‰ï¼ŒåŒæ—¶æ„é€ ä¸€ä¸ªå®½åº¦ä¸ºkçš„ç›´æ–¹å›¾

    å°†å±äºè¯¥ç®±å­çš„æ ·æœ¬æ•°æ®æ›´æ–°ä¸ºç®±å­çš„å€¼ï¼Œç”¨ç›´æ–¹å›¾è¡¨ç¤º

    - **å¯ä»¥å‡å°‘å†…å­˜æ¶ˆè€—ï¼š**å› ä¸ºä¸ç”¨é¢å¤–å­˜å‚¨é¢„æ’åºçš„ç»“æœï¼Œå¯ä»¥åªä¿å­˜ç‰¹å¾ç¦»æ•£åŒ–åçš„å€¼
    - **è®¡ç®—ä»£ä»·æ›´å°**ï¼š
      - é¢„æ’åºç®—æ³•æ¯éå†ä¸€ä¸ªç‰¹å¾å€¼å°±è¦è®¡ç®—ä¸€æ¬¡åœ¨è¿™é‡Œåˆ†è£‚çš„information gainï¼Œä½†ç›´æ–¹å›¾åªéœ€è¦è®¡ç®—kä¸ªç»Ÿçš„æ•°
      - åŒæ—¶ï¼Œä¸€ä¸ªå¶å­çš„ç›´æ–¹å›¾å¯ä»¥ç”±å®ƒçš„çˆ¶äº²èŠ‚ç‚¹çš„ç›´æ–¹å›¾ä¸å®ƒå…„å¼Ÿçš„ç›´æ–¹å›¾åšå·®å¾—åˆ°

  - å•è¾¹æ¢¯åº¦é‡‡æ · `Gradient-based One-Sided Sampling (GOSS)` 

    - GBDT ç®—æ³•çš„æ¢¯åº¦å¤§å°å¯ä»¥ååº”æ ·æœ¬çš„æƒé‡ï¼Œæ¢¯åº¦è¶Šå°è¯´æ˜æ¨¡å‹æ‹Ÿåˆçš„è¶Šå¥½ï¼Œå•è¾¹æ¢¯åº¦æŠ½æ ·ç®—æ³•åˆ©ç”¨è¿™ä¸€ä¿¡æ¯å¯¹æ ·æœ¬è¿›è¡Œ**<u>æŠ½æ ·</u>**ï¼Œå‡å°‘äº†å¤§é‡æ¢¯åº¦å°çš„æ ·æœ¬ï¼Œåœ¨æ¥ä¸‹æ¥çš„è®¡ç®—é”…ä¸­åªéœ€å…³æ³¨æ¢¯åº¦é«˜çš„æ ·æœ¬ï¼Œæå¤§çš„å‡å°‘äº†è®¡ç®—é‡
      - åœ¨å¯¹æ¯ä¸ªtreeåšsamplingä»è€ŒåŠ é€Ÿçš„æ—¶å€™ï¼š**å› ä¸ºæ¯ä¸€æ­¥çš„gradientå°±æ˜¯residualï¼Œæˆ‘å°±å¯ä»¥sample based on this residual. æŠŠæ¢¯åº¦å¤§çš„é€‰å‡ºæ¥ï¼Œ**æ¢¯åº¦å°çš„sample it out. å¯ä»¥è®¾ç½®ä¸€ä¸ªthresholdæŠŠä½çš„ç­›æ‰
    - è¿™ä¸ªæ“ä½œåé¢ä¹Ÿä¼šç”¨æƒé‡å¹³è¡¡å›æ¥ï¼Œè®©**ä¸€æ–¹é¢ç®—æ³•å°†æ›´å¤šçš„æ³¨æ„åŠ›æ”¾åœ¨è®­ç»ƒä¸è¶³çš„æ ·æœ¬ä¸Šï¼Œå¦ä¸€æ–¹é¢é€šè¿‡ä¹˜ä¸Šæƒé‡æ¥é˜²æ­¢é‡‡æ ·å¯¹åŸå§‹æ•°æ®åˆ†å¸ƒé€ æˆå¤ªå¤§çš„å½±å“**

  - äº’æ–¥ç‰¹å¾æ†ç»‘Exclusive feature `bundling` to handle sparse features

    - å¦‚æœä¸¤ä¸ªç‰¹å¾å¹¶ä¸å®Œå…¨äº’æ–¥ï¼ˆå¦‚åªæœ‰ä¸€éƒ¨åˆ†æƒ…å†µä¸‹æ˜¯ä¸åŒæ—¶å–éé›¶å€¼ï¼‰ï¼Œå¯ä»¥ç”¨äº’æ–¥ç‡è¡¨ç¤ºäº’æ–¥ç¨‹åº¦ã€‚äº’æ–¥ç‰¹å¾æ†ç»‘ç®—æ³•ï¼ˆExclusive Feature Bundling, EFBï¼‰æŒ‡å‡ºå¦‚æœå°†ä¸€äº›ç‰¹å¾è¿›è¡Œèåˆç»‘å®šï¼Œåˆ™å¯ä»¥é™ä½ç‰¹å¾æ•°é‡ã€‚
    - speed up the process of splitting
    - åœ¨å®é™…åº”ç”¨ä¸­ï¼Œé«˜ç»´åº¦ç‰¹å¾å…·æœ‰ç¨€ç–æ€§ï¼Œè¿™æ ·å¯ä»¥è®¾è®¡ä¸€ä¸ªå‡å°‘æœ‰æ•ˆç‰¹å¾æ•°é‡çš„æ— æŸçš„æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¨€ç–ç‰¹å¾ä¸­ï¼Œè®¸å¤šç‰¹å¾æ˜¯äº’æ–¥çš„ï¼Œå‡ºç°å¤§é‡0ï¼Œä¾‹å¦‚one-hotã€‚æˆ‘ä»¬å¯ä»¥æ†ç»‘äº’æ–¥çš„ç‰¹å¾ã€‚æœ€åæˆ‘ä»¬è¿˜åŸæ†ç»‘äº’æ–¥é—®é¢˜ä¸ºå›¾ç€è‰²é—®é¢˜ï¼Œä½¿ç”¨è´ªå¿ƒç®—æ³•è¿‘ä¼¼æ±‚è§£ã€‚

  - **<u>LightGBM åŸç”Ÿæ”¯æŒç±»åˆ«ç‰¹å¾ï½œ</u>**Supports GPU training, sparse data & missing valuesï½œGenerally faster than XGBoost on CPUsï½œSupports **distributed training** on diï¬€erent frameworks like Ray, Spark, Dask etc.

  - ç¼ºå¤±å€¼å¤„ç†ï¼šæ¯æ¬¡åˆ†å‰²çš„æ—¶å€™ï¼Œåˆ†åˆ«æŠŠç¼ºå¤±å€¼æ”¾åœ¨å·¦å³ä¸¤è¾¹å„è®¡ç®—ä¸€æ¬¡ï¼Œç„¶åæ¯”è¾ƒä¸¤ç§æƒ…å†µçš„å¢ç›Šï¼Œæ‹©ä¼˜å½•å–



### XGBoostå’ŒLightGBMçš„åŒºåˆ«

- æ ‘ç”Ÿé•¿ç­–ç•¥ä¸åŒ

  - XGBé‡‡ç”¨level-wiseçš„åˆ†è£‚ç­–ç•¥ï¼šXGBå¯¹æ¯ä¸€å±‚æ‰€æœ‰èŠ‚ç‚¹åšæ— å·®åˆ«åˆ†è£‚ï¼Œä½†æ˜¯å¯èƒ½æœ‰äº›èŠ‚ç‚¹å¢ç›Šéå¸¸å°ï¼Œå¯¹ç»“æœå½±å“ä¸å¤§ï¼Œå¸¦æ¥ä¸å¿…è¦çš„å¼€é”€ã€‚

  - LGBé‡‡ç”¨leaf-wiseçš„åˆ†è£‚ç­–ç•¥ï¼šLeaf-wiseæ˜¯åœ¨æ‰€æœ‰å¶å­èŠ‚ç‚¹ä¸­é€‰å–åˆ†è£‚æ”¶ç›Šæœ€å¤§çš„èŠ‚ç‚¹è¿›è¡Œçš„ï¼Œä½†æ˜¯å¾ˆå®¹æ˜“å‡ºç°è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œæ‰€ä»¥éœ€è¦å¯¹æœ€å¤§æ·±åº¦åšé™åˆ¶ 

- æ ‘å¯¹ç‰¹å¾åˆ†å‰²ç‚¹æŸ¥æ‰¾ç®—æ³•ä¸åŒï¼š

  - XGBä½¿ç”¨ç‰¹å¾é¢„æ’åºç®—æ³•

  - LGBä½¿ç”¨åŸºäºç›´æ–¹å›¾çš„åˆ‡åˆ†ç‚¹ç®—æ³•ï¼š

    - å‡å°‘å†…å­˜å ç”¨ï¼Œæ¯”å¦‚ç¦»æ•£ä¸º256ä¸ªbinæ—¶ï¼Œåªéœ€è¦ç”¨8ä½æ•´å½¢å°±å¯ä»¥ä¿å­˜ä¸€ä¸ªæ ·æœ¬è¢«æ˜ å°„ä¸ºå“ªä¸ªbin(è¿™ä¸ªbinå¯ä»¥è¯´å°±æ˜¯è½¬æ¢åçš„ç‰¹å¾)ï¼Œå¯¹æ¯”é¢„æ’åºçš„exact greedyç®—æ³•æ¥è¯´ï¼ˆç”¨int_32æ¥å­˜å‚¨ç´¢å¼•+ ç”¨float_32ä¿å­˜ç‰¹å¾å€¼ï¼‰ï¼Œå¯ä»¥èŠ‚çœ7/8çš„ç©ºé—´ã€‚

    - è®¡ç®—æ•ˆç‡æé«˜ï¼Œé¢„æ’åºçš„Exact greedyå¯¹æ¯ä¸ªç‰¹å¾éƒ½éœ€è¦éå†ä¸€éæ•°æ®ï¼Œå¹¶è®¡ç®—å¢ç›Šã€‚è€Œç›´æ–¹å›¾ç®—æ³•åœ¨å»ºç«‹å®Œç›´æ–¹å›¾åï¼Œåªéœ€è¦å¯¹æ¯ä¸ªç‰¹å¾éå†ç›´æ–¹å›¾å³å¯

  - ç„¶åè¿™é‡Œä¹Ÿè·Ÿåˆ†è£‚æ–¹å¼æœ‰å…³

    - XGB åœ¨æ¯ä¸€å±‚éƒ½åŠ¨æ€æ„å»ºç›´æ–¹å›¾ï¼Œ å› ä¸ºXGBçš„ç›´æ–¹å›¾ç®—æ³•ä¸æ˜¯é’ˆå¯¹æŸä¸ªç‰¹å®šçš„featureï¼Œè€Œæ˜¯æ‰€æœ‰featureå…±äº«ä¸€ä¸ªç›´æ–¹å›¾(æ¯ä¸ªæ ·æœ¬çš„æƒé‡æ˜¯äºŒé˜¶å¯¼)ï¼Œæ‰€ä»¥æ¯ä¸€å±‚éƒ½è¦é‡æ–°æ„å»ºç›´æ–¹å›¾ã€‚

    - LGBä¸­å¯¹æ¯ä¸ªç‰¹å¾éƒ½æœ‰ä¸€ä¸ªç›´æ–¹å›¾ï¼Œæ‰€ä»¥æ„å»ºä¸€æ¬¡ç›´æ–¹å›¾å°±å¤Ÿäº†
    
      LGBè¿˜å¯ä»¥ä½¿ç”¨ç›´æ–¹å›¾åšå·®åŠ é€Ÿï¼Œä¸€ä¸ªèŠ‚ç‚¹çš„ç›´æ–¹å›¾å¯ä»¥é€šè¿‡çˆ¶èŠ‚ç‚¹çš„ç›´æ–¹å›¾å‡å»å…„å¼ŸèŠ‚ç‚¹çš„ç›´æ–¹å›¾å¾—åˆ°ï¼Œä»è€ŒåŠ é€Ÿè®¡ç®—

- æ ·æœ¬é€‰æ‹©ï¼šä¼šåšå•è¾¹æ¢¯åº¦é‡‡æ ·ï¼ŒLightGBMä¼šå°†æ›´å¤šçš„æ³¨æ„åŠ›æ”¾åœ¨è®­ç»ƒä¸è¶³çš„æ ·æœ¬ä¸Š

- è¿˜æœ‰ä¸€äº›å°çš„åŒºåˆ«ï¼š
  - ç¦»æ•£å˜é‡å¤„ç†ä¸Šï¼šXGBæ— æ³•ç›´æ¥è¾“å…¥ç±»åˆ«å‹å˜é‡å› æ­¤éœ€è¦äº‹å…ˆå¯¹ç±»åˆ«å‹å˜é‡è¿›è¡Œç¼–ç ï¼ˆä¾‹å¦‚ç‹¬çƒ­ç¼–ç ï¼‰ï¼ŒLGBå¯ä»¥ç›´æ¥å¤„ç†ç±»åˆ«å‹å˜é‡
  - è¯†åˆ«ä¸€äº›äº’æ–¥çš„ç‰¹å¾ä¸Šï¼ŒLightGBMå¯ä»¥bundlingç­‰ç­‰

### CatBoost

- Optimized for **categorical** features
- Uses `target encoding` to handle categorical features
- Uses ordered boosting to build "symmetirc" trees
  - ç»™æ¯ä¸ª sampleç¼–ä¸€ä¸ªtime ï¼ˆ incorporate a sense of time on the data itself, which means that these samples have occurred before and these samples have occurred later.ï¼‰
  - Every three trains on a portion of the data based on that time and then it makes a prediction on the other part.
- Overfitting dertector
- Supports GPU training, sparse data & missing values
- Monotonicity constraints



# Reference

[çŸ¥ä¹|å°±æ˜¯æ¨å®—|SVMçš„æ ¸å‡½æ•°å¦‚ä½•é€‰å–ï¼Ÿ](https://www.zhihu.com/question/21883548/answer/205191440)

[çŸ¥ä¹|é˜¿æ³½|ã€æœºå™¨å­¦ä¹ ã€‘å†³ç­–æ ‘ï¼ˆä¸‹ï¼‰â€”â€”XGBoostã€LightGBMï¼ˆéå¸¸è¯¦ç»†ï¼‰](https://zhuanlan.zhihu.com/p/87885678)

