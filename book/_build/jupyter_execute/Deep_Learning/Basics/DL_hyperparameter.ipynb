{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91283427-e9f8-479d-90f5-76312d37b30b",
   "metadata": {},
   "source": [
    "# DL常见超参及调整策略"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01429e6e-f38f-4312-b488-57ae452edc1c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 学习率"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63445b3c-7961-4516-88bd-0aa9ce1a7928",
   "metadata": {
    "tags": []
   },
   "source": [
    "### constant learning rate会遇到的问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf42e4c2-1351-468c-b912-c3241ac3472b",
   "metadata": {},
   "source": [
    "- Constant Learning Rate过大或者过小：<center><img src=\"../../images/DL_hyperparameters_1.png\" width=\"70%\"/></center><center><img src=\"../../images/DL_hyperparameters_2.png\" width=\"70%\"/></center>\n",
    "\n",
    "    - 最佳：model adjust weight **in subsequent training loops** to arrive at cost minima\n",
    "\n",
    "    - 低learning rate ⇒ converge过慢：converge to cost minima but very slowly\n",
    "\n",
    "    - 高learning rate ⇒ 震荡(miss the minima)、不能converge（随着loops不断提升）\n",
    "    \n",
    "    > Large learning rate may allow the algorithm to come close to a good solution but will then oscillate around the point or even diverge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faf26bb-6bb5-4594-992d-8565638647af",
   "metadata": {},
   "source": [
    "### Learning rate schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcaf1732-c199-4ff4-8538-8c55ecba4be0",
   "metadata": {},
   "source": [
    "> 解决constant learning rate的各种问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c636a0e0-6e37-4347-8760-fdb5b7dbfb36",
   "metadata": {},
   "source": [
    "#### 1、 Decay functions⇒先快后慢\n",
    "\n",
    "一般来说，我们希望在训练初期学习率大一些，使得网络收敛迅速，在训练后期学习率小一些，使得网络更好的收敛到最优解。\n",
    "\n",
    "- Start with a higher learning rate to explore the loss space => **find a good starting values for the weights**\n",
    "- Use smaller learning rates in later steps to converge to a minima => **tune the weights slowly**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Function：\n",
    "> t: number of iterations/epochs\n",
    "\n",
    "- `multi-step`：根据非均匀步长对学习率进行调整，每隔一定步数（或者epoch）就减少为原来的gamma分之一：$\\alpha_{t}=\\frac{\\alpha_{0}}{\\gamma^{n}} \\quad \\text { at step } \\mathrm{n}$ 表示在第n次减少的时候reduce it by factor of $\\frac{1}{\\gamma^{n}}$\n",
    "\n",
    "    - 例1：\n",
    "\n",
    "        ```python\n",
    "        # 使用固定步长衰减依旧先定义优化器\n",
    "        optimizer_StepLR = torch.optim.SGD(net.parameters(), lr=0.1)\n",
    "        # 再给优化器绑定StepLR对象\n",
    "        StepLR = torch.optim.lr_scheduler.StepLR(optimizer_StepLR, step_size=step_size, gamma=0.65)\n",
    "        ```\n",
    "\n",
    "        - 其中gamma参数表示衰减的程度，step_size参数表示每隔多少个step进行一次学习率调整，下面对比了不同gamma值下的学习率变化情况：\n",
    "        ![img](https://pic1.zhimg.com/80/v2-a1c38e6c8e26ad3e953d1ebb67d7243c_1440w.jpg)\n",
    "\n",
    "    - 例2：Alexnet中的learningrate<br><center><img src=\"../../images/DL_hyperparameters_3.png\" width=\"70%\"/></center><br>[Link](https://blog.csdn.net/guzhao9901/article/details/116484887)：在每10、15、25、30的时候把原先的学习率乘个0.1\n",
    "        ```python\n",
    "        model = ANet(classes=5)     #加载模型\n",
    "        optimizer = optim.SGD(params = model.parameters(), lr=0.05)   #优化方法使用SGD\n",
    "        #在指定的epoch值，如[10,15，25，30]处对学习率进行衰减，lr = lr * gamma\n",
    "        scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[10,15,25,30], gamma=0.1)\n",
    "        ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f24d4e9-9b31-4795-914d-cbaea8db6ab3",
   "metadata": {},
   "source": [
    "- `inverse`: $\\alpha_{t}=\\frac{\\alpha_{0}}{1+\\gamma \\cdot t}$\n",
    "\n",
    "- `Exponential 指数衰减`: $\\alpha_{t}=\\alpha_{0} \\exp (-\\gamma \\cdot t)$\n",
    " \n",
    "    ```python\n",
    "    # 首先需要确定需要针对哪个优化器执行学习率动态调整策略，也就是首先定义一个优化器\n",
    "    optimizer_ExpLR = torch.optim.SGD(net.parameters(), lr=0.1)\n",
    "\n",
    "    # 定义好优化器以后，就可以给这个优化器绑定一个指数衰减学习率控制器\n",
    "    ExpLR = torch.optim.lr_scheduler.ExponentialLR(optimizer_ExpLR, gamma=0.98)\n",
    "    ```\n",
    "    - 其中参数gamma表示衰减的底数，选择不同的gamma值可以获得幅度不同的衰减曲线，如下：\n",
    "    \n",
    "    <center><img src=\"https://pic3.zhimg.com/80/v2-d990582cda2fc2aa88ae91d5aa17a6b6_1440w.jpg\" width=\"40%\"/></center>\n",
    "\n",
    "\n",
    "\n",
    "- `polynomial`: $\\alpha_{t}=\\alpha_{0}\\left(1-\\frac{t}{\\max_{t}}\\right)^{n}$\n",
    "    - $\\mathrm{n}=1$ gives linear\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1bd780-cc6f-48d7-9b15-38231ff9745f",
   "metadata": {},
   "source": [
    "#### 2、Cyclical Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04366aa6-1ef4-4ce8-a42a-779c3906a591",
   "metadata": {},
   "source": [
    "Idea is to have learning rate continuously change in cyclical manner with alternate increase and decrease phases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7fec01-e1f4-499d-a4e4-b6b4abbafd27",
   "metadata": {},
   "source": [
    "<img src=\"../../images/DL_hyperparameters_4.png\" width=\"40%\"/>\n",
    "\n",
    "- 意义：比如说我们到一个local minimum之后 gradient很低，比如到了一个很平的地方，并不是一个good minimum，这个时候通过提升learning rate我们还有probability可以escape it\n",
    "\n",
    "\n",
    "- `余弦退火衰减`：严格的说，余弦退火策略不应该算是学习率衰减策略，因为它使得学习率按照周期变化，其定义方式如下：\n",
    "\n",
    "    ```python\n",
    "    optimizer_CosineLR = torch.optim.SGD(net.parameters(), lr=0.1)\n",
    "    CosineLR = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_CosineLR, T_max=150, eta_min=0)\n",
    "    ```\n",
    "    - 参数T_max表示余弦函数周期；eta_min表示学习率的最小值，默认它是0表示学习率至少为正值。下图展示了不同周期下的余弦学习率更新曲线：\n",
    "    <center><img src=\"https://pic2.zhimg.com/80/v2-bb255df05eb665cc6530845bde637bc9_1440w.jpg\" width=\"70%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5114c47-b404-45b5-936c-dd5a3e9fbb48",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3、WarmUp\n",
    "\n",
    "warmup 需要在训练最初使用较小的学习率来启动，并很快切换到大学习率而后进行常见的衰减decay。\n",
    "\n",
    "这是因为，刚开始模型对数据的“分布”理解为零，或者是说“均匀分布”（当然这取决于你的初始化）；在第一轮训练的时候，每个数据点对模型来说都是新的，模型会**很快地进行数据分布修正**，如果这时候学习率就很大，极有可能导致开始的时候就对该数据过拟合，后面要通过多轮训练才能拉回来，浪费时间。当训练了一段时间（比如两轮、三轮）后，模型已经对每个数据点看过几遍了，或者说对当前的batch而言有了一些正确的[先验](https://www.zhihu.com/search?q=%E5%85%88%E9%AA%8C&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A438851458%7D)，较大的学习率就不那么容易会使模型学偏，所以可以适当调大学习率。这个过程就可以看做是warmup。那么为什么之后还要decay呢？当模型训到一定阶段后（比如十个epoch），模型的分布就已经比较固定了，或者说能学到的新东西就比较少了。如果还沿用较大的学习率，就会破坏这种稳定性，用我们通常的话说，就是已经接近loss的local optimal了，为了靠近这个point，我们就要慢慢来。\n",
    "\n",
    "\n",
    "\n",
    "BERT的预训练过程就是用了学习率WarmUp的方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42912494-381a-4260-a4ed-1ca1cebd4df2",
   "metadata": {},
   "source": [
    "### Parameter-specific learning rates\n",
    "\n",
    "Apply a different learning rate to **each parameter at each step**\n",
    "\n",
    "- Encourage faster relative movement in gently sloping direction <br>\n",
    "  Penalize dimension with large fluctuations in gradient - 坏的gradient更新feature！\n",
    "- Several methods: AdaGrad, RMSProp, RMSProp+Nestrov Momentum, AdaDelta, Adam\n",
    "    - Differ in the manner parameter specific learning rates are calculated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b427cf87-930c-49b5-9596-5b7d145830e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Batch Size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d434e85-f4a0-45eb-80a0-2d4ad8109b96",
   "metadata": {},
   "source": [
    "\n",
    "`Batch Size`表示input中我们用来计算Loss、gradient，从而对weight进行**一次update**所用的数据batch的行数\n",
    "- 常见问题——GPU限制：Batch size is restricted by the GPU memory (12GB for K40, 16GB for P100 and V100) and the model size\n",
    "> Model and batch of data needs to remain in GPU memory for one iteration <br>\n",
    "ResNet152 we need to stay below 10\n",
    "    - 后果：calculate gradient using a small number of samples⇒ gradient estimates will be much noisy，即使做了更多的updates也会导致poor convergence\n",
    "    \n",
    "但是，Are you restricted to work with small size mini-batches for large models and/or GPUs with limited memory？\n",
    "- No, you can simulate large batch size by **delaying gradient/weight updates** to happen every $n$ iterations (instead of $n=1$ ) ; supported by frameworks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbc87fd-980f-42e7-8579-3270efd2c48f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Effective Mini-batch方法\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ba5576-28a1-490e-9f4c-d8da1959b37e",
   "metadata": {},
   "source": [
    "核心：Calculate and accumulate gradients over **multiple mini-batches**\n",
    "\n",
    "<center><img src=\"../../images/DL_hyperparameters_5.png\" width=\"65%\"/></center>\n",
    "\n",
    "- 左边：每过一个mini batch就做update\n",
    "- 右边：Simulate large batch——Perform optimizer step (update model parameters) only after specified number of mini-batches\n",
    "    \n",
    "参数名\n",
    "    \n",
    "- Caffe: `iter_size`; \n",
    "- Pytorch: `batch_multiplier`\n",
    "    \n",
    "需要注意：scale up the learning rate when working with large mini-batch size！因为我们现在对gradient更加confident了，大的lr可以让我们converge **faster** to a consisten direction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9582d05-213e-4913-a129-4c06750d0347",
   "metadata": {},
   "source": [
    "#### What Batch size to choose ?\n",
    "\n",
    "Hardware constraints (GPU memory) dictate the largest batch size. Should we try to work with the largest possible batch size ?\n",
    "- 正向因素\n",
    "    - Large batch size gives more confidence in gradient estimation\n",
    "    - Large batch size allows you to work with higher learning rates, faster convergence\n",
    "- 副作用\n",
    "    - Large batch size leads to poor **generalization** (Keskar et al 2016)\n",
    "    <img src=\"../../images/DL_hyperparameters_4.png\" width=\"65%\"/>\n",
    "    \n",
    "        - 大batch会lands on **sharp minima** wheareas <br>\n",
    "          Small batch SGD find flat minimas which generalize better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de030b1-b3be-450a-a325-82de3540cd95",
   "metadata": {},
   "source": [
    "#### Learning rate and Batch size relationship\n",
    "\n",
    "“Noise scale” in stochastic gradient descent (Smith et al 2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb4f36b-7b90-4905-8a80-10da27d62fd7",
   "metadata": {},
   "source": [
    "$$g=\\epsilon\\left(\\frac{N}{B}-1\\right)$$\n",
    "$$g \\approx \\frac{\\epsilon N}{B} \\quad \\text{as } B \\ll N$$\n",
    "\n",
    "> training dataset size｜B: batch size｜$\\epsilon$: learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ffa053-2b28-4e1b-a6d7-166da95eeca5",
   "metadata": {},
   "source": [
    "- g: an optimum noise scale g that maximizes the test set accuracy (at constant learning rate)\n",
    "\n",
    "    - when B ≪ N的时候，学习率和Batch size是成正比的！增大batch size再减小learning rate可能效果互相抵消 \n",
    "    - Increasing batch size will have the same effect as **decreasing learning rate**\n",
    "        - Achieves near-identical model performance on the test set with the same number of training epochs but significantly fewer parameter updates<center><img src=\"../../images/DL_hyperparameters_7.png\" width=\"90%\"/></center>        \n",
    "            - 红色线和蓝色线一个表示降learning rate，一个表示提高batch size，两个方法在train和test的表现都差不多\n",
    "            - 这说明存在某个optimal scale，让effect of 这两个操作得到类似的效果\n",
    "        - 按照这个思路：固定BS后cyclic learning rate policy with max bound decay $\\iff$ 固定lr后cyclic batch size policy with base batch size increase to match the max batch size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}